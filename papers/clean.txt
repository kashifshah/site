Translation Model Adaptation by Resampling
Kashif Shah, Lo ic Barrault, Holger Schwenk LIUM, University of Le Mans Le Mans, France. FirstName.LastName@lium.univ-lemans.fr

Abstract
The translation model of statistical machine translation systems is trained on parallel data coming from various sources and domains. These corpora are usually concatenated, word alignments are calculated and phrases are extracted. This means that the corpora are not weighted according to their importance to the domain of the translation task. This is in contrast to the training of the language model for which well known techniques are used to weight the various sources of texts. On a smaller granularity, the automatic calculated word alignments differ in quality. This is usually not considered when extracting phrases either. In this paper we propose a method to automatically weight the different corpora and alignments. This is achieved with a resampling technique. We report experimental results for a small (IWSLT) and large (NIST) Arabic/English translation tasks. In both cases, significant improvements in the BLEU score were observed.

1

Introduction

Two types of resources are needed to train statistical machine translation (SMT) systems: parallel corpora to train the translation model and monolingual texts in the target language to build the language model. The performance of both models depends of course on the quality and quantity of the available resources. Today, most SMT systems are generic, i.e. the same system is used to translate texts of all kinds. Therefore, it is the domain of the training resources that influences the translations that are selected among several choices. While monolingual
392

texts are in general easily available in many domains, the freely available parallel texts mainly come from international organisations, like the European Union or the United Nations. These texts, written in particular jargon, are usually much larger than in-domain bitexts. As an example we can cite the development of an NIST Arabic/English phrase-based translation system. The current NIST test sets are composed of a news wire part and a second part of web-style texts. For both domains, there is only a small number of in-domain bitexts available, in comparison to almost 200 millions words of out-of-domain UN texts. The later corpus is therefore likely to dominate the estimation of the probability distributions of the translation model. It is common practice to use a mixture language model with coefficients that are optimized on the development data, i.e. by these means on the domain of the translation task. Domain adaptation seems to be more tricky for the translation model and it seems that very little research has been done that seeks to apply similar ideas to the translation model. To the best of our knowledge, there is no commonly accepted method to weight the bitexts coming from different sources so that the translation model is best optimized to the domain of the task. Mixture models are possible when only two different bitexts are available, but are rarely used for more corpora (see discussion in the next section). In this work we propose a new method to adapt the translation model of an SMT system. We only perform experiments with phrase-based systems, but the method is generic and could be easily applied to an hierarchical or syntax-based system. We first associate a weighting coefficient to each bitext. The main idea is to use resampling to produce a new collection of weighted alignment files, followed by the standard procedure to extract the phrases. In a second step, we also consider the

Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392399, Uppsala, Sweden, 15-16 July 2010. c 2010 Association for Computational Linguistics

alignment score of each parallel sentence pair, emphasizing by these means good alignments and down-weighting less reliable ones. All the parameters of our procedure are automatically tuned by optimizing the BLEU score on the development data. The paper is organized as follows. The next section describes related work on weighting the corpora and model adaptation. Section 3 describes the architecture allowing to resample and to weight the bitexts. Experimental results are presented in section 4 and the paper concludes with a discussion.

2

Related Work

Adaptation of SMT systems is a topic of increasing interest since few years. In previous work, adaptation is done by using mixture models, by exploiting comparable corpora and by selfenhancement of translation models. Mixture models were used to optimize the coefficients to the adaptation domain. (Civera and Juan, 2007) proposed a model that can be used to generate topic-dependent alignments by extension of the HMM alignment model and derivation of Viterbi alignments. (Zhao et al., 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. The training corpus was divided into different components, a model was trained on each part and then weighted appropriately for the given context. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. Two decoding paths were used to translate the text. Comparable corpora are exploited to find additional parallel texts. Information retrieval techniques are used to identify candidate sentences (Hildebrand et al., 2005). (Snover et al., 2008) used cross-lingual information retrieval to find texts in the target language that are related to the domain of the source texts. A self-enhancing approach was applied by (Ueffing, 2006) to filter the translations of the test set with the help of a confidence score and to use reliable alignments to train an additional
393

phrase table. This additional table was used with the existing generic phrase table. (Ueffing, 2007) further refined this approach by using transductive semi-supervised methods for effective use of monolingual data from the source text. (Chen et al., 2008) performed domain adaptation simultaneously for the translation, language and reordering model by learning posterior knowledge from N-best hypothesis. A related approach was investigated in (Schwenk, 2008) and (Schwenk and Senellart, 2009) in which lightly supervised training was used. An SMT system was used to translate large collections of monolingual texts, which were then filtered and added to the training data. (Matsoukas et al., 2009) propose to weight each sentence in the training bitext by optimizing a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. Then certain parts of the training bitexts were downweighted to optimize an objective function on the development data. This can lead to parameter over-fitting if the function that maps sentence features to weights is complex. The technique proposed in this paper is somehow related to the above approach of weighting the texts. Our method does not require an explicit specification of the in-domain and out-ofdomain training data. The weights of the corpora are directly optimized on the development data using a numerical method, similar to the techniques used in the standard minimum error training of the weights of the feature functions in the log-linear criterion. All the alignments of the bitexts are resampled and given equal chance to be selected and therefore, influence the translation model in a different way. Our proposed technique does not require the calculation of extra sentence level features, however, it may use the alignments score associated with each aligned sentence pair as a confidence score.

3

Description of the algorithm

The architecture of the algorithm is summarized in figure 1. The starting point is an (arbitrary) number of parallel corpora. We first concatenate these bitexts and perform word alignments in both directions using GIZA++. This is done on the concatenated bitexts since GIZA++ may perform badly if some of the individual bitexts are rather small. Next, the alignments are separated in parts corre-

be used by the standard phrase extraction procedure. 3.1 Resampling the alignments

In statistics, resampling is based upon repeated sampling within the same sample until a sample is obtained which better represents a given data set (Yu, 2003). Resampling is used for validating models on given data set by using random subsets. It overcomes the limitations to make assumptions about the distribution of the data. Usually resampling is done several times to better estimate and select the samples which better represents the target data set. The more often we resample, the closer we get to the true probability distribution. In our case we performed resampling with replacement according to the following algorithm: Algorithm 1 Resampling 1: for i = 0 to required size do 2: Select any alignment randomly 3: Alscore  normalized alignment score 4: T hreshold  rand[0, 1] 5: if Alscore > T hreshold then 6: keep it 7: end if 8: end for Let us call resampling factor, the number of times resampling should be done. An interesting question is to determine the optimal value of this resampling factor. It actually depends upon the task or data we are experimenting on. We may start with one time resampling and could stop when results becomes stable. Figure 2 plots a typical curve of the BLEU score as a function of the number of times we resample. It can be observed that the curve is growing proportionally to the resampling factor until it becomes stable after a certain point. 3.2 Weighting Schemes

Figure 1: Architecture of SMT Weighting System

sponding to the individual bitexts and a weighting coefficient is associated to each one. We are not aware of a procedure to calculate these coefficients in an easy and fast way without building an actual SMT system. Note that there is an EM procedure to do this for language modeling. In the next section, we will experimentally compare equal coefficients, coefficients set to the same values than those obtained when building an interpolated language model on the source language, and a new method to determine the coefficients by optimizing the BLEU score on the development data. One could imagine to directly use these coefficients when calculating the various probabilities of the extracted phrases. In this work, we propose a different procedure that makes no assumptions on how the phrases are extracted and probabilities are calculated. The idea is to resample alignments from the alignment file corresponding to the individual bitexts according to their weighting coefficients. By these means, we create a new, potentially larger alignment file, which then in turn will
394

We concentrated on translation model adaptation when the bitexts are heterogeneous, e.g. indomain and out-of-domain or of different sizes. In this case, weighting these bitexts seems interesting and can be used in order to select data which better represent the target domain. Secondly when sentences are aligned, some alignments are reliable and some are less. Using unreliable alignments can put negative effect on the translation quality. So we need to exclude or down-weight

56

dev test baseline(test)

satisfies F (x ) = min F (x)
x

55.5

(2)

55

54.5 BLEU

where n is the dimension of search space and x is the optimum of x. The following algorithm was used to weight the bitexts. Algorithm 2 W eightingCorpora 1: Determine word to word alignment with GIZA++ on concatenated bitext. 2: while Not converged do 3: Run Condor initialized with LM weights. 4: Create new alignment file by resampling according to weights given by Condor. 5: Use the alignment file to extract phrases and build the translation table (phrase table) 6: Tune the system with MERT (this step can be skipped until weights are optimized to save time) 7: Calculate the BLEU score 8: end while 3.2.2 Weighting Alignments Alignments produced by GIZA++ have alignment scores associated with each sentence pair in both direction, i.e. source to target and target to source. We used these alignment scores as confidence measurement for each sentence pair. Alignment scores depend upon the length of each sentence, therefore, they must be normalized regarding the size of the sentence. Alignment scores have a very large dynamic range and we have applied a logarithmic mapping in order to flatten the probability distribution :  ( ntrg asrc trg + log(  2
nsrc

54

53.5

53

52.5

52 0 5 10 Resampling factor 15 20

Figure 2: The curve shows that by increasing the resampling factor we get better and stable results on Dev and Test. unreliable alignments and keep or up-weight the good ones. We conceptually divided the weighting in two parts that is (i) weighting the corpora and (ii) weighting the alignments 3.2.1 Weighting Corpora We started to resample the bitexts with equal weights to see the effect of resampling. This gives equal importance to each bitext without taking into account the domain of the text to be translated. However, it should be better to give appropriate weights according to a given domain as shown in equation 1 1 bitext1 + 2 bitext2 + .. + n bitextn (1)

where the n are the coefficients to optimize. One important question is how to find out the appropriate coefficient for each corpus. We investigated a technique similar to the algorithm used to minimize the perplexity of an interpolated target LM. Alternatively, it is also possible to construct a interpolated language model on the source side of bitexts. This approach was implemented and these coefficients were used as the weights for each bitext. One can certainly ask the question whether the perplexity is a good criterion for weighting bitexts. Therefore, we worked on direct optimization of these coefficients by CONDOR (Berghen and Bersini, 2005). This freely available tool is a numerical optimizer based on Powell's UOBYQA algorithm (Powell, 1994). The aim of CONDOR is to minimize a objective function using the least number of function evaluations. Formally, it is used to find x  Rn with given constraints which
395



atrg src )

)

(3)

where a is the alignment score, n the size of a sentence and  a coefficient to optimize. This is also done by Condor. Of course, some alignments will appear several times, but this will increase the probability of certain phrase-pairs which are supposed to be more related to the target domain. We have observed that the weights of an interpolated LM build on the source side of the bitext are good initial values for CONDOR. Moreover, weights optimized by Condor are in the same order than these "LM weights". Therefore, we do not perform MERT of the SMT systems build at each step of the optimization of the weights i and  by CONDOR,

Baseline With equal weights With LM weights Condor weights

IWSLT Task Dev (Dev6) Test (Dev7) 53.98 53.37 53.71 53.20 54.20 53.71 54.80 53.98

NIST Task Dev (NIST06) Test (NIST08) 43.16 42.21 43.10 42.11 43.42 42.22 43.49 42.28

Table 1: BLEU scores when weighting corpora (one time resampling) IWSLT Task Dev (Dev6) Test (Dev7) 53.98 53.37 53.80 53.30 54.32 53.91 55.10 54.13 NIST Task Dev (NIST06) Test (NIST08) 43.16 42.21 43.13 42.15 43.54 42.37 43.80 42.40

Baseline With equal weights With LM weights Condor weights

Table 2: BLEU scores when weighting corpora (optimum number of resampling) IWSLT Task Test (Dev7) 53.37 53.33 54.10 54.58 NIST Task Test (NIST08) 42.21 42.21 42.41 42.54

Baseline With equal weights With LM weights Condor weights

Dev (Dev6) 53.98 53.85 54.80 55.48

TER(Test) 32.75 32.80 31.50 31.31

Dev (NIST06) 43.16 43.28 43.42 43.95

TER(Test) 51.69 51.72 51.50 51.35

Table 3: BLEU and TER scores when weighting corpora and alignments (optimum number of resampling)

but use the values obtained by running MERT on a system obtained by using the "LM weights" to weight the alignments. Once CONDOR has converged to optimal weights, we can then tune our system by MERT. This saves lot of time taken by the tuning process and it had no impact on the results.

sition. We considered two well known official evaluation tasks to evaluate our approach, namely NIST and IWSLT. For IWSLT, we used the BTEC bitexts (194M words), Dev1, Dev2, Dev3 (60M words each) as training data, Dev6 as development set and Dev7 as test set. From previous experiments, we have evidence that the various development corpora are not equally important and weighting them correctly should improve the SMT system. We analyze the translation quality as measured by the BLEU score for the three methods: equal weights, LM weights and Condor weights and considering one time resampling. Further experiments were performed using the optimized number of resampling with and without weighting the alignments. We have realized that it is beneficial to always include the original alignments. Even if we resample many times there is a chance that some alignments might never be selected but we do not want to loose any information. By keeping original alignments, all alignments are given a chance to be se396

4

Experimental evaluation

The baseline system is a standard phrase-based SMT system based on the Moses SMT toolkit (Koehn and et al., 2007). In our system we used fourteen features functions. These features functions include phrase and lexical translation probabilities in both directions, seven features for lexicalized distortion model, a word and phrase penalty, and a target language model. The MERT tool is used to tune the coefficients of these feature functions. We considered Arabic to English translation. Tokenization of the Arabic source texts is done by a tool provided by SYSTRAN which also performs a morphological decompo-

lected at least once. All these results are summarized in tables 1, 2 and 3. One time resampling along with equal weights gave worse results than the baseline system while improvements in the BLEU score were observed with LM and Condor weights for the IWSLT task, as shown in table 1. Resampling many times always gave more stable results, as already shown in figure 2 and as theoretically expected. For this task, we resampled 15 times. The improvements in the BLEU score are shown in table 2. Furthermore, using the alignment scores resulted in additional improvements in the BLEU score. For the IWSLT task, we achieved and overall improvement of 1.5 BLEU points on the development set and 1.2 BLEU points on the test set as shown in table 3 To validate our approach we further experimented with the NIST evaluation task. Most of the training data used in our experiments for the NIST task is made available through the LDC. The bitexts consist of texts from the GALE project1 (1.6M words), various news wire translations2 (8.0M words) on development data from previous years (1.6M words), LDC treebank data (0.4M words) and the ISI extracted bitexts (43.7M words). The official NIST06 evaluation data was used as development set and the NIST08 evaluation data was used as test set. The same procedure was adapted for the NIST task as for the IWSLT task. Results are shown in table 1 by using different weights and one time resampling. Further improvements in the results are shown in table 2 with the optimum number of resampling which is 10 for this task. Finally, results by weighting alignments along with weighting corpora are shown in table 3. Our final system achieved an improvement of 0.79 BLEU points on the development set and 0.33 BLEU points on the test set. TER scores are also shown on test set of our final system in table 3. Note that these results are state-of-the-art when compared to the official results of the 2008 NIST evaluation3 . The weights of the different corpora are shown in table 4 for the IWSLT and NIST task. In both cases, the weights optimized by CONDOR are substantially different form those obtained when
LDC2005E83, 2006E24, E34, E85 and E92 LDC2003T07, 2004E72, T17, T18, 2005E46 and 2006E25. 3 http://www.nist.gov/speech/tests/mt/ 2008/
2 1

creating an interpolated LM on the source side of the bitexts. In any case, the weights are clearly non uniform, showing that our algorithm has focused on in-domain data. This can be nicely seen for the NIST task. The Gale texts were explictely created to contain in-domain news wire and WEB texts and actually get a high weight despite their small size, in comparison to the more general news wire collection from LDC.

5

Conclusion and future work

We have proposed a new technique to adapt the translation model by resampling the alignments, giving a weight to each corpus and using the alignment score as confidence measurement of each aligned phrase pair. Our technique does not change the phrase pairs that are extracted,4 but only the corresponding probability distributions. By these means we hope to adapt the translation model in order to increase the weight of translations that are important to the task, and to downweight the phrase pairs which result from unreliable alignments. We experimentally verified the new method on the low-resource IWSLT and the resource-rich NIST'08 tasks. We observed significant improvement on both tasks over state-of-the-art baseline systems. This weighting scheme is generic and it can be applied to any language pair and target domain. We made no assumptions on how the phrases are extracted and it should be possible to apply the same technique to other SMT systems which rely on word-to-word alignments. On the other hand, our method is computationally expensive since the optimisation of the coefficients requires the creation of a new phrase table and the evaluation of the resulting system in the tuning loop. Note however, that we run GIZA++ only once. In future work, we will try to directly use the weights of the corpora and the alignments in the algorithm that extracts the phrase pairs and calculates their probabilities. This would answer the interesting question whether resampling itself is needed or whether weighting the corpora and alignments is the key to the observed improvements in the BLEU score. Finally, it is straight forward to consider more feature functions when resampling the alignments. This may be a way to integrate linguistic knowl4

when also including the original alignments

397

IWSLT Task # of Words LM Coeffs Condor Coeffs NIST TASK # of words LM Coeffs Condor Coeffs Gale 1.6M 0.3215 0.4278

BTEC 194K 0.7233 0.6572

Dev1 60K 0.1030 0.1058

Dev2 60K 0.0743 0.1118

Dev3 60K 0.0994 0.1253 Dev 1.7M 0.1102 0.1763 ISI 43.7M 0.3726 0.2417

NewsWire 8.1M 0.1634 0.1053

TreeBank 0.4M 0.0323 0.0489

Table 4: Weights of the different bitexts. edge into the SMT system, e.g. giving low scores to word alignments that are "grammatically not reasonable". translation based on information retrieval. In EAMT, pages 133142. Philipp Koehn and et al. 2007. Moses: Open source toolkit for statistical machine translation. In Association for Computational Linguistics, demonstration session., pages 224227. Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224227. Association for Computational Linguistics. Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708717. M.J.D. Powell. 1994. A direct search optimization method that models the objective and constraint functions by linar interpolation. In In Advances in Optimization and Numerical Analysis, Proceedings of the sixth Workshop on Optimization and Numerical Analysis, Oaxaca, Mexico, volume 275, pages 5167. Kluwer Academic Publishers. Holger Schwenk and Jean Senellart. 2009. Translation model adaptation for an Arabic/French news translation system by lightlysupervised training. In MT Summit. Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT, pages 182189. Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation
398

Acknowledgments
This work has been partially funded by the European Commission under the project Euromatrix and by the Higher Education Commission(HEC) Pakistan as Overseas scholarship. We are very thankful to SYSTRAN who provided support for the Arabic tokenization.

References
Frank Vanden Berghen and Hugues Bersini. 2005. CONDOR, a new parallel, constrained extension of Powell's UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157175, September. Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for SMT self- enhancement. In Association for Computational Linguistics, pages 157160. Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling. In Second Workshop on SMT, pages 177180. George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128135. Association for Computational Linguistics. Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine

model adaptation using comparalble corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 857866. Nicola Ueffing. 2006. Using monolingual sourcce language data to improve MT performance. In IWSLT, pages 174181. Nicola Ueffing. 2007. Transductive learning for statistical machine translation. In Association for Computational Linguistics, pages 2532. Chong Ho Yu. 2003. Resampling methods: Concepts, applications, and justification. In Practical Assessment Research and Evaluation. Bing Zhao, Matthias Ech, and Stephen Vogal. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics.

399

An efficient and user-friendly tool for machine translation quality estimation
Kashif Shah , Marco Turchi , Lucia Specia
Department of Computer Science, University of Sheffield, UK {kashif.shah,l.specia}@sheffield.ac.uk Fondazione Bruno Kessler, University of Trento, Italy turchi@fbk.eu Abstract
We present a new version of Q U E ST  an open source framework for machine translation quality estimation  which brings a number of improvements: (i) it provides a Web interface and functionalities such that non-expert users, e.g. translators or lay-users of machine translations, can get quality predictions (or internal features of the framework) for translations without having to install the toolkit, obtain resources or build prediction models; (ii) it significantly improves over the previous runtime performance by keeping resources (such as language models) in memory; (iii) it provides an option for users to submit the source text only and automatically obtain translations from Bing Translator; (iv) it provides a ranking of multiple translations submitted by users for each source text according to their estimated quality. We exemplify the use of this new version through some experiments with the framework. Keywords: Machine Translation, Translation Evaluation, Translation Quality Estimation

1.

Introduction

Metrics to predict the quality of texts translated automatically by Machine Translation (MT) systems have become a necessity in many scenarios. These metrics, referred to as quality estimation (QE), or also confidence estimation, are aimed at MT systems in use. They consist in prediction models generally built using supervised machine learning algorithms from examples of source texts and their machine translations (i.e., no access to reference translations) described through a number of features and labelled for quality. The notion of "quality" in QE metrics is defined according to the application and represented by labels  post-editing effort, gisting reliability, etc.  and features  for example, a binary grammar checker feature will be important for fluency prediction, but less useful for gisting reliability prediction. A number of positive results have been reported in recent work in the field. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010; Avramidis, 2013), and highlighting sub-segments that need revision (Bach et al., 2011). For recent overviews of various algorithms and features we refer the reader

to the WMT12-13 editions of the shared task on QE (Callison-Burch et al., 2012; Bojar et al., 2013). Q U E ST (Specia et al., 2013) is an open-source framework for QE which provides a wide range of feature extractors from source and translation texts, as well as external resources and tools. These lead to an average of 150 features (depending on the language pair) and go from simple, language-independent features, to advanced, linguistically motivated features. They include features that rely on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced. In addition, Q U E ST integrates a wellknown machine learning toolkit, scikit-learn,1 and other algorithms that are known to perform well on this task, facilitating experiments with existing features and techniques for feature selection and model building. Q U E ST also provides documentation for users to add their own features and learning algorithms. However, Q U E ST is not directly usable by end-users, such as professional translators. The tasks of installing and configuring the toolkit, obtaining the necessary resources, and building new models from data require technical knowledge of natural language processing, machine translation and machine learning. In this paper we describe a number of improvements over the current version of Q U E ST which are meant to make it more accessible to non-expert users, as well as more efficient (i.e., faster). In particular, we provide
1

http://scikit-learn.org/

3560

a client-server architecture which allows users to access pre-built models and resources remotely through XML-RPC requests, and which is optimised for speed by keeping resources in memory (Section 2.); a Web interface where users can upload files with source and translation segments, with the possibility of getting translations from Bing Translator (Section 3.); and a ranking mechanism that provides a sorted list of multiple the options of translations given for each source segment based on their predicted quality (Section 4.).

more suitable to be embedded in a client-server framework, and to deal with any unseen sentence pair. Language Model Servers Some of the most effective features for QE require the computation of sentence level language model (LM) probabilities and perplexities. In general, effective LMs are obtained from large corpora, and can thus can be very large files as well. This implies that starting the LMs during the Q U E ST computation and having them running on the same machine can be problematic. To cope with these problems, multiple LMs can be distributed in various servers which can run independently from Q U E ST, in different machines. Within Q U E ST, the computation of LM scores is done in a client that, given a sentence, queries the LM server to get the relevant scores. In the initialisation step, the connections with the LM servers are established and a fake query is used to force the initialisation of the LMs. Client-Server Framework To conclude the adaptation, the new version of Q U E ST has been embedded in a server that allows connection to the feature extractor from different clients located in various machines. This wrapper links Q U E ST to external machines using sockets, while it is linked to Q U E ST using standard input and output streams. The new structure of Q U E ST is outlined in Figure 1. Offline Pre-processing Launching external software within Q U E ST, such as tokenization and true-casing, is time consuming. To mitigate this effect, Q U E ST can now easily deal with already pre-processed source and target sentences. These modifications have made Q U E ST slimmer and easier to be used. In particular, they have speeded up the feature extraction process allowing its use in an online scenario or as a part of a Web interface, as we discuss in the next Section.

2.

Client-server architecture

The adaptation of Q U E ST to the online scenario has required an upgrade of different components in the previous version. The main goals of such changes were to: i) allow the processing of one sentence pair at the time; ii) speed up the feature extraction; iii) reduce the usage of computational resources; iv) make Q U E ST easily accessible remotely. Code Refactoring The previous version of Q U E ST was designed to process text files containing multiple source and target sentences. This required the following steps to be performed at run time, before any feature could start being extracted: 1. Loading in memory of the main resources needed to extract features, such as a list of the n-grams from the MT training corpus, source and target language models, and bilingual dictionaries. 2. Pre-processing of the whole source and target files extracting information such as part-ofspeech tagging and language model probabilities. 3. Filtering of the main resources according to the source and target sentences to reduce the computational effort while extracting features. Only when these steps were completed for the entire input files, the extraction of features for each sentence pair could start. In the current version of Q U E ST, this structure has been refactored for efficiency when processing a large number of sentences, and to better fit the demand of the interface: processing one sentence pair at the time. Changes were necessary in the first two steps, with the last step being removed. Dealing with on the fly requests of predictions for a given sentence pair does not allow the pre-filtering of resources. This modification has made the resources stored in memory completely independent of the sentences to be processed. The loading in memory of the resources is now part of an initialisation step. This modification has increased the amount of memory required to store all resources, but on the other hand it made Q U E ST

3.

An interface to Q U E ST

In order to facilitate the use of Q U E ST by non-expert end-users, such as translators or users of online MT systems, we have developed a Web interface that allows users to access the tool remotely from a Web browser, without the need to understand the internal functioning of framework, nor to install/configure the tool or build models. It offers the following functionalities:  Features: Values for individual features describing the source and translation sentence, e.g. source and target length, LM scores, average translation ambiguity level of source words, etc.

3561

Figure 1: Client-Server skema.  Predictions: An estimated quality score for the translated sentence given the source sentence, produced using Support Vector Regression (SVR) and pre-built models for specific language pairs.  Ranking: The ranking of multiple translations submitted by the user for a given source sentence. This is done based on SVR quality predictions for each source-target sentence pair performed independently. The Web interface is developed using PHP and XMLRPC for communication across the main Q U E ST server and resource servers. For the convenience of users, we have also integrated the free Bing translation API to this Web interface.2 The pipeline of the framework accessed via its Web interface is the following:  User inputs a file containing source sentences only or tab separated source sentences and their translation(s)  as many translations as desired.  User selects the language pair and text type (domain, etc.).  File is uploaded to the Web server, and read line by line (sentence by sentence).  If the input file contains only source sentences, a request is sent to Bing's API with the selected target language.  Based on the choices (language pair, text type) selected by the user, an instance of Q U E ST with the appropriate prediction model and resources is triggered.
Please note that the free version only allows 2,000,000 characters to be translated per month per user.
2

 Q U E ST extracts the features by calling the Feature Extractor module. LMs, other resources and prediction models are already loaded into memory by a fake call.  Q U E ST generates a prediction for each sourcetarget combination by applying the prediction model for that language pair.  If the input file contains multiple translations for the same source sentence, Q U E ST ranks these translations. These functionalities require prediction models previously trained offline for each language pair of interest. Options to build models from examples of translations, quality scores and language resources will be added to the interface in the future.

4.

Experiments

Q U E ST has recently been benchmarked on a number of datasets (Shah et al., 2013b; Shah et al., 2013a). To make this paper self-contained, we provide experiments with models trained offline which are already available through the Web interface. We also present figures in terms of the running time of our models. Our experiments include two language pairs, i.e., French-English and German-English, and two different tasks: absolute quality scores prediction and ranking of up to five translation options. The data used and results for these tasks are given in Tables 1 and 2. For both tasks, a set of 17 well established baseline features was used.3 The language models and other resources were built using standard tools: SRILM (Stolcke, 2002) and GIZA++ (Och and Ney, 2003). SVR
This corresponds to those used by the baseline system in WMT12-13. The list of features can be found on http://www.quest.dcs.shef.ac.uk/quest_ files/features_blackbox_baseline_17.
3

3562

Method Mean Prediction

# Training 1,881 1,881

# Test 9,000 9,000

MAE 0.151 0.129

RMSE 0.201 0.171

Dataset Fr-En De-En

BING 1.1 1.1

FE 1.12 2.10

PR 1.17 1.51

FE + PR 2.29 3.61

Table 1: Absolute score prediction: datasets and results for Fr-En Method Random Prediction # Training 7,098 7,098 # Test 365 365 Kendall's T 0.04 0.16

Table 4: Response time in seconds  per sentence  with an online interface of various models (FE = Feature Extractor, PR = Prediction)

loads all models only once, clearly showing better performance. We have also tested the response time of these prebuilt models for each module in online Q U E ST, as shown in Table 4. These figures refer to running Q U E ST at a local host on a single core of machine Intel(R) Xeon(R) CPU E5-2620 0 @ 2.00GHz with 190GB of RAM. The response time for remote requests will depend upon the network speed. It is important to note the difference between response time for each of the dataset: The use of larger resources to extract features yields overall slower response time.

Table 2: Ranking of alternative translations: datasets and results for De-En

with radial basis function (RBF) kernel was used as learning algorithm, since it has been shown to perform well in previous work (Callison-Burch et al., 2012; Bojar et al., 2013). The optimisation of parameters was done using grid search. Both datasets are freely available. The French-English dataset is described in (Potet et al., 2012). It has 10,881 source sentences and their MT output and posteditions. We measure and estimate HTER scores between the MT and its post-edition. The first 1,881 sentences were used for training, and the rest for test. Performance was measured in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The German-English dataset is provided by the WMT13 shared task on QE, with the official training/test splits used.4 It has up to five alternative machine translations produced by different MT systems for each source sentence, which were ranked for quality by humans as part of the translation task in WMT08-WMT12. A baseline prediction model was trained using the rankings provided as absolute scores. This model was applied to each of the five alternative sentences and the predicted scores were used for ranking them. Performance was measured by comparing Q U E ST predictions to rankings performed by humans in terms of Kendall tau's correlation. Table 3 gives a comparison between online Q U E ST and its previous, offline version in term of cumulative response time for all sentence pairs in each our two test sets along with the sizes of the language resources used to extract features for each of them. These figures include the time for loading models and sentences and input/output processing. Offline Q U E ST needs to do this for each sentence pair, while the new version
http://www.statmt.org/wmt13/ quality-estimation-task.html
4

5.

Remarks

Q U E ST can be downloaded from http://www. quest.dcs.shef.ac.uk/. The Web interface can be accessed at http://www.quest.dcs. shef.ac.uk/QuEstClient_v1/test.php

6.

Acknowledgements

This work has received funding from the European Union's Seventh Framework Programme for research, technological development and demonstration under grant agreements no. 296347 (QTLaunchPad) and 2011.4.2-287688 (Matecate).

7.

References

E. Avramidis. 2013. Sentence-level ranking with quality estimation. Machine Translation, 28:120. N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: a method for measuring machine translation confidence. In ACL11, pages 211219, Portland. O. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In WMT13, pages 144, Sofia. C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In WMT12, pages 1051, Montr eal. Y. He, Y. Ma, J. van Genabith, and A. Way. 2010. Bridging SMT and TM with Translation Recommendation. In ACL10, pages 622630, Uppsala.

3563

Dataset Fr-En De-En

SRC-LM 40M 2G

TGT-LM 35M 1.1G

POS-LM 348K 980K

GIZA 16M 73M

NGRAM 8.6M 438M

Offline Q U E ST 2,021 (0.224) 110 (0.300)

Online Q U E ST 343 (0.038) 22 (0.060)

Table 3: Sizes of resources and cumulative response time in minutes for QuEst offline vs QuEst online for all sentence pairs (and per sentence pair) on each of the two test sets. The resources used to extract the features are source and target 3-gram language models (SRC-LM and TGT-LM), part-of-speech tag source language model (POS-LM), source-target Giza++ lexical table (GIZA), and raw counts of 1-3 grams in a corpus of the source language (NGRAM).

F. Josef Och and H. Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):1951. M. Potet, E. Esperana-Rodier, L. Besacier, and H. Blanchon. 2012. Collection of a large database of french-english smt output corrections. In LREC12, Istanbul, Turkey. K. Shah, E. Avramidis, E Bic  ici, and L Specia. 2013a. Quest - design, implementation and extensions of a framework for machine translation quality estimation. Prague Bull. Math. Linguistics, 100:1930. K. Shah, T. Cohn, and L. Specia. 2013b. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In Proceedings of MT Summit XIV, pages 167174. R. Soricut and A. Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In ACL11, pages 612621, Uppsala. L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. 2009. Estimating the SentenceLevel Quality of Machine Translation Systems. In EAMT09, pages 2837, Barcelona. L. Specia, D. Raj, and M. Turchi. 2010. Machine translation evaluation versus quality estimation. Machine Translation, 24(1):3950. L. Specia, K. Shah, J.G.C. de Souza, and T. Cohn. 2013. Quest - a translation quality estimation framework. In 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL-2013, pages 7984, Sofia, Bulgaria. L. Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In EAMT11, pages 7380, Leuven. A. Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proc. Intl. Conf. on Spoken Language Processing, pages 901904, Denver, CO.

3564

Parametric Weighting of Parallel Data for Statistical Machine Translation
Kashif Shah, Lo ic Barrault, Holger Schwenk LIUM, University of Le Mans Le Mans, France. FirstName.LastName@lium.univ-lemans.fr

Abstract
During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the official WMT'11 task, translating from English to French. Our method achieves improvements of about 0.6 points BLEU on the test set with respect to a system trained on data without any weighting.

a particular translation task may vary quite a lot. Nevertheless, the standard procedure is to concatenate all available parallel data, to perform word alignment using GIZA++ (Och and Ney, 2000) and to extract and score the phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors:  the data may come from different sources that are more or less relevant to the translation task (in-domain versus out-of-domain data).  more generally, the topic or genre of the data may be more or less relevant.  the data may be of different quality (carefully performed human translations versus automatically crawled and aligned data).  the recency of the data with respect to the task may have an influence. This is of interest in the news domain where named entities, etc change over time. There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it does not

1

Introduction

Statistical machine translation (SMT) systems are based on two types of resources: monolingual data to build a language model (LM) and bilingual data  also called bitexts  to train the translation model (TM). The parallel data often comes from different sources, e.g. Europarl, UN, in-domain data in limited amounts, data crawled from the Internet or even bitexts automatically extracted from comparable corpora. It seems obvious that the appropriateness and the usefulness of this parallel data for

1323
Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 13231331, Chiang Mai, Thailand, November 8  13, 2011. c 2011 AFNLP

Alignments obtained from time-stamped training parallel text 1996 1997 .......................... 2009 2010 1

e-.t
0.4 !t 0.05 0.09 0.45 Normalized weights .................................. Resampling with replacement

0.9 0.8

0.17

0.20

1996

1997

..........................

2009

2010

Resampled alignments used to build Phrase Table

Figure 1: Overview of the weighting scheme. The alignments are weighted by an exponential decay function, parameterized by . Resampling with replacement is used to create a new corpus (parts with higher weight will appear more often). The phrase table is built from this corpus using the standard procedure. require the calculation of additional sentence-level features. In our previous work Shah et al. (2010) we proposed a technique to weight heterogeneous data by weighted resampling of the alignments. The weights were numerically optimized on development data. Hardt and Elming (2010) has shown recency effect in terms of file-context and concluded that the data within the same file is of greater importance than the rest. Levenberg et al. (2010) proposed an incremental training procedure to deal with a continuous stream of parallel text. Word alignment was performed by the stepwise online EM algorithm and the phrase table was represented with suffix arrays. The authors showed that it is better to use parallel data close to the test data than all the available data. The research presented in this paper is the extension of our previous work Shah et al. (2010) to weight corpora by resampling and is inspired by the work of Levenberg et al. (2010) to consider the recency of the training data. In fact, we could split the training data into several parts over time scale and use our previous resampling approach Shah et al. (2010) to automatically optimize the weights of each time period. However, this approach does not seem to scale very well when the number of individual corpora increases. Numerical optimization of more than ten corpus weights would probably need a large number of iterations, each one consisting in the creation of a complete phrase table and its evaluation on the development data. The main idea of our work is to consider some kind of meta-weights for each part of the training data. Instead of numerically optimizing all the weights, these meta-weights only depend on few parameters that need to be optimized. Concretely, in this work we study the exponential decrease of the importance of parallel data in function of its temporal distance to the development and test data. The weighting of the parts is still done by resampling the alignments. However, our general approach is not limited to weighting the training data with respect to recency to the development and test data. Any other criterion could be used as long as it can be calculated by a parametric function, i.e. to measure the topic appropriateness.

2

Weighting Scheme

The main idea of our work is summarized in Figure 1. We consider that time information is available for the bitexts. If this is not the case, one can consider that the time advances sequentially with the lines in the file. First, the data is considered in parts according to the time information. In Figure 1, we group together all data within the same year, but any other granularity is possible (months, weeks, days, etc). Given the observation that more recent training data seems to be more

1324

important than older one, we apply an exponential decay function: e
-t

Time Stamped Parallel Text Target Texts

Bitexts Alignments Giza++ Time Information

(1)

SRILM

where  is the decay factor and t is the discretized time distance (0 for most recent part, 1 for the next one, etc.). Therefore, our weighting scheme has only one parameter to be optimized. Following our previous work Shah et al. (2010), we resample the alignments in order to obtain a weighting of the bitexts according to their recency. The weight of each part of the bitexts is normalized (sum to one). The normalized weights represent the percentage of final aligned corpus that is originated from each part of the source corpus: word alignments corresponding to bitexts that are close to the test period will appear more often than the older ones in the final corpus. In addition, we considered the quality of the alignments during resampling, as described in our previous work (Shah et al., 2010). Alignments produced by GIZA++ have alignment scores associated with each sentence pair in both direction, i.e. source to target and target to source. Alignment scores have a very large dynamic range and are concentrated around very low values, consequently the following logarithmic mapping is applied in order to flatten the distribution:  ( ntrg asrc trg + log(  2
nsrc

Phrase Extraction and Scoring TM LM

New Alignments

Resampling with ! and
" given by Optimizer

Moses Decoder

Source

Target

BLEU

Optimization Loop

Figure 2: Architecture of SMT Weighting System. Algorithm 1 Weighting with Exponential Decay function using resampling 1: Determine word to word alignment with GIZA++ on concatenated bitexts. 2: Initialize  and  with equal weights. 3: while not Optimized do 4: Compute time-spans weights by eq. 1 5: Normalize weights 6: for i = 0 to #time-span do 7: proportion  required size1  weights[i] 8: j=0 9: while j < proportion do 10: Al  Random alignment 11: Alscore  normalized score of Al 12: Flatten Alscore with  13: T hreshold  rand[0, 1] 14: if Alscore > T hreshold then 15: keep it 16: j =j+1 17: end if 18: end while 19: end for 20: Create new resampled alignment file. 21: Extract phrases and build the phrase table. 22: Decode 23: Calculate the BLEU score on Dev 24: Update  and  25: end while Then, for each part, resampling with replacement is performed in order to select the required number of alignments and form the final corpus. The resampling is done as follows: for each alignment considered, a new random threshold is gen-



atrg src )

)

(2)

where a is the alignment score, n the size of a sentence and  a smoothing coefficient to optimize. We used these normalized alignment scores as confidence measurement for each sentence pair.

3

Description of the algorithm

The architecture is presented in Figure 2. The starting point is a parallel corpus. We performed word alignment in both directions using GIZA++. The corpus is then separated into several parts on the basis of a given time span. We performed experiments with different span sizes, namely year, month, week and day. The decaying function is scaled so that the range does not change when using different span sizes. A weighting coefficient obtained with the exponential decay function is then associated to each part.
1 required size depends upon the number of times we resample - see section 5.

1325

erated and compared to the alignment score. The alignment is kept only if its score is above the threshold. This ensures that all alignments have a chance to be selected, but this chance is proportional to its alignment score. Note that some alignments may appear several times, but this is exactly what is expected as it will increase the probability of certain phrase pairs which are supposed to be more related to the test data (in terms of recency) and of better quality. The smoothing and decay factors,  and  respectively, are optimized with a numerical optimizer called CONDOR (Berghen and Bersini, 2005). The procedure and steps involved in our weighting scheme are shown in algorithm 1.

Since we want to focus on the impact of the weighting scheme of the bitexts, we used the same language model for all systems. It has been trained with the SRILM toolkit (Stolcke, 2002) on the target side of all the training data. In addition, the weights of the feature functions were tuned once for the system that uses all the training data and then kept constant for all the subsequent experiments, i.e. no tuning of the feature functions weights is done during the optimization of the weighting coefficients  and . Table 1 presents the results of the systems trained on various parts of the available bitexts without using the proposed weighting scheme. The best performance is obtained when using all the data (55M words, BLEU=30.48), but almost the same BLEU score is obtained by using only the most recent part of the data (24M words, part Recent 2). However, if we use the same amount of data that is further away from the time period of the test data (25M words, part Ancient 2), we observe a significant loss in performance. These results are in agreement with the observations already described in (Levenberg et al., 2010). Using less data, but still close to the evaluation period (15M words, part Recent 1) results in a small loss in the BLEU score. The goal of the proposed weighting scheme is to be able to take advantage of all the data while giving more weight to recent data than to older one. By these means we are not obliged to disregard older parts of the data that may contain additional useful translations. If the weighting scheme does work correctly, we cannot perform worse than using all the data. Of course, we expect to achieve better results by finding the optimal weighting between recent and ancient data. The amount of data per year in the Europarl data can vary substantially in function of time period since it depends on the frequency and length of the sessions of the European Parliament. As an example Figure 4 shows the histogram of the data per year. One can ask which time granularity should be used to achieve best weights. Only one weight is given to each time span, consequently the span size will have an impact on the alignment selection process. Using smaller spans results in a more fine grained weighting scheme. We have tested different settings with different time spans to see whether the impact of weighting changes with the

4

Experimental evaluation

Our first experiments are based on the FrenchEnglish portion of the freely available timestamped Europarl data (Koehn, 2005) from April 1996 to December 2010. We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007), though our approach is equally applicable to any other approach based on alignments and could be used for any language pairs. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The coefficients of these feature functions are optimized by minimum error training. In the first experiments, the whole Europarl corpus was split into train, development and test as shown in Figure 3. The most recent 5K sentences are split into two sets of equal size, one for development and the other for testing. The remaining data was used as training bitexts to build the different systems.
Ancient 1 Recent 1 Dev=2.5k Test=2.5k

Figure 3: Data used to build the different systems (# sentences)

{
All

{ {
Ancient 2 Recent 2

{
500k 300k

200k

{
300k 500k

1326

Europarl # of sentences/words BLEU (on dev) BLEU (on test)

Ancient data Ancient 1 Ancient 2 500K/15M 800K/25M 29.84 30.08 29.30 29.43

Recent data Recent 1 Recent 2 500K/15M 800K/24M 30.80 31.09 30.32 30.44

All 1800K/55M 31.34 30.48

Table 1: BLEU scores obtained with systems trained on data coming from different time spans. Europarl Time span Optimized  BLEU (on dev) BLEU (on test) Weighting + alignment selection Days Weeks Months Years 0.0099 0.0109 0.0110 0.0130 31.73 31.82 31.75 31.80 30.94 30.97 30.92 30.98 Best+retune Years 0.0130 31.92 31.09

Table 2: Results in BLEU score after weighting. size of each span. The results are shown in Table 2. It is observed that all four systems obtained very similar results, which indicates that the size of the spans is not very important. One surprising observation is that the optimized decay factor for all time span sizes are really close to each other. The reason to this could be the scaling of the exponential decaying function based on the time span size. In fact scaled values ensure that the oldest data point get roughly the same value independent of using years, months or days as time span. Looking at the optimized values of  in Table 2, we can observe that the relative difference between recent and ancient data is rather small, i.e. the ancient data is still somehow important and cannot be neglected. By using years as time span, we obtain an improvement of +0.50 BLEU score on the test

Figure 5: Distribution of data after weighting

set compared to using all data without weighting (30.48  30.98). It is clear that recency has a positive impact on system performance, however, weighting properly the different parts gives better performance than using the most recent or all available data. Finally, the best system is retuned (feature functions weights) and an overall improvement of +0.61 in the BLEU score is observed on test set.

5

Discussion

Figure 4: Amount of data available in the Europarl corpus for each year

The optimal decay factor of approximately 0.01 actually leads to an almost linear decrease over time. The difference in the quantity of data taken from most recent and least recent data is only 1.4% (which still represent 200k sentences). Therefore, one could think that the weighting does not favor recent data that much. This is not the case as we can see in Figure 5 where the distribution of data used to build the adapted model is presented.

1327

Europarl BLEU (on dev) BLEU (on test)

Resampling only 31.36 30.51

Weighting only 31.69 30.84

alignment selection only 31.45 30.64

Table 3: Results in BLEU score with different settings. Example 1 A: Mr Ribeiro e Castro, we shall see all this in the Conference of Presidents. B: Mr Ribeiro e Castro, we will see all this at the Conference of Presidents. R: Mr Ribeiro e Castro, we will look at all these questions in the Conference of Presidents' meeting. Example 2 A: We shall most probably consider again lodge a complaint with the Court of Justice of the European Communities. B: We will most probably consider again to lodge a complaint to the European Court of Justice. R: Most probably we will again discuss renewed recourse to the European Court of Justice. Example 3 A: no Member State has not led to field trials as regards the BST . B: no Member State has led to tests on the ground as regards BST . R: No Member State has yet carried out field tests with BST . Table 4: Example translations produced by systems All (A) and Best+retune (B) versus reference (R) When comparing to Figure 4, the overall proportion of data coming from recent years is clearly bigger when using our resampling approach. This leads to different word choices while decoding. Note that resampling is performed several times to estimate and select the samples which better represent the target data set. The more often we resample, the closer we get to the true probability distribution. The required-size in algorithm 1 depends upon the number of times we resample. We resampled ten times in our experiments. It is also worth to note that, we keep the original training data along with resampled one. It ensures that no information is lost and the set of extracted phrase pairs remain the same - only the corresponding probability distributions in the phrase table are changed. In order to get more insight in our method, we separately performed the different techniques:  resampling the training data without weighting;  resampling the training data using weighting only (with respect to recency);  resampling the training data using alignment selection. These results are summarized in Table 3. Note that the first case does not correspond to duplicating the training data a certain amount of time (which would of course produce exactly the same phrase-table). Since we perform resampling with replacement, this procedure introduces some randomness which could be beneficial. According to our results, this is not the case: we obtained exactly the same BLEU scores on the dev and test data than with the standard training procedure. Weighting with respect to recency or alignment quality both slightly improve the BLEU, but not as much as both techniques together. The performance increase seems actually to be complementary. Some comparative examples between the translations produced by systems All and Best+retune versus the reference translations are given in Table 4. It was noticed that a lot of occurrences of "will" in the reference are actually translated into "shall" with system All whereas the correct word choice is made by the system Best+retune as shown in Example 1. This could be explained by the fact that recently the word "will" is more frequently seen in the training corpus and adapting the model by weighting the most recent data pro-

1328

WMT Task BLEU (on dev) BLEU (on test)

Baseline 26.08 28.16

Receny weighting + alignment selection 26.51 28.59

Recency weighting + alignment selection + relative importance 26.60 28.69

Table 5: Results in BLEU score after weighting on English to French WMT Task. duced correct translation. Actually, it was found that the word "will" is 10% more frequent in recent data (Recent 1) than in ancient data (Ancient 1) while the word "shall" is 2% less frequent. Another interesting example is Example 2, in which the correct name for the European Court of Justice is proposed by the adapted system unlike the system All which proposed Court of Justice of the European Communities. Actually, it appears that the Court of Justice of the European Communities is the former name of the European Court of Justice prior to December 2009. The use of recent data allows to correctly translate the named entities which can change over time. The correct translation proposed by System Best+retune could be observed in Example 3 because of alignment selection procedure. In our experiments, we assume that the test data is in present time (the usual case in a news translation system), and consequently we decrease the weight of the bitexts towards the past. This principle could be of course adapted to other scenarios. An alternative approach could be to directly use the time decay function as the count for each extracted phrase. However, resampling the alignments and changing the counts of extracted phrases is not exactly the same. Same phrase pairs could be extracted from different parallel sentences coming from different time spans. Furthermore, weighting the alignments with their scores has shown improvements in the BLEU score as presented in Table 3, but considering the alignment score at the phrase level is not straight forward. et al., 2011) i.e news-test09 and news-test10 respectively. We built English-French systems by using the Europarl and News-Commentary (NC) corpora, both contain news data over a long time period. For this set-up, there are three coefficients to optimize: the decay factor for Europarl 1 , the decay factor for the news-commentary texts 2 and a coefficient for the alignments . The Europarl corpus was divided into time span according to years and NC corpus was assumed to be sorted over time since time-stamp information was not available for the NC corpus. Remaining settings are kept same as mentioned in previous experiments to build the system Best+retune. The results are shown in Table 5. Finally, we considered the relative importance of the Europarl and NC corpora. For this, a weight is attached to each corpus which represents the percentage of the final aligned corpus that comes from each source corpus. These weights are also optimized on the development data using the same technique as proposed in our previous work (Shah et al., 2010). Using all these methods, we have achieved an overall improvement of approximately +0.5 BLEU on the development and test data, as shown in Table 5.

7

Conclusion and future work

6

Experiments on the WMT task

To further verify whether our results are robust beyond the narrow experimental conditions, we considered a task where the development and test data do not come from the same source than the bitexts. We took the official test sets of the 2011 WMT translation tasks as dev and test sets (Schwenk

In this paper, a parametric weighting technique along with resampling is proposed to weight the training data of the translation model of an SMT system. By using a parametric weighting function we circumvented the difficult problem to numerically optimize a large number of parameters. Using this formalism, we were able to weight the parallel training data according to the recency with respect to the period of the test data. By these means, the system can still take advantage of all data, in contrast to methods which only use a part of the available bitexts. We evaluated our approach on the Europarl corpus, translating from French into English and further tested it on official English to French WMT Task. A reasonable improvement in

1329

BLEU score on the test data was observed in comparison to using all the data or only the most recent one. We argue that weighting the training data with respect to its temporal closeness should be quite important for translating news material since word choice in this domain is rapidly changing. An interesting continuation of this work is to consider other criteria for weighting the corpora than the temporal distance. It is clear that recency is a relevant information and this could be associated with other features, e.g. thematic or linguistic distance. Also, this work can be included into a stream-based framework where new data is incorporated in an existing system by exponential growth function and making use of online retraining procedure as discussed in (Levenberg et al., 2010).

Christine Moran, Richard Zens, Chris Dyer, Ond rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL '07, pages 177180, Stroudsburg, PA, USA. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 7986, Phuket, Thailand. AAMT. Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT '10, pages 394402, Stroudsburg, PA, USA. Association for Computational Linguistics. Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708717. Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL '00, pages 440447, Stroudsburg, PA, USA. Association for Computational Linguistics. Holger Schwenk, Patrik Lambert, Lo ic Barrault, Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli, and Kashif Shah. 2011. Lium's smt machine translation systems for wmt 2011. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 464469, Edinburgh, Scotland. Association for Computational Linguistics. Kashif Shah, Lo ic Barrault, and Holger Schwenk. 2010. Translation model adaptation by resampling. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT '10, pages 392399,

Acknowledgments
This work has been partially funded by the European Commission under the project EuromatrixPlus, the French government under the project Cosmat and by an Overseas scholarship of Higher Education Pakistan. We would like to thank the unknown reviewers for their valuable comments.

References
Frank Vanden Berghen and Hugues Bersini. 2005. CONDOR, a new parallel, constrained extension of Powell's UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157175, September. George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP '10, pages 451 459, Stroudsburg, PA, USA. Association for Computational Linguistics. Daniel Hardt and Jakob Elming. 2010. Incremental re-training for post-editing smt. In The Ninth Conference of the Association for Machine Translation in the Americas 2010. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, 1330

Stroudsburg, PA, USA. Association for Computational Linguistics. Andreas Stolcke. 2002. Srilm--an extensible language modeling toolkit. In Proceesings of the 7th International Conference on Spoken Language Processing (ICSLP 2002, pages 901 904.

1331

QuEst - A translation quality estimation framework
Lucia Specia , Kashif Shah , Jose G. C. de Souza and Trevor Cohn Department of Computer Science University of Sheffield, UK {l.specia,kashif.shah,t.cohn}@sheffield.ac.uk Fondazione Bruno Kessler University of Trento, Italy desouza@fbk.eu Abstract
We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms. dence estimation, which we believe is a narrower term. A 6-week workshop on the topic at John Hopkins University in 2003 (Blatz et al., 2004) had as goal to estimate automatic metrics such as BLEU (Papineni et al., 2002) and WER. These metrics are difficult to interpret, particularly at the sentence-level, and results of their very many trials proved unsuccessful. The overall quality of MT was considerably lower at the time, and therefore pinpointing the very few good quality segments was a hard problem. No software nor datasets were made available after the workshop. A new surge of interest in the field started recently, motivated by the widespread used of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments similar to "fuzzy match scores" from translation memory systems is needed. QE work addresses this problem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. "Quality" is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al.,

1

Introduction

As Machine Translation (MT) systems become widely adopted both for gisting purposes and to produce professional quality translations, automatic methods are needed for predicting the quality of a translated segment. This is referred to as Quality Estimation (QE). Different from standard MT evaluation metrics, QE metrics do not have access to reference (human) translations; they are aimed at MT systems in use. QE has a number of applications, including:  Deciding which segments need revision by a translator (quality assurance);  Deciding whether a reader gets a reliable gist of the text;  Estimating how much effort it will be needed to post-edit a segment;  Selecting among alternative translations produced by different MT systems;  Deciding whether the translation can be used for self-training of MT systems. Work in QE for MT started in the early 2000's, inspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities. Back then it was called confi-

2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labourintensive. Different language pairs or optimisation against specific quality scores (e.g., post-editing time vs translation adequacy) can benefit from very different feature sets. Q U E ST, our framework for quality estimation, provides a wide range of feature extractors from source and translation texts and external resources and tools (Section 2). These go from simple, language-independent features, to advanced, linguistically motivated features. They include features that rely on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced (Section 2.1). In addition, by integrating a well-known machine learning toolkit, scikit-learn,1 and algorithms that are known to perform well on this task, Q U E ST provides a simple and effective way of experimenting with techniques for feature selection and model building, as well as parameter optimisation through grid search (Section 2.2). In Section 3 we present experiments using the framework with nine QE datasets. In addition to providing a practical platform for quality estimation, by freeing researchers from feature engineering, Q U E ST will facilitate work on the learning aspect of the problem. Quality estimation poses several machine learning challenges, such as the fact that it can exploit a large, diverse, but often noisy set of information sources, with a relatively small number of annotated data points, and it relies on human annotations that are often inconsistent due to the subjectivity of the task (quality judgements). Moreover, QE is highly
1

non-linear: unlike many other problems in language processing, considerable improvements can be achieved using non-linear kernel techniques. Also, different applications for the quality predictions may benefit from different machine learning techniques, an aspect that has been mostly neglected so far. Finally, the framework will also facilitate research on ways of using quality predictions in novel extrinsic tasks, such as self-training of statistical machine translation systems, and for estimating quality in other text output applications such as text summarisation.

2

The Q U E ST framework

Q U E ST consists of two main modules: a feature extraction module and a machine learning module. The first module provides a number of feature extractors, including the most commonly used features in the literature and by systems submitted to the WMT12 shared task on QE (Callison-Burch et al., 2012). More than 15 researchers from 10 institutions contributed to it as part of the Q U E ST project.2 It is implemented in Java and provides abstract classes for features, resources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the source MT training corpus and language models of source and target. Configuration files are used to indicate the resources available and a list of features that should be extracted. The machine learning module provides scripts connecting the feature files with the scikit-learn toolkit. It also uses GPy, a Python toolkit for Gaussian Processes regression, which outperformed algorithms commonly used for the task such as SVM regressors. 2.1 Feature sets

In Figure 1 we show the types of features that can be extracted in Q U E ST. Although the text unit for which features are extracted can be of any length, most features are more suitable for sentences. Therefore, a "segment" here denotes a sentence. From the source segments Q U E ST can extract features that attempt to quantify the complexity
http://www.dcs.shef.ac.uk/~lucia/ projects/quest.html
2

http://scikit-learn.org/

Adequacy indicators

Source text

MT system

Translation

Complexity indicators

Confidence indicators

Fluency indicators

Figure 1: Families of features in Q U E ST. of translating those segments, or how unexpected they are given what is known to the MT system. Examples of features include:  number of tokens in the source segment;  language model (LM) probability of source segment using the source side of the parallel corpus used to train the MT system as LM;  percentage of source 13-grams observed in different frequency quartiles of the source side of the MT training corpus;  average number of translations per source word in the segment as given by IBM 1 model with probabilities thresholded in different ways. From the translated segments Q U E ST can extract features that attempt to measure the fluency of such translations. Examples of features include:  number of tokens in the target segment;  average number of occurrences of the target word within the target segment;  LM probability of target segment using a large corpus of the target language to build the LM. From the comparison between the source and target segments, Q U E ST can extract adequacy features, which attempt to measure whether the structure and meaning of the source are preserved in the translation. Some of these are based on word-alignment information as provided by GIZA++. Features include:  ratio of number of tokens in source and target segments;  ratio of brackets and punctuation symbols in source and target segments;  ratio of percentages of numbers, content- / non-content words in the source & target segments;  ratio of percentage of nouns/verbs/etc in the

source and target segments;  proportion of dependency relations between (aligned) constituents in source and target segments;  difference between the depth of the syntactic trees of the source and target segments;  difference between the number of PP/NP/VP/ADJP/ADVP/CONJP phrases in the source and target;  difference between the number of person/location/organization entities in source and target sentences;  proportion of person/location/organization entities in source aligned to the same type of entities in target segment;  percentage of direct object personal or possessive pronouns incorrectly translated. When available, information from the MT system used to produce the translations can be very useful, particularly for statistical machine translation (SMT). These features can provide an indication of the confidence of the MT system in the translations. They are called "glass-box" features, to distinguish them from MT system-independent, "black-box" features. To extract these features, Q U E ST assumes the output of Moses-like SMT systems, taking into account word- and phrasealignment information, a dump of the decoder's standard output (search graph information), global model score and feature values, n-best lists, etc. For other SMT systems, it can also take an XML file with relevant information. Examples of glassbox features include:  features and global score of the SMT system;  number of distinct hypotheses in the n-best list;  13-gram LM probabilities using translations in the n-best to train the LM;  average size of the target phrases;  proportion of pruned search graph nodes;  proportion of recombined graph nodes.

We note that some of these features are language-independent by definition (such as the confidence features), while others can be dependent on linguistic resources (such as POS taggers), or very language-specific, such as the incorrect translation of pronouns, which was designed for Arabic-English QE. Some word-level features have also been implemented: they include standard word posterior probabilities and n-gram probabilities for each tar-

get word. These can also be averaged across the whole sentence to provide sentence-level value. The complete list of features available is given as part of Q U E ST's documentation. At the current stage, the number of BB features varies from 80 to 123 depending on the language pair, while GB features go from 39 to 48 depending on the SMT system used (see Section 3). 2.2 Machine learning

Data WMT12 (en-es) EAMT11 (en-es) EAMT11 (fr-en) EAMT09-s1 -s4 (en-es) GALE11-s1 -s2 (ar-en)

Training 1,832 900 2,300 3,095 2,198

Test 422 64 225 906 387

Table 1: Number of sentences used for training and testing in our datasets.

Q U E ST provides a command-line interface module for the scikit-learn library implemented in Python. This module is completely independent from the feature extraction code and it uses the extracted feature sets to build QE models. The dependencies are the scikit-learn library and all its dependencies (such as NumPy3 and SciPy4 ). The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. The pipeline with feature selection and hyperparameter optimisation can be set using a configuration file. Currently, the module has an interface for Support Vector Regression (SVR), Support Vector Classification, and Lasso learning algorithms. They can be used in conjunction with the feature selection algorithms (Randomised Lasso and Randomised decision trees) and the grid search implementation of scikit-learn to fit an optimal model of a given dataset. Additionally, Q U E ST includes Gaussian Process (GP) regression (Rasmussen and Williams, 2006) using the GPy toolkit.5 GPs are an advanced machine learning framework incorporating Bayesian non-parametrics and kernel machines, and are widely regarded as state of the art for regression. Empirically we found the performance to be similar to SVR on most datasets, with slightly worse MAE and better RMSE.6 In contrast to SVR, inference in GP regression can be expressed analytically and the model hyperparameters optimised directly using gradient ascent, thus avoiding the need for costly grid search. This also makes the method very suitable for feature selection.
http://www.numpy.org/ http://www.scipy.org/ 5 https://github.com/SheffieldML/GPy 6 This follows from the optimisation objective: GPs use a quadratic loss (the log-likelihood of a Gaussian) compared to SVR which penalises absolute margin violations.
4 3

3

Benchmarking

In this section we benchmark Q U E ST on nine existing datasets using feature selection and learning algorithms known to perform well in the task. 3.1 Datasets The statistics of the datasets used in the experiments are shown in Table 1.7 WMT12 English-Spanish sentence translations produced by an SMT system and judged for post-editing effort in 1-5 (worst-best), taking a weighted average of three annotators. EAMT11 English-Spanish (EAMT11-en-es) and French-English (EAMT11-fr-en) sentence translations judged for post-editing effort in 1-4. EAMT09 English sentences translated by four SMT systems into Spanish and scored for postediting effort in 1-4. Systems are denoted by s1 -s4 . GALE11 Arabic sentences translated by two SMT systems into English and scored for adequacy in 1-4. Systems are denoted by s1 -s2 . 3.2 Settings Amongst the various learning algorithms available in Q U E ST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been shown to perform very well in this task (Callison-Burch et al., 2012). The optimisation of parameters is done with grid search using the following ranges of values:  penalty parameter C : [1, 10, 10]  : [0.0001, 0.1, 10]  : [0.1, 0.2, 10] where elements in list denote beginning, end and number of samples to generate, respectively. For feature selection, we have experimented with two techniques: Randomised Lasso and
7 The datasets can be downloaded from http://www. dcs.shef.ac.uk/~lucia/resources.html

Gaussian Processes. Randomised Lasso (Meinshausen and B uhlmann, 2010) repeatedly resamples the training data and fits a Lasso regression model on each sample. A feature is said to be selected if it was selected (i.e., assigned a non-zero weight) in at least 25% of the samples (we do this 1000 times). This strategy improves the robustness of Lasso in the presence of high dimensional and correlated inputs. Feature selection with Gaussian Processes is done by fitting per-feature RBF widths (also known as the automatic relevance determination kernel). The RBF width denotes the importance of a feature, the narrower the RBF the more important a change in the feature value is to the model prediction. To make the results comparable with our baseline systems we select the 17 top ranked features and then train a SVR on these features.8 As feature sets, we select all features available in Q U E ST for each of our datasets. We differentiate between black-box (BB) and glass-box (GB) features, as only BB are available for all datasets (we did not have access to the MT systems that produced the other datasets). For the WMT12 and GALE11 datasets, we experimented with both BB and GB features. For each dataset we build four systems:  BL: 17 baseline features that performed well across languages in previous work and were used as baseline in the WMT12 QE task.  AF: All features available for dataset.  FS: Feature selection for automatic ranking and selection of top features with:  RL: Randomised Lasso.  GP: Gaussian Process. Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to evaluate the models. 3.3 Results

Dataset WMT12

EAMT11(en-es)

EAMT11(fr-en)

EAMT09-s1

EAMT09-s2

EAMT09-s3

EAMT09-s4

GALE11-s1

GALE11-s2

System BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP) BL AF FS(RL) FS(GP)

#feats. 17 80 69 17 17 80 29 17 17 80 65 17 17 80 73 17 17 80 59 17 17 80 67 17 17 80 40 17 17 123 56 17 17 123 54 17

MAE 0.6802 0.6703 0.6628 0.6537 0.4867 0.4696 0.4657 0.4640 0.4387 0.4275 0.4266 0.4240 0.5294 0.5235 0.5190 0.5195 0.4604 0.4734 0.4601 0.4610 0.5321 0.5437 0.5338 0.5320 0.3583 0.3569 0.3554 0.3560 0.5456 0.5359 0.5358 0.5410 0.5532 0.5381 0.5369 0.5424

RMSE 0.8192 0.8373 0.8107 0.8014 0.6288 0.5438 0.5424 0.5420 0.6357 0.6211 0.6196 0.6189 0.6643 0.6558 0.6516 0.6511 0.5856 0.5973 0.5837 0.5825 0.6643 0.6827 0.6627 0.6630 0.4953 0.5000 0.4995 0.4949 0.6905 0.6665 0.6649 0.6721 0.7177 0.6933 0.6955 0.6999

Table 2: Results with BB features.
Dataset WMT12 GALE11-s1 System AF FS(RL) FS(GP) AF FS(RL) FS(GP) AF FS(RL) FS(GP) #feats. 47 26 17 39 46 17 48 46 17 MAE 0.7036 0.6821 0.6771 0.5720 0.5691 0.5711 0.5510 0.5512 0.5501 RMSE 0.8476 0.8388 0.8308 0.7392 0.7388 0.7378 0.6977 0.6970 0.6978

GALE11-s2

Table 3: Results with GB features.
Dataset WMT12 GALE11-s1 System AF FS(RL) FS(GP) AF FS(RL) FS(GP) AF FS(RL) FS(GP) #feats. 127 26 17 162 69 17 171 82 17 MAE 0.7165 0.6601 0.6501 0.5437 0.5310 0.5370 0.5222 0.5152 0.5121 RMSE 0.8476 0.8098 0.7989 0.6741 0.6681 0.6701 0.6499 0.6421 0.6384

The error scores for all datasets with BB features are reported in Table 2, while Table 3 shows the results with GB features, and Table 4 the results with BB and GB features together. For each table and dataset, bold-faced figures are significantly better than all others (paired t-test with p  0.05). It can be seen from the results that adding more BB features (systems AF) improves the results in most cases as compared to the baseline systems
8 More features resulted in further performance gains on most tasks, with 2535 features giving the best results.

GALE11-s2

Table 4: Results with BB and GB features.

BL, however, in some cases the improvements are not significant. This behaviour is to be expected as adding more features may bring more relevant information, but at the same time it makes the representation more sparse and the learning prone to overfitting. In most cases, feature selection with both or either RL and GP improves over all features (AF). It should be noted that RL automatically selects the number of features used for training while FS(GP) was limited to selecting the top 17 features in order to make the results comparable with our baseline feature set. It is interesting to note that system FS(GP) outperformed the other systems in spite of using fewer features. This technique is promising as it reduces the time requirements and overall computational complexity for training the model, while achieving similar results compared to systems with many more features. Another interesting question is whether these feature selection techniques identify a common subset of features from the various datasets. The overall top ranked features are:  LM perplexities and log probabilities for source and target;  size of source and target sentences;  average number of possible translations of source words (IBM 1 with thresholds);  ratio of target by source lengths in words;  percentage of numbers in the target sentence;  percentage of distinct unigrams seen in the MT source training corpus. Interestingly, not all top ranked features are among the baseline 17 features which are reportedly best in literature. GB features on their own perform worse than BB features, but in all three datasets, the combination of GB and BB followed by feature selection resulted in significantly lower errors than using only BB features with feature selection, showing that the two features sets are complementary.

for any purposes, including commercial. For preexisting code and resources, e.g., scikit-learn, GPy and Berkeley parser, their licenses apply, but features relying on these resources can be easily discarded if necessary.

Acknowledgments
This work was supported by the QuEst (EU FP7 PASCAL2 NoE, Harvest program) and QTLaunchPad (EU FP7 CSA No. 296347) projects.

References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: a method for measuring machine translation confidence. In ACL11, pages 211219, Portland. J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confidence Estimation for Machine Translation. In Coling04, pages 315321, Geneva. C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In WMT12, pages 1051, Montr eal. Y. He, Y. Ma, J. van Genabith, and A. Way. 2010. Bridging SMT and TM with Translation Recommendation. In ACL10, pages 622630, Uppsala. N. Meinshausen and P. B uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72:417473. K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL02, pages 311318, Philadelphia. C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press, Cambridge. R. Soricut and A. Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In ACL11, pages 612621, Uppsala. L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. 2009. Estimating the SentenceLevel Quality of Machine Translation Systems. In EAMT09, pages 2837, Barcelona. L. Specia, D. Raj, and M. Turchi. 2010. Machine translation evaluation versus quality estimation. Machine Translation, 24(1):3950. L. Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In EAMT11, pages 7380, Leuven.

4

Remarks

The source code for the framework, the datasets and extra resources can be downloaded from http://www.quest.dcs.shef.ac.uk/. The project is also set to receive contribution from interested researchers using a GitHub repository: https://github.com/lspecia/quest. The license for the Java code, Python and shell scripts is BSD, a permissive license with no restrictions on the use or extensions of the software

A General Framework to Weight Heterogeneous Parallel Data for Model Adaptation in Statistical Machine Translation
Kashif Shah, Lo ic Barrault, Holger Schwenk LIUM, University of Le Mans Le Mans, France. FirstName.LastName@lium.univ-lemans.fr

Abstract
The standard procedure to train the translation model of a phrase-based SMT system is to concatenate all available parallel data, to perform word alignment, to extract phrase pairs and to calculate translation probabilities by simple relative frequency. However, parallel data is quite inhomogeneous in many practical applications with respect to several factors like data source, alignment quality, appropriateness to the task, etc. We propose a general framework to take into account these factors during the calculation of the phrase-table, e.g. by better distributing the probability mass of the individual phrase pairs. No additional feature functions are needed. We report results on two well-known tasks: the IWSLT'11 and WMT'11 evaluations, in both conditions translating from English to French. We give detailed results for different functions to weight the bitexts. Our best systems improve a strong baseline by up to one BLEU point without any impact on the computational complexity during training or decoding.

1

Introduction

Two type of resources are needed to build statistical machine translation (SMT) systems: monolingual texts to build a language model (LM) and bilingual texts  also called bitexts  to train the translation model (TM). While huge amounts of monolingual data are available in large variety of domains, parallel data is a sparse resource in many domains. The parallel data often comes from international organizations, e.g. Europarl, UN, or data crawled from the Internet or even bitexts automatically extracted from comparable corpora. Usually, parallel data really relevant to the translation task

is only available in limited amount. The performance of an SMT system for a particular translation task heavily depends upon the appropriateness and usefulness of the data used to build the models. It is a common practice to concatenate all available parallel data, to perform word alignment, to extract phrase pairs and to calculate translation probabilities by simple relative frequency. The parallel data is incorrectly assumed to be uniform with respect to several aspects. It seem obvious that not all available bitexts are relevant to the translation task, usually called in-domain versus out-of domain data. Even within one corpus, eventually considered to be in-domain, there is no reason to assume that all the parallel sentences are equally appropriate. The genre of the data may be also different, for instance, scientific text is translated with the models trained mainly on news data. Similarly, the quality of the data may differ when considering human translations versus automatically crawled data from the web. Moreover, in certain domains it is worth to consider the temporal distance of the data with respect to the task also called recency effect. Considering all these factors, model adaptation is a topic of increasing interest and various techniques are proposed in literature. One way to adapt the translation model is to use mixture models (Civera and Juan, 2007; Zhao et al., 2004a; Foster and Kuhn, 2007; Koehn and Schroeder, 2007), or to perform self-enhancement (Ueffing, 2006; Ueffing, 2007; Chen et al., 2008), or more generally unsupervised-training (Schwenk, 2008; Bertoldi and Federico, 2009; Lambert et al., 2011; Bojar and Tamchyna, 2011). Most recently weighting the data is getting much attention from the research community. Various goodness scores extracted at different levels during the model training are considered to weight the data. The data with a higher goodness scores is given higher weights to have positive impact on transla-

tion quality. Matsoukas et al. (2009) proposed a technique in which they weighted each sentence in the training bitexts to optimize a discriminative function on a given tuning set. Sentence level features were extracted to estimate the weights that are relevant to the given task. The feature vectors were mapped to scalar weights (0, 1) which are then used to estimate probability with weighted counts. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instanceweighting model into a linear combination. Phillips and Brown (2011) trained the models with a second-order Taylor approximation of weighted translation instances and discount models on the basis of this approximation. Zhao et al. (2004b) rescore phrase translation pairs for statistical machine translation using tf.idf to encode the weights in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Huang and Xiang (2010) proposed a rescoring algorithm in which phrase pair features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to boost good phrase translations and bad translations are discarded. Shah et al. (2010) proposed a technique to weight heterogeneous data by weighted resampling of the alignments. In an extended work, the same authors proposed to consider meta-weights for each part of the training data (Shah et al., 2011). The work proposed in this paper is an extension and generalization of several ideas proposed in previous works such as weighted counts with goodness scores. However our proposed framework gives the flexibility to inject the goodness scores in a unified formulation calculated at various levels. It is based on the following principles:  the use of a set of "quality measures" at different levels: weights for each corpus (or data source) and for each individual sentence in the bitexts.  no additional feature functions to express the quality or appropriateness of certain phrase pairs, but we modify only the existing phrase probabilities. By these means, we don't have to deal with the additional complexity of de-

coding and optimizing the weights of many feature functions.  resampling the bitexts or alignments is computationally expensive for large corpora since the resampled data is ideally much bigger than the original one. Instead, we integrate the various weighting schemes directly in the calculation of the translation probabilities.  our approach has only a small number of parameter to optimize. We unified several ideas into one efficient and simple framework that can be easily used. We clearly distinguished between weighting at different levels i.e corpus level and sentence level, contrary to the approach discussed in (Matsoukas et al., 2009). Moreover, they used a neural network to map the sentence level feature scores to a single weight. Our approach does not need any such mapping of features, goodness scores are directly used to produce weighted counts. The proposed framework doesn't depend on how these weights or goodness scores are calculated - one can use any measure which predicts the relevance of the training data to a given domain. Further, the proposed technique has the ability to take into account the goodness scores extracted at any level i.e corpus level, sentence level, word-to-word alignment level or even phrase level. The rest of the paper is organized as follows. In the next section we present in detail the architecture of our approach. Experimental results for IWSLT'11 and WMT'11 task are summarized and discussed in section 3. The paper concludes with a discussion and perspective on this work.

2

Architecture of our approach

In the following we will first summarize how the phrase-table is calculated in the popular Moses SMT toolkit. Each research team has its own (undocumented) heuristics, but we assume that the basic procedure is very similar for most phrasebased systems. To formalize our ideas, let us assume that we translate a sentence from the source language s = si . . . sI to the target language ~ is a sequence of t = ti . . . tJ . A phrase s ~ or t one or more words in the source or target language language respectively. The phrase table ~). Note is a large collection of phrase pairs (~ s, t that each source phrase s ~i generally has several ~ij . For each phrase pair (~ ~), we translations t s, t

usually have several probabilities used to weight it. Moses uses four probabilities: the forward ~|s phrase-translation probability P (t ~), the back~), and ward phrase-translation probability P (~ s|t two lexical probabilities, again in the forward and backward direction. These probabilities are used in the standard log-linear model as feature functions fi (s, t): t = arg max
t i

above procedure. For this we modified the tool memscore (Hardmeier, 2010). In practice, we also need to adapt step 4 since we need to keep track for each phrase pair from which corpus it was extracted and what are the scores of the corresponding sentence. 2.1 Standard phrase probabilities

The standard procedure to calculate the phrase probabilities is simple relative frequency: i log fi (s, t) (1) ~ij |s P (t ~i ) = ~ij ) Count(~ si , t ~ik ) Count(~ si , t
k

(2)

Moses uses in total fourteen feature functions: the above mentioned four scores for the phrases, a phrase and word penalty, six scores for the lexicalized distortion model, a language model score and a distance based reordering model. The phrase-table itself is created by the following procedure 1. collect parallel training data 2. eventually discard sentence pairs that are too long or which have a large length difference 3. run Giza++ on this data in both directions (source-to-target and target-to-source) 4. use some heuristics to symmetrize the alignments in both directions, e.g. the so-called grow-diagonal-... (Koehn et al., 2003) and extract a list of phrases 5. calculate the lexical probabilities ~|s 6. calculate the phrase probabilities P (t ~) and ~ P (~ s|t). 7. create the phrase table by merging the forward and backward probabilities In our approach we only modify the way how ~|s the phrase translations probabilities P (t ~) and ~) are calculated. The goal is to increase the P (~ s|t probability of phrase pairs which we believe to be more important for the considered task, to be more reliable, etc; and consequently, to down weight those which should be used less often. It is important to point out that our phrase table has exactly the same number of entries as the original one and that we do not add more feature functions. Currently, we do not modify the lexical scores of each phrase pair, but we will investigate this in the future. In summary, we only modify step 6 in the

The memscore tool also implements various smoothing methods such as Witten-Bell, KneserNey discounting etc. but to the best of our knowledge, their eventual benefit was not extensively studied and these smoothing techniques are not widely used. In any case, the calculation of the phrase probabilities does not consider from which corpus the phrase was extracted, or more generally, any kind of weight that was attached to the originating sentence. This can obviously lead to wrong probability distributions. As a simple example we can con~ij which appears a cousider a phrase pair psi , t ple of times in the in-domain corpus, and which provides the correct translation for the task, and ~ik which appears many another phrase pair psi , t times in a (larger) out of-domain corpus. This wrong translation will wrongly get a higher probability when relative frequency estimates are used (or any of the standard smoothing techniques). A similar argument holds at the sentence or even phrase level, for instance even a generally indomain corpus can contain few sentences which are out-of topic, badly aligned, etc. 2.2 Weighted phrase probabilities

We have modified the memscore tool in order to take into account a weight attached to each corpus and let us assume that we have the following information on our parallel training data:  the parallel data can be organized into C different parts. In most of the cases, we will use the source of the data to partition it, e.g. Europarl, United Nations, web-crawled, but one could also use some kind of clustering algorithm. We associate the weight wc , to each

ness scores. Any value that expresses the appropriateness of the corpus and sentence with respect to the task can be used. In the following we outline  a set of S "goodness scores" qs (si , ti ), s = some possibilities which were used in our experi1 . . . S for each parallel sentence pair ments. (si , ti ), i = 1 . . . L where L is the numWeighting parallel corpora was already invesber of parallel sentences. Again, we will tigated previously in the literature. For instance delay for now how to produce those senShah et al. (2010) used a resampling technique tence scores. We keep track of these sento weight parallel corpora. They have proposed tence scores when extracting phrases. All the two methods to obtain the corpus weights: via LM phrases extracted from the same sentence obinterpolation and numerical optimization to maxitain the same phrase-level goodness scores mize the BLEU score on some development data. hs (sj , tj ), j = 1 . . . P where P S is the The second approach showed slightly better pernumber of extracted phrases. formance, but it is computationally quite expensive (a new phrase table must be build for each Using these notations we will calculate the optimization loop). Therefore, we decided to use phrase probability as follows. Let us first consider corpus weights obtained by LM interpolation in only the weights of the individual corpora. This is our experiments. The idea is to build a LM on the achieved by extending equation 2 as follows: source (or target) side of the bitexts, independently C for each corpus. There is a well known EM proce~ wc Countc (~ si , tij ) dure to linearly interpolate these individual LMs c =1 to minimize the perplexity on some development ~ij |s P (t ~i ) = C (3) data. The resulting corpus coefficients can be di~ik ) wc Countc (~ si , t rectly used to weight the parallel corpora. c=1 k Perplexity can also be used to weight each individual sentence. This was used to select a releThe equation 3 is identical as given in (Matvant subset of LM data (Axelrod et al., 2011) or soukas et al., 2009), where wc represents the bitexts (Moore and Lewis, 2010). In our case, we features-mapped to a weight calculated for each build a LM on the source side of the in-domain sentence by neural network. However in our case corpus and use this model to calculate the perplexit represents the direct weight for each corpus. If ity of each sentence in all the other corpora. Since all corpus weights are identical, equation 3 simlower perplexity represents "better" sentences, we plifies to the original formulation in equation 2. set q (si , ti ) to the inverse of the perplexity. It is Considering in addition the goodness scores at the important to note that our approach is a generalizasentence level, we will get: tion of data selection approaches: instead of doing a hard decision which data to keep to discard, we ( ) C S X Y keep all the sentences and attach a weight to each s ~ij )  ~ij ) wc Countc (~ si , t h si , t c,s (~ one (this weight could be zero in an extreme case). c=1 s=1 ~ij |s P (t ~i ) = C ( ) S X X Y It was also observed that parallel sentences s ~ik )  ~ik ) wc Countc (~ si , t h si , t c,s (~ which are closer to the test set period are more imc=1 s=1 k (4) portant than older ones (Hardt and Elming, 2010; where s is an additional parameter to weight Levenberg et al., 2010; Shah et al., 2011), in parthe different sentence goodness scores among each ticular when translating texts in the news domain. other. We implemented phrase probability calculaFollowing (Shah et al., 2011), we use an exponention according to equation 4 in the the memscore tial decay function: tool of Moses. q (si , ti ) = e-t i (5) 2.3 Calculation of the corpus weights and where  is the decay factor and t is the dissentence goodness scores cretized time distance (0 for most recent part, 1 Our theoretical framework and implementation is generic and does not depend on the exact calculation of the corpus weights or the sentence goodfor the next one, etc.). Finally, it was argued that the alignment score produced by Giza++ could be used as a mea-

corpus c=1 . . . C . We will discuss later how to obtain those weights.

Corpus-1

Corpus-2

Corpus-n

Target Texts SRILM

Bitexts Giza++ f Align. Score

Features Feature Scores

Alignments

Features

Phrase Extraction
Phrase Pairs

Features Syncronized with Phrases
Features

Phrase Pairs Counts corpus-1

Phrase Pairs Counts corpus-2

Phrase Pairs Counts corpus-n Phrase Pairs Counts

be also used. Finally, the phrase-translation probabilities are calculated according to equation 4 in forward and backward direction. The parameters of our approach  ,  and  along with wc are numerically optimized 1 . In this optimization loop we keep the weights of the feature functions constant, i.e. i in equation 1 (we use the ones of the standard system without weighted phrase translation probabilities). Eventually, these weights optimized using the standard MERT procedure once we have fixed the parameters of our approach. Corpus TED News-Commentary Europarl v6 ccb2 TOTAL En tokens 2.0 2.8 50.6 232.5 287.9 Fr tokens 2.2 3.3 56.2 272.6 334.3

LM Moses Decoder Scoring Normalized Feature scores

TM

Source Target

Source Texts LM Weights

BLEU

SRILM

Optimization Loop

Figure 1: Overall architecture of our approach. sure whether the phrases extracted from the corresponding sentence pair should be up- or downweighted. In order to ease comparison, we used the same equation as Shah et al. (2010): ) (6) where a is the alignment score, n the size of a sentence and  a smoothing coefficient to optimize. 2.4 Overall architecture  ( ntrg asrc trg + q (si , ti ) = log(  2
nsrc

Table 1: Size of parallel corpora (in millions) to build baseline systems for WMT and IWSLT Tasks.

3

Experimental evaluation



atrg src )

The overall architecture of our approach is given in figure 1. Suppose we have number of parallel corpora coming from various sources. First of all, sentence level goodness scores are calculated and synchronized with the parallel sentences. These sentences are concatenated to perform word-toword alignment in both directions using GIZA++. This is done on the concatenated sentences since GIZA++ may perform badly if some of the individual bitexts are rather small. Alignment scores corresponding to each sentence pair are added to the goodness scores file. Then, phrases are extracted and the goodness score q (si , ti ) is synchronized with the phrases. In the case that one phrase occurs in multiple sentences (this actually happens quite often), we use the arithmetic mean of the goodness scores in our experiments. The maximum or some other interpolation functions could

We have built several phrase-based systems using the Moses toolkit (Koehn et al., 2007). The scoring framework is implemented by extending the memory based scoring tool called memscore (Hardmeier, 2010) available in the Moses toolkit. In our system, fourteen feature functions are used. These feature functions include phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and phrase penalty, and the target language model. The MERT tool (Och, 2003) is used to tune the coefficients of these feature functions. The experiments are performed on two well-known evaluation tasks i.e. the 2011 WMT and IWSLT English/French evaluations. The corpora and their sizes used to build the systems for both these tasks are given in table 1. 3.1 Experiments on the WMT task

For the WMT task we used the official development sets of the 2011 WMT translation tasks, i.e news-test09 as development corpus and newstest10 as test corpus. We built English-French systems by using the time-stamped Europarl and
1

with CONDOR (Berghen and Bersini, 2005)

WMT Task Baseline System 1 System 2 System 3 System 4 System 5 System 6 System 7 System 8 System 9 System 10 System 11 System 12

Corpus weights yes

Alignment scores

Temporal distance

Perplexity

BLEU (on test) 28.16 28.41 28.21 28.35 28.56 28.55 28.60 28.61 28.79 28.65 28.67 28.89 29.11 (optimized)

yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes

yes yes

Table 2: BLEU scores obtained with systems trained with different goodness scores on WMT Task. news-commentary (nc) corpora. The LM is created by interpolating several language models trained separately on the target side of the bitexts and all available target language monolingual data (about 1.5G words). These individual language models are interpolated and merged into one huge model. The coefficient of the individual models are optimized using the usual EM procedure to minimize perplexity on the development data. Initial corpus weights for the bitexts were obtained by building another interpolated LM on the target side of the bitexts only. We explored the following goodness scores to weight the relevance of the bitexts and the individual sentences: corpus weights, alignment scores, recency of the data with respect to the test set period and the sentence perplexity in the target language with respect to an in-domain language model. The news-commentary (nc) corpus was used for that purpose. The time information provided with Europarl data is used to estimate recency feature. This information was not available for nc, so we considered the sentences in chronologically ordered with respect to temporal distance. The alignment scores provided by GIZA++ were normalized using equation 6. The results of the baseline system and various combinations of the different goodness scores are summarized in table 2. In order to get an idea which goodness score give best results, we have first performed experiments using default values for the parameters of the feature functions. For this purpose, we have used the values reported to be optimal in (Shah et al., 2011). The baseline system achieves a BLEU score of 28.16 on the test set. Each goodness score alone brought small improvements in the BLEU score (systems 14 in Table 2), the best being sentence perplexity (+0.4 BLEU). An interesting property of our approach is that the individual gains seem to add up when we use several goodness scores, for instance combining recency and sentence perplexity gives and improvement by 0.63 BLEU (system 8) while the individual improvements are only +0.19 and 0.40 respectively. Combining corpus weights and sentence perplexity is less useful, as expected, since sentence perplexity implicitly weights the corpora. This is in fact an improved corpus weighting with a finer granularity. Our best system was obtained when combining corpus weights, recency and sentence perplexity weighting (system 11). For this system only, we numerically optimized the weights wc ,  and  on the development set (see figure 1). The default and new weights are: weparl = 0.47714  0.32823 wnc = 0.52285  0.67121  = 0.01300  0.02102  = 0.14530  0.12901 as = 0.1  0.01289 td = 0.1  0.19201 ppl = 0.1  0.15451

IWSLT Task Baseline System 1 System 2 System 3 System 4 System 5 System 6 System 7 System 8 System 9

Corpus weights yes

Alignment scores

Perplexity

yes yes yes yes yes yes yes yes yes yes yes yes yes

yes yes yes

BLEU (on test) 26.34 26.61 26.41 26.77 26.51 26.86 26.99 (optimized) 26.81 26.91 27.07 (optimized)

Table 3: BLEU scores obtained with systems trained with different goodness scores on IWSLT Task. where x is the coefficient among alignment score (as), temporal distance (td) and perplexity (ppl). By these means, we get an overall improvement of roughly +1 BLEU score (28.16  29.11) on test set. It is important to stress that this system is trained on exactly the same data than the baseline system and that the phrase table contains the same phrase-pairs. Our approach only improves the forward and backward probability estimates ~|s ~). P (t ~) and P (~ s|t 3.2 Experiments on the IWLST task in-domain TED to calculate the sentence perplexities and the LM interpolation weights are used as corpus weights. The recency score was not used for this task since the test set of the TED corpus has no time information. We observed the same behavior as for the WMT task: each individual goodness score improves the BLEU score on the test set (systems 13), weighting by sentence perplexity being the best one (+0.43 BLEU). The best system is obtained when combining all three goodness scores, leading a BLEU score of 26.91 (system 8). Again, the numerical optimization of the weights of the feature functions achieves an additional small improvement, giving an overall improvement of 0.73 BLEU. The weight of the goodness scores are shown in table 4.

We performed the same type of experiments for the IWSLT task. The parallel training data was the in-domain TED corpus, the news-commentary corpus (nc) and a subset of the FrenchEnglish 109 Gigaword (internally called ccb2). The results for this task are summarize in Table 3. the official Dev and test sets of the 2011 IWSLT talk task are used. Initial experiments have shown that large parts of the ccb2 corpus are not relevant for this task (looking at the sentence perplexities). Therefore, we decided to only use a subset of this corpus, namely all the sentences with a perplexity lower than 70. This process preserve only 3% of the ccb2 data. The baseline system trained on this data achieves a BLEU score of 26.34 on the test data. Using all the data in ccb2 worsens significantly the results: the BLEU scores is 25.73. In principle, it is not necessary to select subsets of the parallel training data with our approach to weight sentences by perplexity, but this speeds up processing since we do not need to consider many sentences pairs with a very low weight. We perform a kind of pruning: all those sentences get a zero weight and are discarded right away. We used the LM build on the

4

Comparative Analysis

It is interesting to compare the impact of the different goodness scores considered. An interesting fact is that the trend is similar for both tasks. By comparing the results obtained with the various systems, we can observe that the corpus weights, used alone or in combination with other features, are always beneficial (by pairwise comparison of e.g. systems 2 and 5, systems 3 and 6 or systems 4 and 7 from WMT task). The average gain provided by such weighting is around 0.2. Those weights correspond to the LM interpolation coefficient optimized to minimize the perplexity on the development set. They are useful to weight a whole corpus and to ensure that the indomain corpus will globally receive higher weight than the other corpora.

IWSLT Task Default values Optimized

cted 0.74032 0.69192

cnc 0.17378 0.16982

cccb2.px70 0.08591 0.13831

 0.1 0.19251

ppl 0.1 0.18151

as 0.1 0.03118

Table 4: Weights on IWSLT Task (ppl=perlexity, as=alignment score). Sentence level perplexity is also always useful (compare e.g. systems 1 and 7 or systems 6 and 11 from WMT task). While one could think that this goodness scores is redundant with corpus weight, it does bring additional information about the relevance of the sentence. This can be explained by the fact that a globally out-of-domain corpus can contain a fraction of useful sentences while, on the contrary, an in-domain corpus may contain some less useful ones. This is part of the heterogeneous aspect of any corpus. The average gain of using sentence perplexity is almost 0.3 for the WMT task and 0.37 for IWSLT task. Concerning the alignment score, the results obtained are more mitigated (see e.g. the comparison between systems 2 and 5 on WMT and systems 2 and 4 from IWSLT task). The average gain is very low, and it is the only goodness score which sometimes decrease the BLEU score. The temporal distance has the expected behavior. When comparing systems 1 and 6, 3 and 8 or 4 and 11 from WMT Task, we can observe that an improvement of more than 0.2 is obtained. ness scores: weights for corpora coming from different sources and weights at the sentence level based on the quality of the GIZA++ alignments, the recency with respect to the test set period and task appropriateness measured by the perplexity with respect to an in-domain language model. Using each one of these goodness scores, improved the BLEU score with respect to a strong baseline. However, best results were obtained by using all the goodness scores. This yielded an overall improvement of almost 1 point BLEU for the WMT task and more than 0.7 BLEU on the IWSLT task. Future work will concentrate on other goodness scores. It would be interesting to compare the results with proposed goodness scores by integrating them directly into log-linear model as feature functions.

Acknowledgments
This work has been partially funded by the European Commission under the project EuromatrixPlus, the French government under the project COSMAT and by an overseas scholarship of Higher Education Commission (HEC) Pakistan. We would like to thank the unknown reviewers for their valuable comments.

5

Conclusion and future work

We have proposed a general framework to improve the phrase translation probabilities in a phrasebased SMT system. For this, we use a set of "goodness scores" at the corpus or sentence level. These scores are used to calculate forward and backward phrase translations probabilities which are better adapted to the task. Our framework and implementation is generic and does not depend on the exact calculation of the corpus weights or the sentences goodness scores. Any value that expresses the appropriateness of the corpus and sentence with respect to the task can be used. The adapted system has exactly the same time and space complexity as the baseline system since we do not modify the number of entries in the phrasetable or add additional features. Also, the training time is only slightly increased. We evaluated this approach on two well-known tasks: the 2011 WMT and IWSLT English/French evaluations. We have investigated several good-

References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo indomain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics. Frank Vanden Berghen and Hugues Bersini. 2005. CONDOR, a new parallel, constrained extension of Powell's UOBYQA algorithm: Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181:157175, September. Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine

translation. In Fourth Workshop on SMT, pages 182189. Ondrej Bojar and Ale s Tamchyna. 2011. Improving translation model by monolingual data. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 330336, Edinburgh, Scotland, July. Association for Computational Linguistics. Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008. Exploiting n-best hypotheses for SMT self- enhancement. In Association for Computational Linguistics, pages 157160. Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling. In Second Workshop on SMT, pages 177180. George Foster and Roland Kuhn. 2007. Mixture-model adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128135. Association for Computational Linguistics. George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP '10, pages 451 459, Stroudsburg, PA, USA. Association for Computational Linguistics. Christian Hardmeier. 2010. Fast and extensible phrase scoring for statistical machine translation. The Prague Bulletin of Mathematical Linguistics, pages 8796. Daniel Hardt and Jakob Elming. 2010. Incremental re-training for post-editing smt. In The Ninth Conference of the Association for Machine Translation in the Americas 2010. Fei Huang and Bing Xiang. 2010. Feature-rich discriminative phrase rescoring for smt. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING '10, pages 492500, Stroudsburg, PA, USA. Association for Computational Linguistics. Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical

machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224227. Association for Computational Linguistics. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on ` Human Language Technology - Volume 1, NAACL '03, pages 4854, Stroudsburg, PA, USA. Association for Computational Linguistics. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond rej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL '07, pages 177180, Stroudsburg, PA, USA. Association for Computational Linguistics. Patrik Lambert, Holger Schwenk, Christophe Servan, and Sadaf Abdul-Rauf. 2011. Investigations on translation model adaptation using monolingual data. In Sixth Workshop on SMT. Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT '10, pages 394402, Stroudsburg, PA, USA. Association for Computational Linguistics. Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708717. Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort '10, pages 220

224, Stroudsburg, PA, USA. Association for Computational Linguistics. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL '03, pages 160167, Stroudsburg, PA, USA. Association for Computational Linguistics. Aaron B. Phillips and Ralf D. Brown. 2011. Training machine translation with a secondorder taylor approximation of weighted translation instances. In Machine Translation Summit XIII. Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT, pages 182189. Kashif Shah, Lo ic Barrault, and Holger Schwenk. 2010. Translation model adaptation by resampling. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT '10, pages 392399, Stroudsburg, PA, USA. Association for Computational Linguistics. Kashif Shah, Lo ic Barrault, and Holger Schwenk. 2011. Parametric weighting of parallel data for statistical machine translation. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 13231331, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing. Nicola Ueffing. 2006. Using monolingual source language data to improve MT performance. In IWSLT, pages 174181. Nicola Ueffing. 2007. Transductive learning for statistical machine translation. In Association for Computational Linguistics, pages 2532. Bing Zhao, Matthias Eck, and Stephan Vogel. 2004a. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the 20th international conference on Computational Linguistics. Association for Computational Linguistics.

Bing Zhao, Stephan Vogel, Matthias Eck, and Alex Waibel. 2004b. Phrase pair rescoring with term weighting for statistical machine translatio. In EMNLP, pages 206213.

SHEF-Lite: When Less is More for Translation Quality Estimation
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia Department of Computer Science University of Sheffield Sheffield, United Kingdom {debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk Abstract
We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation (subtasks 1.1 and 1.3). Our submissions use the framework of Gaussian Processes to investigate lightweight approaches for this problem. We focus on two approaches, one based on feature selection and another based on active learning. Using only 25 (out of 160) features, our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only 25% of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive. the submissions by the University of Sheffield team. Our models are based on Gaussian Processes (GP) (Rasmussen and Williams, 2006), a non-parametric probabilistic framework. We explore the application of GP models in two contexts: 1) improving the prediction performance by applying a feature selection step based on optimised hyperparameters and 2) reducing the dataset size (and therefore the annotation effort) by performing Active Learning (AL). We submitted entries for two of the four proposed tasks. Task 1.1 focused on predicting HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 2254 EnglishSpanish news sentences translated by Moses (Koehn et al., 2007) and post-edited by a professional translator. The evaluation used a blind test set, measuring MAE (Mean Absolute Error) and RMSE (Root Mean Square Error), in the case of the scoring variant, and DeltaAvg and Spearman's rank correlation in the case of the ranking variant. Our submissions reached 1st (feature selection) and 2nd (active learning) places in the scoring variant, the task the models were optimised for, and outperformed the baseline by a large margin in the ranking variant. The aim of task 1.3 aimed at predicting postediting time using a dataset composed of 800 English-Spanish news sentences also translated by Moses but post-edited by five expert translators. Evaluation was done based on MAE and RMSE on a blind test set. For this task our models were not able to beat the baseline system, showing that more advanced modelling techniques should have been used for challenging quality annotation types and datasets such as this.

1

Introduction

The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Callison-burch et al., 2012). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT13 QE shared task defined a group of tasks related to QE. In this paper, we present 337

2

Features

In our experiments, we used a set of 160 features which are grouped into black box (BB) and glass box (GB) features. They were extracted using the

Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337342, Sofia, Bulgaria, August 8-9, 2013 c 2013 Association for Computational Linguistics

open source toolkit QuEst1 (Specia et al., 2013). We briefly describe them here, for a detailed description we refer the reader to the lists available on the QuEst website. The 112 BB features are based on source and target segments and attempt to quantify the source complexity, the target fluency and the sourcetarget adequacy. Examples of them include:  Word and n-gram based features:

 Source and target sentence intra-lingual mutual information;  Source-target sentence inter-lingual mutual information;  Geometric average of target word probabilities under a global lexicon model. The 48 GB features are based on information provided by the Moses decoder, and attempt to indicate the confidence of the system in producing the translation. They include:  Features and global score of the SMT model;  Number of distinct hypotheses in the n-best list;  13-gram LM probabilities using translations in the n-best to train the LM;  Average size of the target phrases;  Relative frequency of the words in the translation in the n-best list;  Ratio of SMT model score of the top translation to the sum of the scores of all hypothesis in the n-best list;  Average size of hypotheses in the n-best list;  N-best list density (vocabulary size / average sentence length);  Fertility of the words in the source sentence compared to the n-best list in terms of words (vocabulary size / source sentence length);  Edit distance of the current hypothesis to the centre hypothesis;  Proportion of pruned search graph nodes;  Proportion of recombined graph nodes.

 Number of tokens in source and target segments;  Language model (LM) probability of source and target segments;  Percentage of source 13-grams observed in different frequency quartiles of the source side of the MT training corpus;  Average number of translations per source word in the segment as given by IBM 1 model with probabilities thresholded in different ways.

 POS-based features:

 Ratio of percentage of nouns/verbs/etc in the source and target segments;  Ratio of punctuation symbols in source and target segments;  Percentage of direct object personal or possessive pronouns incorrectly translated.

 Syntactic features:

 Source and target Probabilistic Contextfree Grammar (PCFG) parse loglikelihood;  Source and target PCFG average confidence of all possible parse trees in the parser's n-best list;  Difference between the number of PP/NP/VP/ADJP/ADVP/CONJP phrases in the source and target;

3

Model

 Other features:

 Kullback-Leibler divergence of source and target topic model distributions;  Jensen-Shannon divergence of source and target topic model distributions;

Gaussian Processes are a Bayesian non-parametric machine learning framework considered the stateof-the-art for regression. They assume the presence of a latent function f : RF  R, which maps a vector x from feature space F to a scalar value. Formally, this function is drawn from a GP prior: f (x)  GP (0, k (x, x )) which is parameterized by a mean function (here, 0) and a covariance kernel function k (x, x ). Each 338

1

http://www.quest.dcs.shef.ac.uk

response value is then generated from the function evaluated at the corresponding input, yi = f (xi )+ 2 ) is added white-noise.  , where   N (0, n Prediction is formulated as a Bayesian inference under the posterior: p(y |x , D) = p(y |x , f )p(f |D)

All our models were trained using the GPy2 toolkit, an open source implementation of GPs written in Python. 3.1 Feature Selection

f

where x is a test input, y is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in:
2 -1 y  N (kT  (K + n I ) y, 2 -1 k (x , x ) - kT  ( K +  n I ) k )

where k = [k (x , x1 )k (x , x2 ) . . . k (x , xd )]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs. A nice property of this formulation is that y is actually a probability distribution, encoding the model uncertainty and making it possible to integrate it into subsequent processing. In this work, we used the variance values given by the model in an active learning setting, as explained in Section 4. The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we followed previous work on QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD):
2 exp - k ( x, x ) =  f

To perform feature selection, we followed the approach used in Shah et al. (2013) and ranked the features according to their learned length scales (from the lowest to the highest). The length scales of a feature can be interpreted as the relevance of such feature for the model. Therefore, the outcome of a GP model using an ARD kernel can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) ones. For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2. For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al., 2013), for which we had all the necessary resources available for the extraction. We selected the top 25 features for both models, based on empirical results found by Shah et al. (2013) for a number of datasets, and then retrained the GP using only the selected features.

4

Active Learning

1 2

F i=1

xi - xi li

2 is the cowhere F is the number of features, f variance magnitude and li > 0 are the feature length scales. The resulting model hyperparameters (SE vari2 , noise variance  2 and SE length scales l ) ance f i n were learned from data by maximising the model likelihood. In general, the likelihood function is non-convex and the optimisation procedure may lead to local optima. To avoid poor hyperparameter values due to this, we performed a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation.

Active Learning (AL) is a machine learning paradigm that let the learner decide which data it wants to learn from (Settles, 2010). The main goal of AL is to reduce the size of the dataset while keeping similar model performance (therefore reducing annotation costs). In previous work with 17 baseline features, we have shown that with only 30% of instances it is possible to achieve 99% of the full dataset performance in the case of the WMT12 QE dataset (Beck et al., 2013). To investigate if a reduced dataset can achieve competitive performance in a blind evaluation setting, we submitted an entry for both tasks 1.1 and 1.3 composed of models trained on a subset of instances selected using AL, and paired with feature selection. Our AL procedure starts with a model trained on a small number of randomly selected instances from the training set and then uses this model to query the remaining instances in the training set (our query pool). At every iteration, the model selects the more "informative" instance, asks an oracle for its true label (which in our case is already given in the dataset, and therefore we
2

http://sheffieldml.github.io/GPy/

339

only simulate AL) and then adds it to the training set. Our procedure started with 50 instances for task 1.1 and 20 instances for task 1.3, given its reduced training set size. We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks. As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008). This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:
U 

ID(x) = V ar(y |x) 

1 U

sim(x, x
u=1

(u)

)

The  parameter controls the relative importance of the density term. In our experiments, we set it to 1, giving equal weights to variance and density. The U term is the number of instances in the query pool. The variance values V ar(y |x) are given by the GP prediction while the similarity measure sim(x, x(u) ) is defined as the cosine distance between the feature vectors. In a real annotation setting, it is important to decide when to stop adding new instances to the training set. In this work, we used the confidence method proposed by Vlachos (2008). This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop. In our experiments, we used the average test set variance as the confidence measure. In his work, Vlachos (2008) showed a correlation between the confidence and test error, which motivates its use as a stop criterion. To check if this correlation also occurs in our task, we measure the confidence and test set error for task 1.1 using the WMT12 split (1832/422 instances). However, we observed a different behaviour in our experiments: Figure 1 shows that the confidence does not raise or drop according to the test error but it stabilises around a fixed value at the same point as the test error also stabilises. Therefore, instead of using the confidence drop as a stop criterion, we use the point where the confidence stabilises. In Figure 2 we can observe that the confidence curve for the WMT13 test set stabilises after 580 instances. We took that point as our stop criterion and used the first 580 selected instances as the AL dataset. 340

Figure 1: Test error and test confidence curves for HTER prediction (task 1.1) using the WMT12 training and test sets.

Figure 2: Test confidence for HTER prediction (task 1.1) using the official WMT13 training and test sets. We repeated the experiment with task 1.3, measuring the relationship between test confidence and error using a 700/100 instances split (shown on Figure 3). For this task, the curves did not follow the same behaviour: the confidence do not seem to stabilise at any point in the AL procedure. The same occurred when using the official training and test sets (shown on Figure 4). However, the MAE curve is quite flat, stabilising after about 100 sentences. This may simply be a consequence of the fact that our model is too simple for post-editing time prediction. Nevertheless, in order to analyse the performance of AL for this task we submitted an entry using the first 100 instances chosen by the AL procedure for training. The observed peaks in the confidence curves re-

SHEF-Lite-FULL SHEF-Lite-AL Baseline

Task 1.1 - Ranking DeltaAvg  Spearman  9.76 0.57 8.85 0.50 8.52 0.46

Task 1.1 - Scoring MAE  RMSE  12.42 15.74 13.02 17.03 14.81 18.22

Task 1.3 MAE  RMSE  55.91 103.11 64.62 99.09 51.93 93.36

Table 1: Submission results for tasks 1.1 and 1.3. The bold value shows a winning entry in the shared task. ing set, we believe that increasing the dataset size helps to tackle this problem. We plan to investigate this issue in more depth in future work. For both AL datasets we repeated the feature selection procedure explained in Section 3.1, retraining the models on the selected features.

5

Results

Figure 3: Test error and test confidence curves for post-editing time prediction (task 1.3) using a 700/100 split on the WMT13 training set.

Figure 4: Test confidence for post-editing time prediction (task 1.3) using the official WMT13 training and test sets. sult from steps where the hyperparameter optimisation got stuck at bad local optima. These de2 ,  2 ) to very generated results set the variances (f n high values, resulting in a model that considers all data as pure noise. Since this behaviour tends to disappear as more instances are added to the train341

Table 1 shows the results for both tasks. SHEFLite-FULL represents GP models trained on the full dataset (relative to each task) with a feature selection step. SHEF-Lite-AL corresponds to the same models trained on datasets obtained from each active learning procedure and followed by feature selection. For task 1.1, our submission SHEF-Lite-FULL was the winning system in the scoring subtask, and ranked third in the ranking subtask. These results show that GP models achieve the state of the art performance in QE. These are particularly positive results considering that there is room for improvement in the feature selection procedure to identify the optimal number of features to be selected. Results for task 1.3 were below the baseline, once again evidencing the fact that the noise model used in our experiments is probably too simple for postediting time prediction. Post-editing time is generally more prone to large variations and noise than HTER, especially when annotations are produced by multiple post-editors. Therefore we believe that kernels that encode more advanced noise models (such as the multi-task kernel used by Cohn and Specia (2013)) should be used for better performance. Another possible reason for that is the smaller set of features used for this task (blackbox features only). Our SHEF-Lite-AL submissions performed better than the baseline in both scoring and ranking in task 1.1, ranking 2nd place in the scoring subtask. Considering that the dataset is composed by only 25% of the full training set, these are very encouraging results in terms of reducing data an-

notation needs. We note however that these results are below those obtained with the full training set, but Figure 1 shows that it is possible to achieve the same or even better results with an AL dataset. Since the curves shown in Figure 1 were obtained using the full feature set, we believe that advanced feature selection strategies can help AL datasets to achieve better results.

Proceedings of the 20th Conference on Computational Linguistics, pages 315321. Chris Callison-burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of 7th Workshop on Statistical Machine Translation. Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of ACL (to appear). Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177180. Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press Cambridge. Burr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 10701079. Burr Settles. 2010. Active learning literature survey. Technical report. Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In Proceedings of MT Summit XIV (to appear). Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas. Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-Taylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of MT Summit XII. Lucia Specia, Kashif Shah, Jos e G. C. De Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. In Proceedings of ACL Demo Session (to appear). Andreas Vlachos. 2008. A stopping criterion for active learning. Computer Speech & Language, 22(3):295312, July.

6

Conclusions

The results obtained by our submissions confirm the potential of Gaussian Processes to become the state of the art approach for Quality Estimation. Our models were able to achieve the best performance in predicting HTER. They also offer the advantage of inferring a probability distribution for each prediction. These distributions provide richer information (like variance values) that can be useful, for example, in active learning settings. In the future, we plan to further investigate these models by devising more advanced kernels and feature selection methods. Specifically, we want to employ our feature set in a multi-task kernel setting, similar to the one proposed by Cohn and Specia (2013). These kernels have the power to model inter-annotator variance and noise, which can lead to better results in the prediction of post-editing time. We also plan to pursue better active learning procedures by investigating query methods specifically tailored for QE, as well as a better stop criteria. Our goal is to be able to reduce the dataset size significantly without hurting the performance of the model. This is specially interesting in the case of QE, since it is a very task-specific problem that may demand a large annotation effort.

Acknowledgments
This work was supported by funding from CNPq/Brazil (No. 237999/2012-9, Daniel Beck) and from the EU FP7-ICT QTLaunchPad project (No. 296347, Kashif Shah and Lucia Specia).

References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013. Reducing Annotation Effort for Quality Estimation via Active Learning. In Proceedings of ACL (to appear). John Blatz, Erin Fitzgerald, and George Foster. 2004. Confidence estimation for machine translation. In

342

Quality estimation for translation selection
Kashif Shah and Lucia Specia Department of Computer Science University of Sheffield S1 4DP, UK {kashif.shah,l.specia}@sheffield.ac.uk

Abstract
We describe experiments on quality estimation to select the best translation among multiple options for a given source sentence. We consider a realistic and challenging setting where the translation systems used are unknown, and no relative quality assessments are available for the training of prediction models. Our findings indicate that prediction errors are higher in this blind setting. However, these errors do not have a negative impact in performance when the predictions are used to select the best translation, compared to non-blind settings. This holds even when test conditions (text domains, MT systems) are different from model building conditions. In addition, we experiment with quality prediction for translations produced by both translation systems and human translators. Although the latter are on average of much higher quality, we show that automatically distinguishing the two types of translation is not a trivial problem.

1

Introduction

Quality Estimation (QE) [Blatz et al., 2004, Specia et al., 2009] has several applications in the context of Machine Translation (MT), considering the use of translations for both inbound (e.g. gisting) and outbound (e.g. post-editing) purposes. To date, research on quality estimation has been focusing mostly on predicting absolute single-sentence quality scores. However, for certain applications
c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND.

an absolute score may not be necessary. Our goal is to model quality estimation by contrasting the output of several translation sources for the same input sentence against each other. The outcome of this process is a ranking of alternative translations based on their predicted quality. For our application, we are only interested in the top-ranked translation, which could for example be provided to a human post-editor for revision. Previous research on this task has focused on ranking translations from multiple MT systems where system identifiers are known beforehand. Based on such identifiers, individual quality prediction models can be trained for each MT system [Specia et al., 2010], and the predicted (absolute) scores for translations of a given source sentence across multiple MT systems used to rank them. Alternatively, quality prediction models can be built to directly output a ranking of alternative translations based on training data annotated with relative quality scores, using for example pairwise ranking algorithms [Avramidis, 2013, Avramidis and Popovi c, 2013]. In this paper we model translation selection considering a scenario where translations are produced by multiple MT systems, but the identifiers of the MT systems are not given, i.e., we assume a blind setting where the sources of the translations are not known. While ranking approaches to system selection could also be used in this blind setting, they require training data labelled with comparative assessments of translations produced by multiple sources. In our experiments, we assume a more general scenario where the labelling of training data is produced for individual translation segments in absolute terms, independently and regardless of their origin. In addition, we also experiment with predicting the quality for human trans-

lations. Although human translations are on average of much higher quality than machine translations, we show that this is not always the case and that automatically distinguishing the two types of translation is not a trivial problem. We present experiments with four language pairs and various prediction models in blind and non-blind settings, as well as with the use of the resulting predictions for translation selection. We show that while prediction errors are higher in blind settings, this does not have a negative impact in performance when using predictions in the task of translation selection. Our best result in terms of the quality scores of the selected translation sets are obtained with prediction models where all available translations are polled together in a system-agnostic way. Finally, we show that these system-agnostic models have good performance when predicting quality for out-of-domain translations, produced by other MT systems.

Bic  ici [2013] uses language and MT system independent features to predict F1 scores with regression algorithms. A threshold for judging if two translations are equal over the predicted F1 scores was learned from data. Avramidis [2013] and Avramidis and Popovi c [2013] decompose rankings into pairwise decisions, with the best translation for each candidate pair predicted using logistic regression. A number of features of the source and target languages, including pseudo-references, are used. A similar pairwise ranking approach was used by Formiga et al. [2013], but with random forest classifiers.

3

Experimental settings

2

Related work

A handful of system ranking and selection techniques have been proposed in recent years. For an overview of various related approaches we refer the reader to the WMT13 shared task on QE [Bojar et al., 2013]. This shared task included a system ranking track aimed at predicting 5-way rankings for translations produced by five MT systems and ranked by humans for model bulding. All related work relies on either knowing the system identifiers or having access to relative rankings of translations at training time. MT system selection was first proposed by Specia et al. [2010]. QE models are trained independently for each MT system, and the translation option with highest prediction score is used. 77% of the sentences with the highest QE score also have the highest score according to humans. In contrast, 54% of accuracy was found when selecting translations from the best MT system on average. He et al. [2010] focus on the ranking between translations from either an MT system or a translation memory for post-editing. Classifiers showed promising results in selecting the option with the lowest estimated edit distance. Hildebrand and Vogel [2013] use an classic nbest list re-ranking approach based on predicting BLEU scores. A feature set where all features that are solely based on the source sentence were removed showed the best results.

Datasets Our datasets contain news domain texts in four language pairs (Table 1): English-Spanish (en-es), Spanish-English (es-en), English-German (en-de), and German-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based (RBMT) system and, for enes/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type (SMT, RBMT and hybrid - hereafter system2, system3, system4) which participated in the translation shared task, plus the additional professional translation given as reference (system1). These are the official datasets used for the WMT14 Task 1.1 on QE.1
Languages en-es en-de de-en es-en # Training Src/Tgt 954/3,816 350/1,400 350/1,050 350/1,050 # Test Src/Tgt 150/600 150/600 150/450 150/450

Table 1: Number of training and test source (Src) and target (Tgt) sentences. Each translation in this dataset has been labelled by a professional translator with [1-3] scores for "perceived" post-editing effort, where:  1 = perfect translation, no editing needed.  2 = near miss translation: maximum of 2-3 errors, and possibly additional errors that can be easily fixed (capitalisation, punctuation).  3 = very low quality translation, cannot be easily fixed.
1 http://www.statmt.org/wmt14/

quality-estimation-task.html

The distribution of true scores in both training and test sets is given in Figures 1 and 2, for each language pair, and for each language pair and translation source, respectively.
#Training##### 60%# 50%# 40%# 30%# 20%# 10%# {en-es-2}## {en-es-3}## {en-es-1}# {es-en-1}# {es-en-2}# {en-de-1}# {en-de-2}# {en-de-3}# {de-en-1}# {de-en-2}# {de-en-3}# {es-en-3}# 0%# #Test####

SVC, we consider the "one-against-all" approach for multi-class classification with all classes are weighted equally. Evaluation metrics To evaluate our models, we use standard metrics for regression (MAE: mean absolute error; RMSE: root mean squared error) and classification (precision, recall and F1). For each Table and dataset, bold-faced figures represent results that are significantly better (paired ttest with p  0.05) with respect to the baseline.

4

Classification experiments

Figure 1: Distribution of true scores by lang. pair. Out-of-domain test sets For three language pairs, we also experiment with out-of-domain test sets (Table 2) provided by translation companies (also made available by WMT14) and annotated in the same way as above by a translation company (i.e., one professional translator). These were generated using the companies' own source data (different domains than news), and own MT system (different from the three used in our main datasets).
ID LSP1 LSP2 LSP3 LSP4 LSP5 Languages en-es en-es en-de es-en es-en # Test 233 738 297 388 677

Table 2: Number of out-of-domain test sentences. Features We use the QuEst toolkit [Specia et al., 2013, Shah et al., 2013] to extract two feature sets for each dataset:  BL: 17 features used as baseline in the WMT shared tasks on QE.  AF: 80 common MT system-independent features (superset of BL). The resources used to extract these features (language models, etc.) are also available as part of the WMT14 shared task on QE. Learning algorithms We use the Support Vector Machines implementation within QuEst to perform either regression (SVR) or classification (SVC) with Radial Basis Function as kernel and parameters optimised using grid search. For

Our main motivation to use classifiers is the need to distinguish human from machine translations to isolate the former for the system selection task, since in most settings they are not available. We are also interested in measuring the performance of classification-based QE in system selection. In the experiments to distinguish human translations from machine translations, we pool all MT and human translations together for each language pair, and build binary classifiers where we label all human translations as 1, and all system translations as 0. Results are given in Table 3, where MC stands for "majority class". They show a large variation across language pairs, although MC is outperformed in all cases in terms of F1. The lower performance for en-es and en-de may be because here translations from three MT systems are put together, while for the remaining datasets, only two MT systems are available. Nevertheless, figures for en-es are substantially better than those for en-de. This could also be due to the fact that more high quality human translations are available for es-en and de-en (see Figure 2). On the the other hand, for language combination datasets where more low quality human translations or more high quality machine translations are found, distinguishing between these sets becomes a more difficult task. With similar classifiers (albeit different datasets), Gamon et al. [2005] reported as trivial the problem of distinguishing human translations from machine translations. Overall, our results could indicate that this is a harder problem nowadays than some years ago, possibly pointing in the direction that MT systems produce translations that are closer to human translation nowadays. Results with SVC in the task of classifying instances with 1-3 labels (including human translations) are shown in Table 4. The performance of

Training# 100%# 90%# 80%# 70%# 60%# 50%# 40%# 30%# 20%# 10%# 0%#

Test#

########{en0es0system101}### ########{en0es0system102}### ########{en0es0system103}### ###### #

######{es0en0system101}### ######{es0en0system102}### ######{es0en0system103}### ### #

######{de0en0system101}## ######{de0en0system102}## ######{de0en0system103}## #### #

######{en0de0system101}# ######{en0de0system102}# ######{en0de0system103}# ### #

#####{en0es0system201}### #####{en0es0system202}### #####{en0es0system203}### ### #

####{en0de0system401}### ####{en0de0system402}### ####{en0de0system403}### ##### # ##### #

####{de0en0system201}### ####{de0en0system202}### ####{de0en0system203}### ###### #

####{de0en0system301}### ####{de0en0system302}### ####{de0en0system303}### ###### # ##### #

####{en0es0system301}### ####{en0es0system302}### ####{en0es0system303}### ### #

####{en0es0system401}### ####{en0es0system402}### ####{en0es0system403}### ######### # ##### #

Figure 2: Distribution of true scores for each MT system and language pair.
System MC BL AF MC BL AF MC BL AF MC BL AF #feats 17 80 17 80 17 80 17 80 Precision 0.3041 0.3272 0.3281 0.5041 0.5420 0.5468 0.6541 0.7012 0.7188 0.7311 0.7665 0.7639 Recall 0.1316 0.1200 0.1193 0.2416 0.2321 0.2333 0.1521 0.1524 0.1533 0.3513 0.3651 0.3667 F1 0.1566 0.1756 0.1801 0.2961 0.3262 0.3271 0.2312 0.2561 0.2527 0.4625 0.4942 0.4954 System MC BL AF MC BL AF MC BL AF MC BL AF #feats 17 80 17 80 17 80 17 80 Precision 0.1521 0.1600 0.3401 0.1121 0.1248 0.1267 0.2911 0.3080 0.3092 0.1941 0.2075 0.2071 Recall 0.4231 0.4000 0.4316 0.3521 0.3533 0.3512 0.5561 0.5550 0.5542 0.4516 0.4555 0.4541 F1 0.2072 0.2285 0.3078 0.1672 0.1844 0.1851 0.4014 0.3961 0.3972 0.2677 0.2851 0.2855

en-de de-en en-es es-en

en-de de-en en-es es-en

Table 3: SVC to distinguish between human translations and machine translations (all MT systems). MC corresponds to always picking machine translation (most frequent) as label. the classifiers is compared to the standard baseline of the majority class in the training set (MC). The classifiers perform better than MC for all language pairs except en-es, particularly in terms of recall and F1. Since this dataset is significantly larger, the majority class is likely to be more representative of the general data distribution. Overall, the classification results are not very positive, and this corroborates the findings of previous work contrasting classification and regression [Specia et al., 2010]. Overall, the use of all features (AF) instead of baseline features (BL) only leads to slight improvements in some cases.

Table 4: SVC to predict 1-3 labels for each language pair, with all translations pooled together. MC corresponds to applying the most frequent class of the training set to all test instances. all MT systems (and humans) together (Table 5)  which would be comparable to the settings used to generate Table 4  against models trained on data from each MT system (or human translation) individually (i.e., system identifier known). For the latter, we consider two settings at test time:  The system (or human) used to produce the translation is unknown (Table 6 blind setting), and therefore all models are applied, one by one, to predict the quality of this translation and the average prediction is used.  The system (or human) is known and thus the model for the same translation system/human can be used for prediction (Table 6 non blind setting). These two variants may be relevant depending on the application scenario. We consider very realistic a scenario where system identifiers are known by developers at model building time, but unknown at test time, e.g. if QE is provided as a web ser-

5

Regression experiments

Here we train models to estimate a continuous score within [1,3], as opposed to discrete 1-3 scores. We compare prediction error for models trained (and tested) on pooled translations from

#####{es0en0system201}## #####{es0en0system202}## #####{es0en0system203}## ### #

####{es0en0system301}### ####{es0en0system302}### ####{es0en0system303}###

####{en0de0system201}## ####{en0de0system202}## ####{en0de0system203}##

#{en0de0system301}### #{en0de0system302}### #{en0de0system303}### ### #

en-de de-en en-es es-en

System Mean BL AF Mean BL AF Mean BL AF Mean BL AF

#feats 17 80 17 80 17 80 17 80

MAE 0.6831 0.6416 0.6303 0.6705 0.6524 0.6518 0.4585 0.5240 0.5092 0.5825 0.5736 0.5662

RMSE 0.7911 0.7620 0.7616 0.7979 0.7791 0.7682 0.6678 0.6590 0.6442 0.6718 0.6788 0.6663

Table 5: SVR to build predictions models for each language pair combination, with all translation sources (including human) pooled together.

vice with pre-trained models (Table 6). Users may request predictions using translations produced by any sources, and for out-of-domain data (Table 7). In all tables, Mean represents a strong baseline: assigning the mean of the training set labels to all test set instances. Comparing the two variants of the blind setting (Tables 5 - blind training and test; and Table 6, blind test only), we see that pooling the data from multiple translation systems for blind model training leads to significantly better results than training models for individual translation sources but testing them in blind settings. This is likely to be due to the larger quantities of data available in the pooled models. In fact, the best results are observed with en-es, the largest dataset overall. Comparing scores between blind versus nonblind test setting in Table 6, we observe a substantial difference in the scores for each of the individual translation system. This shows that the task is much more challenging when QE models are trained independently, but the identifiers of the systems producing the test instances are not known. There is also a considerable difference in the performance of individual models for different translation systems, which can be explained by the different distribution of scores (and also indicated by the performance of the Mean baseline). However, in general the prediction performance of the individual models seems less stable, and worse than the baseline in several cases. Interestingly, the individual models trained on human translations only (system1) do even worse than individual models for MT systems. This can be an indication that the features used for quality prediction are not sufficient to model human translations.

In all cases, the use of all features (AF) instead of baseline features (BL) comparable or better results. Table 7 shows the results for SVR models trained on pooled models for each language pair (i.e., models in Table 5) when applied to predict the quality of the out-of-domain datasets. This is an extremely challenging task, as the only constant between training and test data is the language pair. The text domain is different, and so are MT systems used to produce the translations. In addition, no human translation is available in the out-ofdomain test sets. Surprisingly, the prediction errors are low, even lower than those observed for the indomain test sets. This is true for all except two outof-domain test sets: LSP5 , which contains unusual texts (such as URLs and markup tags), and LSP2 . Manual inspection of these source and translation segments show many extremely short segments (12 words), which may render most features useless.
WMT14 LSP1 (en-es) LSP2 (en-es) LSP3 (en-de) LSP4 (es-en) LSP5 (es-en) System Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF #features 17 80 17 80 17 80 17 80 17 80 MAE 0.2715 0.2524 0.2419 0.8119 0.8094 0.8062 0.4315 0.4270 0.4262 0.5012 0.4847 0.4812 0.7112 0.6862 0.6828 RMSE 0.4311 0.4116 0.4076 0.9703 0.9470 0.9453 0.5914 0.5500 0.5463 0.6711 0.6412 0.6392 0.8881 0.8447 0.8472

Table 7: Results with SVR pooled models tested on out-of-domain datasets.

6

System selection results

In what follows we turn to using the predictions from SVR and SVC models showed before for system selection. The task consists in selecting, for each source segment, the best machine translation among all available: two or three depending on the language pair. For this experiments, we eliminated the human translations  as they do not tend to be represented in settings for system selection. Given the low performance of our classifiers in Table 3, we ruled out human translations based on the metadata available, without using these classifiers. Another reason to rule out human translations from the selection is that they are used as references to

System en-de-system1 en-de-system2 en-de-system3 en-de-system4 Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF Mean BL AF

#feats 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80 17 80

de-en-system1 de-en-system2 de-en-system3

en-es-system1 en-es-system2 en-es-system3 en-es-system4

es-en-system1 es-en-system2 es-en-system3

blind MAE RMSE 1.0351 1.2133 1.0487 1.2348 1.0510 1.2375 0.7780 0.9339 0.7006 0.9499 0.6924 0.9124 0.7369 0.8426 0.6354 0.7950 0.6572 0.8127 0.7231 0.8215 0.6438 0.7842 0.6386 0.7905 0.8594 1.0882 0.8747 1.1299 0.8747 1.1299 0.7321 0.8484 0.6897 0.8330 0.7122 0.8509 0.8137 0.9253 0.7472 0.8903 0.7629 0.9300 0.8542 0.9923 0.8956 1.0480 0.8957 1.0480 0.5567 0.6952 0.5275 0.6827 0.5302 0.6884 0.5653 0.6998 0.5155 0.6711 0.5184 0.6704 0.5573 0.6955 0.5103 0.6680 0.5206 0.6727 0.6617 0.8307 0.6617 0.8307 0.6617 0.8308 0.5637 0.6931 0.5588 0.7023 0.5567 0.7026 0.6602 0.8129 0.7233 0.8621 0.6973 0.8435

non-blind MAE RMSE 0.3552 0.4562 0.3350 0.4540 0.3325 0.4545 0.4857 0.5487 0.3615 0.4634 0.3570 0.4644 0.5577 0.6034 0.4535 0.5363 0.4482 0.5245 0.5782 0.6433 0.4912 0.5834 0.4818 0.5741 0.2506 0.3409 0.2123 0.3421 0.2065 0.3415 0.5412 0.6678 0.4745 0.5931 0.4604 0.5850 0.6000 0.6640 0.4965 0.6011 0.4828 0.5901 0.3883 0.4353 0.3633 0.4390 0.3519 0.4381 0.4232 0.5314 0.3812 0.4951 0.3730 0.4893 0.4288 0.5213 0.3821 0.4844 0.3714 0.4761 0.4300 0.5321 0.4022 0.5162 0.3902 0.5016 0.3026 0.3916 0.3022 0.3917 0.3023 0.3915 0.4494 0.6027 0.4384 0.6061 0.4309 0.6053 0.4720 0.6245 0.4993 0.6220 0.4974 0.6198

Table 6: SVR to build individual predictions models for each language pair and translation source. compute BLEU scores of the selected sets of sentences, as explained below. To provide an indication of the average quality of each MT system, Table 8 presents the BLEU scores on the test and training sets for individual MT systems. The bold-face figures for each language test set indicate the (BLEU) quality that would be achieved for that test set if the "best" system were selected on the basis of the average (BLEU) quality of the training set (i.e., no system selection). There is a significant variance in terms of quality scores, as measured by BLEU, among the outputs of 2-3 MT systems for each language pair, with training set quality being a good predictor of test set quality for all but en-es, once again, the largest dataset. We measure the performance of the selected sets in two ways: (i) by computing the BLEU scores of the entire sets containing the supposedly best translations, using the human translation available in the datasets as reference, and (ii) by computing the accuracy of the selection process against the human labels, i.e., by computing the proportion of times both system selection and human agree (based on the pre-defined 1-3 human labels) that the sentence selected is the best among the 2-3 options (2-3 MT systems). We compare the results obtained from building pooled (all MT systems)

WMT14 en-de de-en en-es es-en

system2 Test Training 15.39 12.79 27.96 24.03 25.89 34.13 37.83 40.01

system3 Test Training 13.75 13.83 22.66 20.19 32.68 28.42 23.55 25.07

system4 Test Training 17.04 16.19 29.25 31.97 -

Table 8: BLEU scores of individual MT systems, without system selection. Bold-faced figures indicate scores obtained when selecting best system on average (using BLEU scores for the training set). against individual prediction models (one per MT system). Table 9 and 10 show the selection results with various models trained on MT translations only:  Best-SVR(I): Best translation selected with regression model trained on data from individual MT systems, where prediction models are trained per MT system, and the translation selected for each source segment is the one with the highest predicted score among these independent models. This requires knowing the source of the translations for training, but not for testing (blind test).  Best-SVR(P): Best translation selected with regression model trained on pooled data from all MT systems. This assumes a blind setting where the source of the translations for both training and test sets is unknown, and thus pooling data is the only option for system selection.  Best-SVC(P): Best translation selected with the classification model trained on pooled data from all MT systems as above. For SVC, only the pooled models were used as predicting exact 1-3 labels with independently trained models leads to an excessively number of ties (i.e., multiple translations with same score), making the decision between them virtually arbitrary. Table 9 shows that the regression models trained on individual systems  Best-SVR(I)  with AF as feature set yield the best results, despite the fact that error scores (MAE and RMSE) for these individual systems are worse than for systems trained on pooled data. This is somewhat expected as knowing the system that produced the translation (i.e., training models for each MT system) adds a strong bias to the prediction problem towards the average quality of such a system, which is generally a decent quality predictor. We note however that the Best-SVR(P) models are not far behind in terms of performance, with the Best-SVC(P) following closely. In all cases, the gains with respect to the MC baseline are substantial. More important, we note the gains in BLEU scores as compared to the bold-face test set figures in Table 8, showing that our system selection approach leads to best translated test sets than simply picking the MT system with best average quality (BLEU). Results in terms of accuracy in selecting the best translation (Table 10) are similar to those in terms of BLEU scores, with models trained independently performing best.

7

Remarks

We have presented a number of experiments showing the potential of a system selection techniques in scenarios where translations are given by multiple MT systems and system identifiers are unknown. System selection was performed based on predictions from classification and regression models. Results in terms of BLEU and accuracy of selected sets with an MT system-agnostic approach show improvements for system selection over strong baselines. Overall  in bind test settings  although the prediction error of models trained on individual MT systems are worse than models trained on pooled data, when used for system selection, models trained on individual systems generally perform better.

Acknowledgments
This work was supported by funding from the from European Union's Seventh Framework Programme for research, technological development and demonstration under grant agreement no. 296347 (QTLaunchPad).

References
E. Avramidis. Sentence-level ranking with quality estimation. Machine Translation, 28:120,

en-de de-en en-es es-en

System MC BL AF MC BL AF MC BL AF MC BL AF

#feats 17 80 17 80 17 80 17 80

Best-SVR(I) 16.14 17.20 18.10 25.81 28.39 28.75 30.88 32.92 33.45 30.13 38.10 38.73

Best-SVR(P) 15.55 17.05 17.55 25.17 28.13 28.43 30.29 32.81 33.25 29.70 38.11 38.41

Best-SVC(P) 16.12 17.33 17.32 25.07 28.19 28.21 30.23 32.74 33.10 29.53 38.14 38.15

Table 9: BLEU scores on best selected translations (I = Individual, P = Pooled).
System MC BL AF MC BL AF MC BL AF MC BL AF #feats 17 80 17 80 17 80 17 80 Best-SVR(I) 0.1823 0.2017 0.2155 0.3511 0.3733 0.3915 0.3698 0.3912 0.4102 0.4073 0.4321 0.4513 Best-SVR(P) 0.1787 0.2001 0.2055 0.3527 0.3711 0.3923 0.3643 0.3901 0.4087 0.4043 0.4301 0.4421 Best-SVC(P) 0.1793 0.2033 0.2065 0.3559 0.3713 0.3821 0.3659 0.3892 0.4051 0.4059 0.4290 0.4423

en-de de-en en-es es-en

Table 10: Accuracy in selecting the best translation for each dataset (I = Individual, P = Pooled). 2013. E. Avramidis and M. Popovi c. Machine learning methods for comparative and time-oriented Quality Estimation of Machine Translation output. In 8th WMT, pages 329336, Sofia, 2013. E. Bic  ici. Referential translation machines for quality estimation. In 8th WMT, Sofia, 2013. J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. Confidence Estimation for Machine Translation. In Coling, pages 315321, Geneva, 2004. O. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. Findings of the 2013 WMT. In 8th WMT, pages 144, Sofia, 2013. L. Formiga, M. Gonz alez, A. Barr on-Cedeno, J. A. Fonollosa, and L. M` arquez. The TALP-UPC approach to system selection: Asiya features and pairwise classification using random forests. In 8th WMT, pages 359364, Sofia, 2013. M. Gamon, A. Aue, and M. Smets. Sentencelevel MT evaluation without reference translations: beyond language modeling. In EAMT2005, Budapest, 2005. Y. He, Y. Ma, J. van Genabith, and A. Way. Bridging smt and tm with translation recommendation. In ACL-2010, pages 622630, Uppsala, Sweden, 2010. S. Hildebrand and S. Vogel. MT quality estimation: The CMU system for WMT'13. In 8th WMT, pages 373379, Sofia, 2013. K. Shah, E. Avramidis, E. Bic  ici, and L. Specia. Quest - design, implementation and extensions of a framework for machine translation quality estimation. Prague Bull. Math. Linguistics, 100: 1930, 2013. L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. Estimating the Sentence-Level Quality of Machine Translation Systems. In EAMT-2009, pages 2837, Barcelona, 2009. L. Specia, D. Raj, and M. Turchi. Machine translation evaluation versus quality estimation. Machine Translation, pages 3950, 2010. L. Specia, K. Shah, J. G. C. d. Souza, and T. Cohn. Quest - a translation quality estimation framework. In ACL-2013 Demo Session, pages 79 84, Sofia, 2013.

An Investigation on the Effectiveness of Features for Translation Quality Estimation
Kashif Shah, Trevor Cohn and Lucia Specia Department of Computer Science University of Sheffield Sheffield, United Kingdom {kashif.shah,t.cohn,l.specia}@sheffield.ac.uk

Abstract
We describe a systematic analysis on the effectiveness of features commonly exploited for the problem of predicting machine translation quality. Using a feature selection technique based on Gaussian Processes, we identify small subsets of features that perform well across many datasets for different language pairs, text domains, machine translation systems and quality labels. In addition, we show the potential of the reduced feature sets resulting from our feature selection technique to lead to significantly better performance in most datasets, as compared to the complete feature sets.

1

Introduction

As Machine Translation (MT) systems become widely adopted both for gisting purposes and to produce professional quality translations, automatic methods are needed for predicting the quality of translations. This is referred to as Quality Estimation (QE). Different from standard MT evaluation metrics, QE metrics do not have access to reference (human) translations; they are aimed at MT systems in use. Applications of QE include:  Decide which segments need revision by a translator (quality assurance);  Decide whether a reader gets a reliable gist of the text;  Estimate how much effort it will be needed to post-edit a segment;  Select among alternative translations produced by different MT systems.

Work in QE started with the goal of estimating automatic metrics such as BLEU (Papineni et al., 2002) and WER (Blatz et al., 2004). However, these metrics are difficult to interpret, particularly at the sentence-level, and results proved unsuccessful. A new surge of interest in the field started recently, motivated by the widespread use of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments similar to "fuzzy match scores" from translation memory systems is needed. QE work addresses this problem by using more complex metrics that go beyond matching the source segment against previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. Recent work focuses on estimating more interpretable metrics, where "quality" is defined according to the task at hand: post-editing, gisting, etc. A number of positive results have been reported (Section 2). QE is generally addressed as a supervised machine learning task using algorithms to induce models from examples of translations described through a number of features and annotated for quality. One of most challenging aspects of the task is the design of feature extractors to capture relevant aspects of quality. A wide range of features from source and translation texts and external resources and tools have been used. These go from simple, languageindependent features, to advanced, linguistically motivated features. They include features that rely

on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced. This variety of features plays a key role in QE, but it also introduces a few challenges. Datasets for QE are usually small because of the cost of human annotation. Therefore, large feature sets bring sparsity issues. In addition, some of these features are more costly to extract as they depend on external resources or require time-consuming computations. Finally, it is generally believed that different datasets (i.e. language pair, MT system or specific quality annotation such as post-editing time vs translation adequacy) can benefit from different features. Feature selection techniques can help not only select the best features for a given dataset, but also understand which features are in general effective. While recent work has exploited selection techniques to some extent, the focus has been on improving QE performance on individual datasets (Section 2). As a result, no general conclusions can be made about the effectiveness of features across language pairs, text domains, MT systems and quality labels. In this paper we propose to use Gaussian Processes for feature selection, a technique that has proved effective in ranking features according to their discriminative power (Specia et al., 2013). We benchmark with this technique on two settings: (i) nine datasets for three language pairs, seven Statistical MT (SMT) systems and three types of quality scores with the same feature sets; (i) one dataset (same language pair and quality scores) with seven feature sets produced in a completely independent fashion (by participants in a shared task on the topic) (Section 3). The experiments showed the potential of feature selection to improve overall regression results, often outperforming published results on features that had been previously selected using other methods. They also allowed us to identify a small number of wellperforming features across datasets (Section 4). We discuss the feasibility of extracting these features based on their dependence on external resources or specific languages.

2

Related work

Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more ef-

fort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011). For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most previous work on QE use machine learning algorithms such as SVMs, which are robust to redundant/noisy features to a certain extent. In what follows we summarise recent work using explicit feature selection methods in the WMT12 QE shared task. Gonz alez-Rubio et al. (2012) performed feature selection on a set of 475 sentence- and subsentence level features. Principal Component Analysis and a greedy selection algorithm to iteratively create subsets of increasing size with the best-scoring individual features were exploited. Both selection methods yielded better performance than all features, with greedy selection achieving the best MAE scores with 254 features. Langlois et al. (2012) reported positive results with a greedy backward selection algorithm that removes 21 poor features from an initial set of 66 features based on error minimisation on a development set. In an oracle-like experiment, Felice and Specia (2012) use a sequential forward selection method, which starts from an empty set and adds one feature at a time as long as it decreases the model's error, evaluating the performance of the feature subsets on the test set directly. 37 features out of 147 are selected, and these significantly improved the overall performance. Avramidis (2012) tested a few feature selection methods using both greedy stepwise and best first search to select among their 266 features with 10fold cross-validation on the training set. These resulted in sets of 30-80 features, all outperforming the complete feature set. Correlation-based selection with best first search strategy was reported to perform the best. Conversely, Moreau and Vogel (2012) reported no improvements in performance in experiments with several selection methods.

Finally, (Soricut et al., 2012), the winning system in the WMT12 QE shared task, used a computationally-intensive method on a development set. For each of the official evaluation metrics (e.g. MAE), from an initial set of 24 features, all 224 possible combinations were tested, followed by an exhaustive search to find the best combinations. The 15 features belonging to most of the top combinations were selected. Other rounds were added to deal with POS features, but the final feature sets included 14-15 features depending on the evaluation metric. This technique outperformed the complete feature set by a very large margin.

Data Training Test WMT12 (en-es) 1,832 422 EAMT11 (en-es) 900 64 EAMT11 (fr-en) 2,300 225 EAMT09-s1 -s4 (en-es) 3,095 906 GALE11-s1 -s2 (ar-en) 2,198 387 Table 1: Number of sentences in our datasets datasets (we did not have access to the MT systems that produced the other datasets). For the WMT12 and GALE11 datasets, we experimented with both BB and GB features. The BB feature sets are the same for all datasets, except for one language pair (Arabic-English), where languagespecific features supplement the initial 80 features. We also distinguish one special feature: the pseudo-reference (PR), as this is not a standard feature in that it requires another MT system to be extracted. This feature consists in translating the source sentence using another MT system (in our case, Google Translate) to obtain a pseudoreference. The geometric mean of (lambdasmoothed) 1-to-4-gram precision scores (i.e. a smoothed version of BLEU to avoid 0-counts without the brevity penalty) is then computed between the original MT and this pseudo-reference. We note that the better the external MT system, the closer the pseudo-reference translation is to a human translation, and therefore the more reliable this feature becomes. For each dataset we built five systems:  BL: 17 features that performed well across languages in previous work and were used as baseline in the WMT12 QE task.  AF: All features available for the dataset, a superset of the above. For a comprehensive list, we refer the reader to QuEst website.3  BL+PR: 17 baseline features along with a pseudo reference feature.  AF+PR: All features along with a pseudo reference feature.  FS(GP): Feature selection for automatic ranking and selection of top features with Gaussian Process on set AF+PR. 3.2 WMT12 feature sets

3
3.1

Experimental settings
Datasets with common feature sets

All datasets used in our experiments are available for download.1 The statistics of these datasets are shown in Table 1. WMT12 English-Spanish news sentence translations produced by a phrase-based (PB) Moses "baseline" SMT system,2 and judged for postediting effort in 1-5 (highest-lowest), taking a weighted average of three annotators. EAMT11 English-Spanish (EAMT11-en-es) and French-English (EAMT11-fr-en) news sentence translations produced by a PB-SMT Moses baseline system and judged for post-editing effort in 1-4 (highest-lowest). EAMT09 English sentences from the European Parliament corpus translated by four SMT systems (two Moses-like PB-SMT systems and two fully discriminative training systems) into Spanish and scored for post-editing effort in 1-4 (highestlowest). Systems are denoted by s1 -s4 . GALE11 Arabic newswire sentences translated by two Moses-like PB-SMT systems into English and scored for adequacy in 1-4 (worst-best). Systems are denoted by s1 -s2 . The features for these datasets are extracted using an open source toolkit QuEst.3 We differentiate between black-box (BB) and glass-box (GB) features, as only BB are available for all
1

http://www.dcs.shef.ac.uk/~lucia/ resources.html 2 http://www.statmt.org/moses/?n=Moses. Baseline 3 http://www.quest.dcs.shef.ac.uk

These very diverse feature sets were provided by the participants in the WMT12 shared task on QE.4
4

These feature sets were made available by the task organisers at http://www.dcs.shef.ac.uk/~lucia/

We note that in a few cases these are a subset of the datasets used in the shared task, e.g. UU. This explains the difference between the official scores reported in (Callison-Burch et al., 2012) and our figures. This difference can also be explained by the learning algorithms: while we used GPs, participants have used SVRs, M5P and other algorithms. Some of these feature sets already result from feature selection techniques. SDL (Soricut et al., 2012): 15 features selected after an exhaustive search algorithm based on all possible combinations of features. This is the optimal set used by the winning submission. It includes many of the baseline features, the pseudoreference feature, phrase table probabilities, and a few part-of-speech tag alignment features. UU (Hardmeier et al., 2012): 82 features, a subset of those used in the shared-task as the parse tree features (based on tree-kernels) were not provided by the participants. These are similar to the common BL and BB features presented above and include various source and target LM features, average number of translations per source word, number of tokens matching certain patterns (hyphens, ellipsis, etc.), percentage of n-grams seen in corpus, percentage of non-aligned words, etc. UEdin (Buck, 2012): 56 black-box features including source translatability, named entities, LM back-off features, discriminative word-lexicon, edit distance between source sentence and the SMT source training corpus, and word-level features based on neural networks to select a subset of relevant words among all words in the corpus. Loria (Langlois et al., 2012): 49 features including 1-5gram LM and back-off LM features, interlingual and cross-lingual mutual information features, IBM1 model average translation probability, punctuation checks, and out-of-vocabulary rate. TCD (Moreau and Vogel, 2012): 43 features based on the similarity between the (source or target) sentence and a reference set (the SMT training corpus or Google N-grams) with n-grams of different lengths, including the TF-IDF metric. WLV-SHEF (Felice and Specia, 2012): 147 features which are a superset of the common 80 BB features above. The additional features include
resources.html

a number of linguistically motivated features for source or target sentences (percentage) or their comparison (ratio), such as content words and function words, width and depth of constituency and dependency trees, nouns, verbs and pronouns. UPC (Pighin et al., 2012): 56 features on top of the baseline features. Most of these features are based on different language models estimated on reference and automatic Spanish translations. 3.3 Gaussian Processes for feature selection and model learning

Gaussian Processes (GPs; Rasmussen and Williams (2006)) are an advanced machine learning framework incorporating Bayesian nonparametrics and kernels, and are widely regarded as state of the art for many regression tasks. Despite that, GPs have been under-exploited for language applications. Most of the previous work on QE uses kernel-based Support Vector Machines for regression (SVR), based on experimental findings that non-linear models significantly outperform linear models. Like SVR, GPs can describe non-linear functions using kernels such as radial basis function (RBF). However in contrast, inference in GP regression can be expressed analytically and the kernel hyper-parameters optimised directly using gradient descent. This avoids the need for costly grid search while also allowing the use of much richer kernel functions with many more parameters. Further differences between the two techniques are that GPs are probabilistic models, and take a fully Bayesian approach by integrating out the model parameters to represent the posterior distribution. GPs allow for many different kernels. Here we consider the RBF with automatic relevance determination,
2 k ( x, x ) =  f exp -

1 2

D i

xi - xi li

(1)

where the k (x, x ) is the kernel function between two data points x, x and D is the number of features, and f and li  0 are the kernel hyperparameters, which control the covariance magnitude and the length scales of variation in each dimension, respectively. This is closely related to the

RBF kernel used with SVR, except that each feature is scaled independently from the others, i.e., li = l for SVR, while GPs allow for a vector of independent values. Following standard practice we also include an additive white-noise term 2 . The kernel hyperin the kernel with variance s parameters (f , n , l) are learned from data using a maximum likelihood estimates. The learned length scale hyper-parameters can be interpreted as the per-feature RBF widths which encode the importance of a feature: the narrower the RBF (the smaller is li ) the more important a change in the feature value is to the model prediction. Therefore, a model trained using GPs can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) features. GPs on their own do not provide a cutoff point on this ranked list of features, instead this needs to be determined by evaluating loss on a separate set to determine the optimal number of features. In our experiments, learning and feature ranking are performed with an open source implementation of GP5 regression. Each feature is centred and scaled to have zero mean and unit standard deviation. For feature ranking, the models are trained on the full training sets. The RBF widths, scale and noise variance are initialised with an isotropic kernel (with a single length scale, li = l) which helps to avoid local minima. The hyper-parameters are learned using gradient descent with a maximum of 100 iterations and cross-validation on the training set. A forward selection approach is then used to select features ranked from top to worst and train models with increasing numbers of features. In an oracle-like experiment, we analyse the performance of models with different sizes of feature sets directly on the test set. The subset of top ranked features that minimises error in each test set is selected to report optimal results and therefore the potential of feature selection using GPs.

4.1

Results on the common feature sets

4

Results

We use Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to evaluate the models.
5

http://sheffieldml.github.io/GPy/

The error scores for all datasets with common BB features are reported in Table 2, while Table 3 shows the results with GB features for a subset of these datasets, and Table 4 the results with BB and GB features together for the same datasets. For each Table and dataset, bold-faced figures are significantly better than all others (paired t-test with p  0.05). It can be seen from the results that adding more BB features (systems AF) improves the results in most cases as compared to the baseline systems BL, however, in some cases the improvements are not significant. This behaviour is to be expected as adding more features may bring more relevant information, but at the same time it makes the representation more sparse and the learning prone to overfitting. It is interesting to note that adding a single feature, the pseudo-reference (systems BL+PR) to our baseline improves results in all datasets, often by a large margin. Similar improvements are observed by adding this feature to the set with all available features (systems AF+PR) . Our experiments with feature selection using GPs lead to significant further improvements in most cases. We note that the FS(GP) figures are produced from selecting the ideal number of topranked features based on the test set results, and therefore should be interpreted as oracle-like optimal results. These results show the potential of feature selection with GPs: FS(GP) outperforms other systems despite using considerably fewer features (10-20 in most cases, with up to 31 in the Arabic-English datasets). These are very promising results, as they show that it is possible to reduce the resources and overall computational complexity for training the models, while achieving similar or better performance. For a more realistic overview of the results of feature selection using GPs, we plot the learning curves for some of our datasets. The learning curves for top-ranked features according to our forward selection method for two of our feature sets is given in Figures 1. The y axis shows the MAE scores, while the x axis shows the number of features selected. Generally, we observe a very fast error decrease in the beginning as features are added until approximately 20 features, where the minimum (optimal) error scores

(a)

0.67 0.66 0.65
MAE

0.46 0.455 0.45
MAE

(b)

0.64 0.63 0.62 0.61 0

0.445 0.44 0.435 0.43 0

10

20

30

40

50

60

70

80

Number of features

10

20

30

40

50

60

70

80

Number of features

Figure 1: Error on (a) WMT12 and (b) EAMT11-en-es datasets with 80 BB features ranked by GPs are found, and as more features are added, the error starts to quickly increase again, until a plateau is reached (approximately 45 features). This shows that while a very small number of features is naturally insufficient, adding features ranked lower by GPs degrades performance. Similar curves were observed for all datasets with slightly different ranges for optimal numbers of features and best score. It is interesting to note that the best performance gains on most datasets are observed within the 10-20 top-ranked features. Therefore, even though our optimal results rely on the test as oracle, this range of features could be used to find optimal results across datasets without an oracle.
0.675 0.67 0.665 0.66 0.655 0.65 0.645 0.64 0 10 20 30 40
Number of features

teams participating in the WMT12 QE shared task. These feature sets are very diverse in terms of the types of features, resources used, and their sizes. As shown in Table 5, we observed similar results: feature selection with GPs has the potential to outperform models with all initial feature sets. Improvements were observed even on feature sets which had already been produced as a result of some other feature selection technique. Table 5 also shows the official results from the shared task (Callison-Burch et al., 2012), which are often different from the results obtained with GPs even before feature selection, simply because of differences in the learning algorithms used. In some cases results with GPs before feature selection are better, notably for WLV-SHEF, showing the potential of GPs as a learning algorithm for QE. The learning curves with the performance on the test sets for different numbers of top-ranked features have a similar shape to those with the common feature sets. As an example, Figure 2 shows the Uppsala University feature set, with the lowest error score for the 15 top-ranked features. 4.3 Commonly selected features

MAE

50

60

70

80

Figure 2: Error on 82 UU ranked features GB features on their own perform worse than BB features, but in all three datasets, the combination of GB and BB followed by feature selection resulted in significantly lower errors than using only BB features with feature selection, showing that the two features sets are complementary. 4.2 Results on the WMT12 dataset

In order to investigate whether our feature selection results hold for other feature sets, we experimented with the feature sets provided by most

Next we investigate whether it is possible to identify a common subset of features which are selected for the optimal feature sets in most datasets. In our experiments with the common feature sets, we found that the following features appear ranked at the top set that maximises the performance of the models in at least 75% of the times out of all datasets where they appear:  LM perplexities and log probabilities for source and target sentences;  size of source and target sentences;  average number of possible translations of

Team SDL UU Loria UEdin TCD WLV-SHEF UPC DCU PRHLT

System AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP) AF FS(GP)

#feats. 15 10 82 10 49 10 56 20 43 10 147 15 57 15 308 15 497 30

Official WMT12 score MAE RMSE 0.61 0.75 0.64 0.79 0.68 0.68 0.68 0.69 0.84 0.75 0.70 0.82 0.82 0.82 0.85 1.01 0.97 0.85 -

Score with GP MAE RMSE 0.6030 0.7510 0.6015 0.7474 0.6507 0.8012 0.6419 0.7931 0.6866 0.8340 0.6824 0.8395 0.6949 0.8540 0.6795 0 8323 0.6906 0.8367 0.6904 0.8370 0.6665 0.8219 0.6592 0.8088 0.8365 0.9601 0.8092 0.9288 0.6782 0.8394 0.6137 0.7602 0.6733 0.8297 0.6647 0.8179

Table 5: Results on WMT12 feature sets. * indicates initial feature sets resulting from feature selection source words (IBM 1 with thresholds);  ratio of target by source lengths in words;  percentage of numbers in the target sentence;  percentage of distinct unigrams seen in the MT source training corpus;  pseudo-reference. Interestingly, not all top ranked features are among the 17 reportedly good baseline features. All of these features are language-independent. Also, most of them are simple and straightforward to extract: they either do not rely on external resources, or use resources that are easily available, such as tools for LM (e.g., SRILM), or wordalignment (e.g., GIZA++). The same analysis on the feature sets from the WMT12 shared task is not possible, given the very little overlap in features used by the different feature sets.

Acknowledgments
This work was supported by the QTLaunchPad project (EU FP7 CSA No. 296347).

References
Avramidis, Eleftherios. 2012. Quality estimation for machine translation output using linguistic analysis and decoding features. In WMT12, pages 8490, Montr eal, Canada. Bach, Nguyen, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: a method for measuring machine translation confidence. In ACL11, pages 211219, Portland, Oregon. Blatz, John, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence Estimation for Machine Translation. In Coling04, pages 315321, Geneva. Buck, Christian. 2012. Black-box Features for the WMT 2012 Quality Estimation Shared Task. In WMT12, pages 9195, Montr eal, Canada. Callison-Burch, Chris, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 WMT. In WMT12, pages 10 51, Montr eal, Canada. Felice, Mariano and Lucia Specia. 2012. Linguistic features for quality estimation. In WMT12, pages 96103, Montr eal, Canada. Gonz alez-Rubio, Jes us, Alberto Sanch is, and Francisco Casacuberta. 2012. PRHLT Submission to the WMT12 Quality Estimation Task. In WMT12, pages 104108, Montr eal, Canada.

5

Conclusion

We have presented a number of experiments showing the potential of a promising feature ranking technique based on Gaussian Processes for translation quality estimation. Using an oracle to select the number of top-ranked features to train quality estimation models, this technique has been shown to outperform strong baseline systems with only a small fraction of their features. In addition, it allowed us to identify a common set of features which perform well across many datasets with different language pairs, machine translation systems, text domains and quality labels.

Dataset WMT12

EAMT11-en-es

EAMT11-fr-en

EAMT09-s1

EAMT09-s2

EAMT09-s3

EAMT09-s4

GALE11-s1

GALE11-s2

System BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP) BL AF BL+PR AF+PR FS(GP)

#feats. 17 80 18 81 19 17 80 18 81 20 17 80 18 81 10 17 80 18 81 13 17 80 18 81 12 17 80 18 81 15 17 80 18 81 19 17 123 18 81 27 17 123 18 81 31

MAE 0.6821 0.6717 0.6290 0.6324 0.6131 0.4857 0.4719 0.4490 0.4471 0.4320 0.4401 0.4292 0.4183 0.4169 0.4110 0.5313 0.5265 0.5123 0.5109 0.5025 0.4614 0.4741 0.4493 0.4609 0.4410 0.5339 0.5437 0.5113 0.5309 0.5060 0.3591 0.3578 0.3401 0.3409 0.3370 0.5462 0.5399 0.5301 0.5249 0.5210 0.5540 0.5401 0.5401 0.5249 0.5194

RMSE 0.8117 0.8103 0.7729 0.7735 0.7598 0.6178 0.5418 0.5329 0.5301 0.5260 0.6301 0.6192 0.6192 0.6181 0.6099 0.6655 0.6538 0 .6492 0.6441 0.6391 0.5816 0.5953 0.5692 0.5821 0.5625 0.6619 0.6827 0.6492 0.6771 0.6410 0.4942 0.4960 0.4811 0.4816 0.4799 0.6885 0.6805 0.6814 0.6766 0.6701 0.7117 0.6911 0.7014 0.6806 0.6779

Dataset WMT12 GALE11-s1 GALE11-s2

System AF FS(GP) AF FS(GP) AF FS(GP)

#feats. 128 29 163 30 172 17

MAE 0.7185 0.6101 0.5455 0.5150 0.5239 0.5109

RMSE 0.8451 0.7561 0.6722 0.6681 0.6529 0.6431

Table 4: Results with common BB & GB features
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way. 2010. Bridging SMT and TM with Translation Recommendation. In ACL2010, pages 622630, Uppsala, Sweden. Langlois, David, Sylvain Raybaud, and Kamel Sma ili. 2012. LORIA System for the WMT12 Quality Estimation Shared Task. In WMT12, pages 114119, Montr eal, Canada. Moreau, Erwan and Carl Vogel. 2012. Quality estimation: an experimental study using unsupervised similarity measures. In WMT12, pages 120126, Montr eal, Canada. Papineni, Kishore, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL02, pages 311318, Philadelphia, Pennsylvania. Pighin, Daniele, Meritxell Gonz alez, and Llu is M` arquez. 2012. The UPC Submission to the WMT 2012 Shared Task on Quality Estimation. In WMT12, pages 127132, Montr eal, Canada. Rasmussen, Carl E. and Christopher K.I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT press Cambridge, MA. Soricut, Radu and Abdessamad Echihabi. 2010. TrustRank: Inducing Trust in Automatic Translations via Ranking. In ACL11, pages 612621, Uppsala, Sweden. Soricut, Radu, Nguyen Bach, and Ziyuan Wang. 2012. The SDL Language Weaver Systems in the WMT12 Quality Estimation Shared Task. In WMT12, pages 145151, Montr eal, Canada. Specia, Lucia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the Sentence-Level Quality of Machine Translation Systems. In EAMT09, pages 2837, Barcelona. Specia, Lucia, Dhwaj Raj, and Marco Turchi. 2010. Machine translation evaluation versus quality estimation. Machine Translation, pages 3950. Specia, Lucia, Kashif Shah, Jos e G. C. De Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. In Proceedings of ACL Demo Session (to appear). Specia, Lucia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In EAMT11, pages 7380, Leuven.

Table 2: Results with common BB features
Dataset WMT12 GALE11-s1 GALE11-s2 System AF FS(GP) AF FS(GP) AF FS(GP) #feats. 47 21 39 19 48 13 MAE 0.7066 0.6755 0.5736 0.5702 0.5540 0.5491 RMSE 0.8445 0.8298 0.7402 0.7361 0.6979 0.6974

Table 3: Results with common GB features

Hardmeier, Christian, Joakim Nivre, and J org Tiedemann. 2012. Tree Kernels for Machine Translation Quality Estimation. In WMT12, pages 109113, Montr eal, Canada.

The Prague Bulletin of Mathematical Linguistics

NUMBER 100 OCTOBER 2013

1930

QuEst -- Design, Implementation and Extensions of a Framework for Machine Translation Quality Estimation Kashif Shaha , Eleftherios Avramidisb , Ergun Biicic , Lucia Speciaa
University of Sheffield German Research Center for Artificial Intelligence c Centre for Next Generation Localization, Dublin City University
b a

Abstract
In this paper we present QUEST, an open source framework for machine translation quality estimation. The framework includes a feature extraction component and a machine learning component. We describe the architecture of the system and its use, focusing on the feature extraction component and on how to add new feature extractors. We also include experiments with features and learning algorithms available in the framework using the dataset of the WMT13 Quality Estimation shared task.

1. Introduction
Quality Estimation (QE) is aimed at predicting a quality score for a machine translated segment, in our case, a sentence. The general approach is to extract a number of features from source and target sentences, and possibly external resources and information from the Machine Translation (MT) system for a dataset labelled for quality, and use standard machine learning algorithms to build a model that can be applied to any number of unseen translations. Given its independence from reference translations, QE has a number of applications, for example filtering out low quality translations from human post-editing. Most of current research focuses on designing feature extractors to capture different aspects of quality that are relevant to a given task or application. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced information can be very labour-intensive. Dif-

 2013 PBML. All rights reserved. Corresponding author: Kashif.Shah@sheffield.ac.uk Cite as: Kashif Shah, Eleftherios Avramidis, Ergun Biici, Lucia Specia. QuEst -- Design, Implementation and Extensions of a Framework for Machine Translation Quality Estimation. The Prague Bulletin of Mathematical Linguistics No. 100, 2013, pp. 1930. doi: 10.2478/pralin-2013-0008.

Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

ferent language pairs or optimisation against specific quality scores (e.g., post-editing time versus translation adequacy) can benefit from different feature sets. QUEST is a framework for quality estimation that provides a wide range of feature extractors from source and translation texts and external resources and tools (Section 2). These range from simple, language-independent features, to advanced, linguistically motivated features. They include features that rely on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced, and also features that only consider the source and/or target sides of the dataset (Section 2.1). QUEST also incorporates wrappers for a wellknown machine learning toolkit, scikit-learn1 and for additional algorithms (Section 2.2). This paper is aimed at both users interested in experimenting with existing features and algorithms and developers interested in extending the framework to incorporate new features (Section 3). For the former, QUEST provides a practical platform for quality estimation, freeing researchers from feature engineering, and facilitating work on the learning aspect of the problem, and on ways of using quality predictions in novel extrinsic tasks, such as self-training of statistical machine translation systems. For the latter, QUEST provides the infrastructure and the basis for the creation of new features, which may also reuse resources or pre-processing techniques already available in the framework, such as syntactic parsers, and which can be quickly benchmarked against existing features.

2. Overview of the QUEST Framework
QUEST consists of two main modules: a feature extraction module and a machine learning module. It is a collaborative project, with contributions from a number of researchers.2 The first module provides a number of feature extractors, including the most commonly used features in the literature and by systems submitted to the WMT1213 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013). It is implemented in Java and provides abstract classes for features, resources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires a pair of raw text files with the source and translation sentences aligned at the sentence-level. Additional resources such as the source MT training corpus and language models of source and target languages are necessary for certain features. Configuration files are used to indicate the resources available and a list of features that should be extracted. It produces a CSV file with all feature values. The machine learning module provides scripts connecting the feature file(s) with the scikit-learn toolkit. It also uses GPy, a Python toolkit for Gaussian Processes regression, which showed good performance in previous work (Shah et al., 2013).
1 http://scikit-learn.org/ 2 See http://www.quest.dcs.shef.ac.uk/

for a list of collaborators.

20
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

Adequacy indicators

Source text

MT system

Translation

Complexity indicators

Confidence indicators

Fluency indicators

Figure 1: Families of features in QUEST.

2.1. Feature Sets In Figure 1 we show the families of features that can be extracted in QUEST. Although the text unit for which features are extracted can be of any length, most features are more suitable for sentences. Therefore, a "segment" here denotes a sentence. Most of these features have been designed with Statistical MT (SMT) systems in mind, although many do not explore any internal information from the actual SMT system. Further work needs to be done to test these features for rule-based and other types of MT systems, and to design features that might be more appropriate for those. From the source segments QUEST can extract features that attempt to quantify the complexity or translatability of those segments, or how unexpected they are given what is known to the MT system. From the comparison between the source and target segments, QUEST can extract adequacy features, which attempt to measure whether the structure and meaning of the source are preserved in the translation. Information from the SMT system used to produce the translations can provide an indication of the confidence of the MT system in the translations. They are called "glass-box" features (GB) to distinguish them from MT system-independent, "black-box" features (BB). To extract these features, QUEST assumes the output of Moses-like SMT systems, taking into account word- and phrase-alignment information, a dump of the decoder's standard output (search graph information), global model score and feature values, n-best lists, etc. For other SMT systems, it can also take an XML file with relevant information. From the translated segments QUEST can extract features that attempt to measure the fluency of such translations. The most recent version of the framework includes a number of previously underexplored features that can rely on only the source (or target) side of the segments and on the source (or target) side of the parallel corpus used to train the SMT system. Information retrieval (IR) features measure the closeness of the QE source sentences and their translations to the parallel training data available to predict the difficulty of translating each sentence. These have been shown to work very well in recent work
21
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

(Biici et al., 2013; Biici, 2013). We use Lucene3 to index the parallel training corpora and obtain a retrieval similarity score based on tf-idf. For each source sentence and its translation, we retrieve top 5 distinct training instances and calculate the following features:  IR score for each training instance retrieved for the source sentence or its translation  BLEU (Papineni et al., 2002) and F1 (Biici, 2011) scores over source or target sentences  LIX readability score4 for source and target sentences  The average number of characters in source and target words and their ratios. In Section 4 we provide experiments with these new features. The complete list of features available is given as part of QUEST's documentation. At the current stage, the number of BB features varies from 80 to 143 depending on the language pair, while GB features go from 39 to 48 depending on the SMT system. 2.2. Machine Learning QUEST provides a command-line interface module for the scikit-learn library implemented in Python. This module is completely independent from the feature extraction code. It reads the extracted feature sets to build and test QE models. The dependencies are the scikit-learn library and all its dependencies (such as NumPy and SciPy). The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. The pipeline with feature selection and hyper-parameter optimisation can be set using a configuration file. Currently, the module has an interface for Support Vector Regression (SVR), Support Vector Classification, and Lasso learning algorithms. They can be used in conjunction with the feature selection algorithms (Randomised Lasso and Randomised decision trees) and the grid search implementation of scikit-learn to fit an optimal model of a given dataset. Additionally, QUEST includes Gaussian Process (GP) regression (Rasmussen and Williams, 2006) using the GPy toolkit.5 GPs are an advanced machine learning framework incorporating Bayesian non-parametrics and kernel machines, and are widely regarded as state of the art for regression. Empirically we found its performance to be similar or superior to that of SVR for most datasets. In contrast to SVR, inference in GP regression can be expressed analytically and the model hyper-parameters optimised using gradient ascent, thus avoiding the need for costly grid search. This also makes the method very suitable for feature selection.
3 lucene.apache.org 4 http://en.wikipedia.org/wiki/LIX 5 https://github.com/SheffieldML/GPy

22
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

3. Design and Implementation
3.1. Source Code We made available three versions of the code, all available from http://www.
quest.dcs.shef.ac.uk:

 An installation script that will download the stable version of the source code, a built up version (jar), and all necessary pre-processing resources/tools (parsers, etc.).  A stable version of the above source code only (no linguistic processors).  A vanilla version of the source code which is easier to run (and re-build), as it relies on fewer pre-processing resources/tools. Toy resources for en-es are also included in this version. It only extracts up to 50 features. In addition, the latest development version of the code can be accessed on GitHub.6 3.2. Setting Up Once downloaded, the folder with the code contains all files required for running or building the application. It contains the following folders and resources:  src: java source files  lib: jar files, including the external jars required by QUEST  dist: javadoc documentation  lang-resources: example of language resources required to extract features  config: configuration files  input: example of input training files (source and target sentences, plus quality labels)  output: example of extracted feature values 3.3. The Feature Extractor The class that performs feature extraction is shef.mt.FeatureExtractor. It handles the extraction of glass-box and/or black-box features from a pair of source-target input files and a set of additional resources specified as input parameters. Whilst the command line parameters relate to the current set of input files, FeatureExtractor also relies on a set of project-specific parameters, such as the location of resources. These are defined in a configuration file in which resources are listed as pairs of key=value entries. By default, if no configuration file is specified in the input, the application will search for a default config.properties file in the current working folder (i.e., the folder where the application is launched from). This default file is provided with the distribution. Another input parameter required is the XML feature configuration file, which gives the identifiers of the features that should be extracted by the system. Unless
6 https://github.com/lspecia/quest

23
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

a feature is present in this feature configuration file it will not be extracted by the system. Examples of such files for all features, black-box, glass-box, and a subset of 17 "baseline" features are provided with the distribution. 3.4. Running the Feature Extractor The following command triggers the features extractor:
FeatureExtractor -input <source file> <target file> -lang <source language> <target language> -config <configuration file> -mode [gb|bb|all] -gb [list of GB resources]

where the arguments are:  -input <source file> <target file> (required): the input source and target text files with sentences to extract features from  -lang <source language> <target language>: source and target languages of the files above  -config <configuration file>: file with the paths to the input/output, XMLfeature files, tools/scripts and language resources  -mode <gb|bb|all>: a choice between glass-box, black-box or both types of features  -gb [list of files]: input files required for computing the glass-box features. The options depend on the MT system used. For Moses, three files are required: a file with the n-best list for each target sentence, a file with a verbose output of the decoder (for phrase segmentation, model scores, etc.), and a file with search graph information. 3.5. Packages and Classes Here we list the important packages and classes. We refer the reader to QUEST documentation for a comprehensive list of modules.  shef.mt.enes: This package contains the main feature extractor classes.  shef.mt.features.impl.bb: This package contains the implementations of black-box features.  shef.mt.features.impl.gb: This package contains the implementations of glass-box features.  shef.mt.features.util: This package contains various utilities to handle information in a sentence and/or phrase.  shef.mt.tools: This package contains wrappers for various pre-processing tools and Processor classes for interpreting the output of the tools.  shef.mt.tools.stf: This package contains classes that provide access to the Stanford parser output.  shef.mt.util: This package contains a set of utility classes that are used throughout the project, as well as some independent scripts used for various data preparation tasks.
24
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

 shef.mt.xmlwrap: This package contains XML wrappers to process the output of SMT systems for glass-box features. The most important classes are as follows:  FeatureExtractor: FeatureExtractor extracts glass-box and/or black-box features from a pair of source-target input files and a set of additional resources specified as input parameters.  Feature: Feature is an abstract class which models a feature. Typically, a Feature consist of a value, a procedure for calculating the value and a set of dependencies, i.e., resources that need to be available in order to be able to compute the feature value.  FeatureXXXX: These classes extend Feature and to provide their own method for computing a specific feature.  Sentence: Models a sentence as a span of text containing multiple types of information produced by pre-processing tools, and direct access to the sentence tokens, n-grams, phrases. It also allows any tool to add information related to the sentence via the setValue() method.  MTOutputProcessor: Receives as input an XML file containing sentences and lists of translation with various attributes and reads it into Sentence objects.  ResourceProcessor: Abstract class that is the basis for all classes that process output files from pre-processing tools.  Pipeline: Abstract class that sets the basis for handling the registration of the existing ResourceProcessors and defines their order.  ResourceManager: This class contains information about resources for a particular feature.  LanguageModel: LanguageModel stores information about the content of a language model file. It provides access to information such as the frequency of n-grams, and the cut-off points for various n-gram frequencies necessary for certain features.  Tokenizer: A wrapper around the Moses tokenizer. 3.6. Developer's Guide A hierarchy of a few of the most important classes is shown in Figure 2. There are two principles that underpin the design choice:  pre-processing must be separated from the computation of features, and  feature implementation must be modular in the sense that one is able to add features without having to modify other parts of the code. A typical application will contain a set of tools or resources (for pre-processing), with associated classes for processing the output of these tools. A Resource is usually a wrapper around an external process (such as a part-of-speech tagger or parser), but it can also be a brand new fully implemented pre-processing tool. The only requirement for a tool is to extend the abstract class shef.mt.tools.Resource. The implementation of a tool/resource wrapper depends on the specific requirements of that
25
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

particular tool and on the developer's preferences. Typically, it will take as input a file and a path to the external process it needs to run, as well as any additional parameters the external process requires, it will call the external process, capture its output and write it to a file. The interpretation of the tool's output is delegated to a subclass of shef.mt.tools.ResourceProcessor associated with that particular Resource. A ResourceProcessor typically:  Contains a function that initialises the associated Resource. As each Resource may require a different set of parameters upon initialisation, ResourceProcessor handles this by passing the necessary parameters from the configuration file to the respective function of the Resource.  Registers itself with the ResourceManager in order to signal the fact that it has successfully managed to initialise itself and it can pass information to be used by features. This registration should be done by calling ResourceManager.registerResource(String resourceName). resourceName is an arbitrary string, unique among all other Resources. If a feature requires this particular Resource for its computation, it needs to specify it as a requirement (see Section 3.7).  Reads in the output of a Resource sentence by sentence, retrieves some information related to that sentence and stores it in a Sentence object. The processing of a sentence is done in the processNextSentence(Sentence sentence) function which all ResourceProcessor-derived classes must implement. The information it retrieves depends on the requirements of the application. For example, shef.mt.tools.POSProcessor, which analyses the output of the TreeTagger, retrieves the number of nouns, verbs, pronouns and content words, since these are required by certain currently implemented features, but it can be easily extended to retrieve, for example, adjectives, or full lists of nouns instead of counts. A Sentence is an intermediate object that is, on the one hand, used by ResourceProcessor to store information and, on the other hand, by Feature to access this information. The implementation of the Sentence class already contains access methods to some of the most commonly used sentence features, such as the text it spans, its tokens, its n-grams, its phrases and its n-best translations (for glass-box features). For a full list of fields and methods, see the associated javadoc. Any other sentence information is stored in a HashMap with keys of type String and values of generic type Object. A pre-processing tool can store any value in the HashMap by calling setValue(String key, Object value) on the currently processed Sentence object. This allows tools to store both simple values (integer, float) as well as more complex ones (for example, the ResourceProcessor). A Pipeline defines the order in which processors will be initialised and run. They are defined in the shef.mt.pipelines package. They allow more flexibility for the execution of pre-processors, when there are dependencies between each other. At the moment QUEST offers a default pipeline which contains the tools required for the "vanilla" version of the code and new FeatureExtractors have to register there. A
26
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

more convenient solution would be a dynamic pipeline which automatically identifies the processors required by the enabled features and then initialises and runs only them. This functionality is currently under development in QUEST. 3.7. Adding a New Feature In order to add a new feature, one has to implement a class that extends A Feature will typically have an index and a description which should be set in the constructor. The description is optional, whilst the index is used in selecting and ordering the features at runtime, therefore it should be set. The only function a new Feature class has to implement is run(Sentence source, Sentence target). This will perform some computation over the source and/or target sentence and set the return value of the feature by calling setValue(float value). If the computation of the feature value relies on some pre-processing tools or resources, the constructor can add these resources or tools in order to ensure that the feature will not run if the required files are not present. This is done by a call to addResource(String resource_name), where resource_name has to match the name of the resource registered by the particular tool this feature depends on.
shef.mt.features.impl.Feature.

4. Benchmarking
In this section we briefly benchmark QUEST using the dataset of the main WMT13 shared task on QE (subtask 1.1) using all our features, and in particular the new source-based and IR features. The dataset contains English-Spanish sentence translations produced by an SMT system and judged for post-editing effort in [0,1] using TERp,7 computed against a human post-edited version of the translations (i.e. HTER). 2, 254 sentences were used for training, while 500 were used for testing. As learning algorithm we use SVR with radial basis function (RBF) kernel, which has been shown to perform very well in this task (Callison-Burch et al., 2012). The optimisation of parameters is done with grid search based on pre-set ranges of values as given in the code distribution. For feature selection, we use Gaussian Processes. Feature selection with Gaussian Processes is done by fitting per-feature RBF widths. The RBF width denotes the importance of a feature, the narrower the RBF the more important a change in the feature value is to the model prediction. To avoid the need of a development set to optimise the number of selected features, we select the 17 top-ranked features (as in our baseline system) and then train a model with these features. For given dataset we build the following systems with different feature sets:  BL: 17 baseline features that have been shown to perform well across languages in previous work and were used as a baseline in the WMT12 QE task
7 http://www.umiacs.umd.edu/~snover/terp/

27
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

(b) A particular feature extends the Feature class and is associated with the Sentence class (a) The Feature class

(c) An abstract Resource class acts as a wrapper for external processes

(d) ResourceProcessor reads the output of a tool and stores it in a Sentence object

28

Figure 2: Class hierarchy for most important classes.

Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

 AF: All features available from the latest stable version of QUEST, either blackbox (BB) or glass-box (GB)  IR: IR-related features recently integrated into QUEST (Section 2.1)  AF+IR: All features available as above, plus recently added IR-related features  FS: Feature selection for automatic ranking and selection of top features from all of the above with Gaussian Processes. Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to evaluate the models. The error scores for all feature sets are reported in Table 1. Boldfaced figures are significantly better than all others (paired t-test with p  0.05).
Feature type BB System Baseline IR AF AF+IR FS AF FS   AF FS #feats. 17 35 108 143 17 48 17 191 17 MAE 14.32 14.57 14.07 13.52 12.61 17.03 16.57 14.03 12.51 RMSE 18.02 18.29 18.13 17.74 15.84 20.13 19.14 19.03 15.64

GB BB+GB

Table 1: Results with various feature sets.

Adding more BB features (systems AF) improves the results in most cases as compared to the baseline systems BL, however, in some cases the improvements are not significant. This behaviour is to be expected as adding more features may bring more relevant information, but at the same time it makes the representation more sparse and the learning prone to overfitting. Feature selection was limited to selecting the top 17 features for comparison with our baseline feature set. It is interesting to note that system FS outperformed the other systems in spite of using fewer features. GB features on their own perform worse than BB features but the combination of GB and BB followed by feature selection resulted in lower errors than BB features only, showing that the two features sets can be complementary, although in most cases BB features suffice. These are in line with the results reported in (Specia et al., 2013; Shah et al., 2013). A system submitted to the WMT13 QE shared task using QUEST with similar settings was the top performing submission for Task 1.1 (Beck et al., 2013).

5. Remarks
The source code for the framework, the datasets and extra resources can be downloaded from http://www.quest.dcs.shef.ac.uk/. The project is also set to receive contribution from interested researchers using a GitHub repository. The license for
29
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

the Java code, Python and shell scripts is BSD, a permissive license with no restrictions on the use or extensions of the software for any purposes, including commercial. For pre-existing code and resources, e.g., scikit-learn, GPy and Berkeley parser, their licenses apply, but features relying on these resources can be easily discarded if necessary.

Acknowledgements
This work was supported by the QuEst (EU FP7 PASCAL2 NoE, Harvest program) and QTLaunchPad (EU FP7 CSA No. 296347) projects. We would like to thank our many contributors, especially Jos G. C. Souza for the integration with scikit-learn, and Lukas Poustka for his work on the refactoring of some of the code.

Bibliography
Beck, Daniel, Kashif Shah, Trevor Cohn, and Lucia Specia. SHEF-Lite: When less is more for translation quality estimation. In Proceedings of WMT13, pages 337342, Sofia, 2013. Biici, E. The Regression Model of Machine Translation. PhD thesis, Ko University, 2011. Biici, E. Referential translation machines for quality estimation. In Proceedings of WMT13, pages 341349, Sofia, 2013. Biici, E., D. Groves, and J. van Genabith. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation, 2013. Bojar, O., C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13, pages 144, Sofia, 2013. Callison-Burch, C., P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of WMT12, pages 1051, Montral, 2012. Papineni, K., S. Roukos, T. Ward, and W. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th ACL, pages 311318, Philadelphia, 2002. Rasmussen, C.E. and C.K.I. Williams. Gaussian processes for machine learning, volume 1. MIT Press, Cambridge, 2006. Shah, K., T. Cohn, and L. Specia. An investigation on the effectiveness of features for translation quality estimation. In Proceedings of MT Summit XIV, Nice, 2013. Specia, L., K. Shah, J. G. C. Souza, and T. Cohn. QuEst  a translation quality estimation framework. In Proceedings of the 51st ACL: System Demonstrations, pages 7984, Sofia, 2013.

Address for correspondence: Kashif Shah
Kashif.Shah@sheffield.ac.uk

Department of Computer Science University of Sheffield Regent Court, 211 Portobello, Sheffield, S1 4DP UK
30
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

LIUM's SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo ic Barrault, Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr

Abstract
This paper describes the development of FrenchEnglish and EnglishFrench statistical machine translation systems for the 2011 WMT shared task evaluation. Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words.

2

Resources Used

The following sections describe how the resources provided or allowed in the shared task were used to train the translation and language models of the system. 2.1 Bilingual data

1

Introduction

This paper describes the statistical machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2011 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year's system (Lambert et al., 2010) are as follows: use of more training data as provided by the organizers, improved translation model adaptation by unsupervised training, a continuous space language model for the translation into French, some attempts to automatically induce translations of unknown words and first experiments with hierarchical systems. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. 464

Our system was developed in two stages. First, a baseline system was built to generate automatic translations of some of the monolingual data available. These automatic translations were then used directly with the source texts to create additional bitexts. In a second stage, these additional bilingual data were incorporated into the system (see Section 5 and Tables 4 and 5). The latest version of the News-Commentary (NC) corpus and of the Europarl (Eparl) corpus (version 6) were used. We also took as training data a subset of the FrenchEnglish Gigaword (109 ) corpus. We applied the same filters as last year to select this subset. The first one is a lexical filter based on the IBM model 1 cost (Brown et al., 1993) of each side of a sentence pair given the other side, normalised with respect to both sentence lengths. This filter was trained on a corpus composed of Eparl, NC, and UN data. The other filter is an n-gram language model (LM) cost of the target sentence (see Section 3), normalised with respect to its length. This filter was trained with all monolingual resources available except the 109 data. We generated two subsets, both by selecting sentence pairs with a lexical cost inferior to 4, and an LM cost respectively inferior to 2.3 9 (109 1 , 115 million English words) and 2.6 (102 , 232 million English words).

Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464469, Edinburgh, Scotland, UK, July 3031, 2011. c 2011 Association for Computational Linguistics

2.2

Use of Automatic Translations

Available human translated bitexts such as the Europarl or 109 corpus seem to be out-of domain for this task. We used two types of automatically extracted resources to adapt our system to the task domain. First, we generated automatic translations of the provided monolingual News corpus and selected the sentences with a normalised translation cost (returned by the decoder) inferior to a threshold. The resulting bitext contain no new translations, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). This year, we improved this method in the following way. In the original approach, the automatic translations are added to the human translated bitexts and a complete new system is build, including time consuming word alignment with GIZA++. For WMT'11, we directly used the word-to-word alignments produced by the decoder at the output instead of GIZA's alignments. This speeds-up the procedure and yields the same results in our experiments. A detailed comparison is given in (Lambert et al., 2011). Second, as in last year's evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. We used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retrieval techniques. The Lemur toolkit (Ogilvie and Callan, 2001) was used for this purpose. Search was limited to a window of 5 days of the date of the French news text. The retrieved candidate sentences were then filtered using the Translation Error Rate (TER) with respect to the automatic translations. In this study, sentences with a TER below 75% were kept. Sentences with a large length difference (French versus English) or containing a large fraction of numbers were also discarded. By these 465

means, about 27M words of additional bitexts were obtained. 2.3 Monolingual data The French and English target language models were trained on all provided monolingual data. In addition, LDC's Gigaword collection was used for both languages. Data corresponding to the development and test periods were removed from the Gigaword collections. 2.4 Development data All development was done on newstest2009, and newstest2010 was used as internal test set. The default Moses tokenization was used. However, we added abbreviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the tool multi-bleu.perl and are case sensitive.

3

Architecture of the SMT system

The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our main system is a phrase-based system (Koehn et al., 2003; Och and Ney, 2003), but we have also performed some experiments with a hierarchical system (Chiang, 2007). Both use a log linear framework in order to introduce several models explaining the translation process: e = arg max p(e|f ) = arg max{exp(
e i

i hi (e, f ))}

(1)

The feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). The phrase-based system uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The hierarchical system uses only 8 features: a LM weight, a word penalty and six weights for the translation model. Both systems are based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows.

First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases, lexical reorderings or hierarchical rules are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on newstest2009, using the `new' MERT tool. We repeated the training process three times, each with a different seed value for the optimisation algorithm. In this way we have an rough idea of the error introduced by the tuning process. 4-gram back-off LMs were used. The word list contains all the words of the bitext used to train the translation model and all words that appear at least ten times in the monolingual corpora. Words of the monolingual corpora containing special characters or sequences of uppercase characters were not included in the word list. Separate LMs were build on each data source with the SRI LM toolkit (Stolcke, 2002) and then linearly interpolated, optimizing the coefficients with an EM procedure. The perplexities of these LMs were 99.4 for French and 129.7 for English. In addition, we build a 5-gram continuous space language model for French (Schwenk, 2007). This model was trained on all the available French texts using a resampling technique. The continuous space language model is interpolated with the 4-gram back-off model and used to rescore n-best lists. This reduces the perplexity by about 8% relative.

Source language French finies effac es hawaienne ...

Source language stemmed form fini effac e hawaien ...

Target language English finished erased Hawaiian ...

Table 1: Example of translations from French to English which are automatically extracted from the phrase-table with the stemmed form.

from Arabic to English. Alternatively, we propose to infer automatically possible translations when translating from a morphologically rich language, to a simpler language. In our case, we use this approach to translate from French to English. Several of the unknown words are actually adjectives, nouns or verbs in a particular form that itself is not known, but the phrase table would contain the translation of a different form. As an example we can mention the French adjective finies which is in the female plural form. After stemming we may be able to find the translation in a dictionary which is automatically extracted from the phrase-table (see Table 1). This idea was already outlined by (Bojar and Tamchyna, 2011) to translate from Czech to English. First, we automatically extract a dictionary from the phrase table. This is done, be detecting all 1-to-1 entries in the phrase table. When there are multiple entries, all are kept with their lexical translations probabilities. Our dictionary has about 680k unique source words with a total of almost 1M translations.
source segment target segment stemmed word found translations found segment proposed segment kept les travaux sont finis works are finis fini finished, ended works are finished works are ended works are finished

4

Treatment of unknown words

Finally, we propose a method to actually add new translations to the system inspired from (Habash, 2008). For this, we propose to identity unknown words and propose possible translations. Moses has two options when encountering an unknown word in the source language: keep it as it is or drop it. The first option may be a good choice for languages that use the same writing system since the unknown word may be a proper name. The second option is usually used when translating between language based on different scripts, e.g. translating
The source is available at http://www.cs.cmu.edu/ ~qing/
1

Table 2: Example of the treatment of an unknown French word and its automatically inferred translation.

The detection of unknown words is performed by comparing the source and the target segment in order to detect identical words. Once the unknown word is selected, we are looking for its stemmed form in the dictionary and propose some translations for the unknown word based on lexical score of the phrase table (see Table 2 for some examples). The snowball

466

Bitext

#Fr Words (M) Eparl+NC 56 Eparl+NC+109 186 1 Eparl+NC+109 323 2 Eparl+NC+news 140 Eparl+NC+109 +news 406 2 Eparl+NC+109 351 2 +IR Eparl+NC+109 +news+IR 435 2 +larger beam+pruned PT 435

PT size (M) 7.1 16.3 25.4 8.4 25.5 25.3 26.1 8.2

newstest2009 BLEU 26.74 27.96 28.20 27.31 27.93 28.07 27.99 28.44

BLEU 27.36 (0.19) 28.20 (0.04) 28.57 (0.10) 28.41 (0.13) 28.70 (0.24) 28.51 (0.18) 28.93 (0.02) 29.05 (0.14)

newstest2010 TER 55.11 (0.14) 54.46 (0.10) 54.12 (0.13) 54.15 (0.14) 54.12 (0.16) 54.07 (0.06) 53.84 (0.07) 53.74 (0.16)

METEOR 60.13 (0.05) 60.88 (0.05) 61.20 (0.05) 61.13 (0.04) 61.30 (0.20) 61.18 (0.07) 61.46 (0.07) 61.68 (0.09)

Table 4: FrenchEnglish results: number of French words (in million), number of entries in the filtered phrase-table (in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3)
corpus number of sentences number of words number of UNK detected nbr of sentences containing UNK BLEU Score without UNK process BLEU Score with UNK process TER Score without UNK process TER Score with UNK process newstest2010 2489 70522 118 109 29.43 29.43 53.08 53.08 subtest2010 109 3586 118 109 24.31 24.33 58.54 58.59

smaller amounts of parallel training data. In future versions of this detection process, we will try to detect unknown words before the translation process and propose alternatives hypothesis to the Moses decoder.

5

Results and Discussion

Table 3: Statistics of the unknown word (UNK) processing algorithm on our internal test (newstest2010) and its sub-part containing only the processed sentences (subtest2010).

stemmer2 was used. Then the different hypothesis are evaluated with the target language model. We processed the produced translations with this method. It can happen that some words are translations of themselves, e.g. the French word "duel" can be translated by the English word "duel". If theses words are present into the extracted dictionary, we keep them. If we do not find any translation in our dictionary, we keep the translation. By these means we hope to keep named entities. Several statistics made on our internal test (newstest2010) are shown in Table 3. Its shows that the influence of the detected unknown words is minimal. Only 0.16% of the words in the corpus are actually unknown. However, the main goal of this process is to increase the human readability and usefulness without degrading automatic metrics. We also expect a larger impact in other tasks for which we have
2

The results of our SMT system for the French English and EnglishFrench tasks are summarized in Tables 4 and 5, respectively. The MT metric scores are the average of three optimisations performed with different seeds (see Section 3). The numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of the significance of the difference between two systems. If the difference between two average scores is less than the sum of the standard deviations, we can say that this difference is not significant. The reverse is not true. Note that most of the improvements shown in the tables are small and not significant. However many of the gains are cumulative and the sum of several small gains makes a significant difference. Baseline FrenchEnglish System The first section of Table 4 shows results of the development of the baseline SMT system, used to generate automatic translations. Although no French translations were generated, we did similar experiments in the EnglishFrench direction (first section of Table 5).

http://snowball.tartarus.org/

467

Bitext Eparl+NC Eparl+NC+109 1 Eparl+NC+109 2 Eparl+NC+109 2 +news Eparl+NC+109 2 +IR Eparl+NC+109 2 +news+IR +rescoring with CSLM

#En Words (M) 52 167 284 299 311 371 371

newstest2009 BLEU 26.20 26.84 26.95 27.34 27.14 27.32 27.46

newstest2010 BLEU TER 28.06 (0.22) 56.85 (0.08) 29.08 (0.12) 55.83 (0.14) 29.29 (0.03) 55.77 (0.19) 29.56 (0.14) 55.44 (0.18) 29.43 (0.12) 55.48 (0.06) 29.73 (0.21) 55.16 (0.20) 30.04 54.79

Table 5: EnglishFrench results: number of English words (in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.)

In both cases the best system is the one trained on the Europarl, News-commentary and 109 2 corpora. This system was used to generate the automatic translations. We did not observe any gain when adding the United Nations data, so we discarded this data. Impact of the Additional Bitexts With the baseline FrenchEnglish SMT system (see above), we translated the French News corpus to generate an additional bitext (News). We also translated some parts of the French LDC Gigaword corpus, to serve as queries to our IR system (see section 2.2). The resulting additional bitext is referred to as IR. The second section of Tables 4 and 5 summarize the system development including the additional bitexts. With the News additional bitext added to Eparl+NC, we obtain a system of similar performance as the baseline system used to generate the automatic translations, but with less than half of the data. Adding the News corpus to a larger corpus, such as Eparl+NC+109 2 , has less impact but still yields some improvement: 0.1 BLEU point in FrenchEnglish and 0.3 in EnglishFrench. Thus, the News bitext translated from French to English may have more impact when translating from English to French than in the opposite direction. This effect is studied in detail in a separate paper (Lambert et al., 2011). With the IR additional bitext added to Eparl+NC+109 2 , we observe no improvement in French to English, and a very small improvement in English to French. However, added to the base468

line system (Eparl+NC+109 2 ) adapted with the News data, the IR additional bitexts yield a small (0.2 BLEU) improvement in both translation directions. Final System In both translation directions our best system was the one trained on Eparl+NC+109 2 +News+IR. We further achieved small improvements by pruning the phrase-table and by increasing the beam size. To prune the phrase-table, we used the `sigtest-filter' available in Moses (Johnson et al., 2007), more precisely the filter3 . We also build hierarchical systems on the various human translated corpora, using up to 323M words (corpora Eparl+NC+109 2 ). The systems yielded similar results than the phrase-based approach, but required much more computational resources, in particular large amounts of main memory to perform the translations. Running the decoder was actually only possible with binarized rule-tables. Therefore, the hierarchical system was not used in the evaluation system.
3 The p-value of two-by-two contingency tables (describing the degree of association between a source and a target phrase) is calculated with Fisher exact test. This probability is interpreted as the probability of observing by chance an association that is at least as strong as the given one, and hence as its significance. An important special case of a table occurs when a phrase pair occurs exactly once in the corpus, and each of the component phrases occurs exactly once in its side of the parallel corpus (1-1-1 phrase pairs). In this case the negative log of the p-value is = logN (N is number of sentence pairs in the corpus). is the largest threshold that results in all of the 1-1-1 phrase pairs being included.

6

Conclusions and Further Work

We presented the development of our statistical machine translation systems for the FrenchEnglish and EnglishFrench 2011 WMT shared task. In the official evaluation the EnglishFrench system was ranked first according to the BLEU score and the FrenchEnglish system second.

Acknowledgments
This work has been partially funded by the European Union under the EuroMatrixPlus project ICT2007.2.2-FP7-231720 and the French government under the ANR project COSMAT ANR-09-CORD004.

References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 16 23, Athens, Greece. Ond rej Bojar and Ale s Tamchyna. 2011. Forms Wanted: Training SMT on Monolingual Data. Abstract at Machine Translation and Morphologically-Rich Languages. Research Workshop of the Israel Science Foundation University of Haifa, Israel, January. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263311. David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201228. Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 4957, Columbus, Ohio, June. Association for Computational Linguistics. Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation. In ACL 08. Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967 975, Prague, Czech Republic. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrased-based machine translation. In HLT/NACL, pages 127133.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, demonstration session. Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk. 2010. LIUM SMT machine translation system for WMT 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 121126, Uppsala, Sweden, July. Patrik Lambert, Holger Schwenk, Christophe Servan, and Sadaf Abdul-Rauf. 2011. Investigations on translation model adaptation using monolingual data. In Sixth Workshop on SMT, page this volume. Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 295302. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignement models. Computational Linguistics, 29(1):1951. Paul Ogilvie and Jamie Callan. 2001. Experiments using the Lemur toolkit. In In Proceedings of the Tenth Text Retrieval Conference (TREC-10), pages 103108. Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492 518. Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT, pages 182189. A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proc. of the Int. Conf. on Spoken Language Processing, pages 901904, Denver, CO.

469

SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation Quality Estimation
Daniel Beck and Kashif Shah and Lucia Specia Department of Computer Science University of Sheffield Sheffield, United Kingdom {debeck1,kashif.shah,l.specia}@sheffield.ac.uk Abstract
We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. (en-de), and German-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator. Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation. The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator. Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset. For each task, participants can submit two types of results: scoring and ranking. For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). For ranking, DeltaAvg and Spearman's rank correlation were used as evaluation metrics.

1

Introduction

The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple datasets to improve our QE models by applying GPs in a multi-task setting. Our hypothesis is that using sensible multi-task learning settings gives improvements over simply pooling all datasets together. Task 1.1 focuses on predicting post-editing effort for four language pairs: English-Spanish (en-es), Spanish-English (es-en), English-German 307

2

Model

Gaussian Processes are a Bayesian non-parametric machine learning framework considered the stateof-the-art for regression. They assume the presence of a latent function f : RF  R, which maps a vector x from feature space F to a scalar value. Formally, this function is drawn from a GP prior: f (x)  GP (0, k (x, x )), which is parameterised by a mean function (here, 0) and a covariance kernel function k (x, x ). Each response value is then generated from the function evaluated at the corresponding input, yi = f (xi )+ 2 ) is added white-noise.  , where   N (0, n

Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 307312, Baltimore, Maryland USA, June 2627, 2014. c 2014 Association for Computational Linguistics

Prediction is formulated as a Bayesian inference under the posterior: p(y |x , D) =
f

p(y |x , f )p(f |D),

where x is a test input, y is the test response value and D is the training set. The predictive posterior can be solved analitically, resulting in:
2 -1 y  N (kT  (K + n I ) y , 2 -1 k (x , x ) - kT  (K + n I ) k ),

where k = [k (x , x1 )k (x , x2 ) . . . k (x , xn )]T is the vector of kernel evaluations between the training set and the test input and K is the kernel matrix over the training inputs (the Gram matrix). The kernel function encodes the covariance (similarity) between each input pair. While a variety of kernel functions are available, here we followed previous work in QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD):
2 exp - k (x, x ) = f

where kdata is a kernel on the input points, d and d are task or metadata information for each input and B  RDD is the multi-task matrix, which encodes task covariances. For task 1.1, we consider each language pair as a different task, while for tasks 1.2 and 1.3 we use additional datasets for the same language pair (en-es), treating each dataset as a different task. To perform the learning procedure the multitask matrix should be parameterised in a sensible way. We follow the parameterisations proposed by Cohn and Specia (2013), which we briefly describe here: Independent: B = I. In this setting each task is modelled independently. This is not strictly equivalent to independent model training because the tasks share the same data kernel (and the same hyperparameters); Pooled: B = 1. Here the task identity is ignored. This is equivalent to pooling all datasets in a single task model; Combined: B = 1 + I. This setting leverages between independent and pooled models. Here,  > 0 is treated as an hyperparameter; Combined+: B = 1 + diag(). Same as "combined", but allowing one different  value per task. 2.2 Sparse Gaussian Processes The performance bottleneck for GP models is the Gram matrix inversion, which is O(n3 ) for standard GPs, with n being the number of training instances. For multi-task settings this can be a potential issue because these models replicate the instances for each task and the resulting Gram matrix has dimensionality nd  nd, where d is the number of tasks. Sparse GPs tackle this problem by approximating the Gram matrix using only a subset of m inducing inputs. Without loss of generalisation, consider these m points as the first instances in the training data. We can then expand the Gram matrix in the following way: K= Kmm Km(n-m) . K(n-m)m K(n-m)(n-m)

1 2

F i=1

xi - xi li

,

2 is the cowhere F is the number of features, f variance magnitude and li > 0 are the feature lengthscales. The resulting model hyperparameters (SE vari2 , noise variance  2 and SE lengthscales l ) ance f i n were learned from data by maximising the model likelihood. All our models were trained using the GPy1 toolkit, an open source implementation of GPs written in Python.

2.1

Multi-task learning

The GP regression framework can be extended to multiple outputs by assuming f (x) to be a vector valued function. These models are commonly referred as coregionalization models in the GP lit erature (Alvarez et al., 2012). Here we refer to them as multi-task kernels, to emphasize our application. In this work, we employ a separable multi-task kernel, similar to the one used by Bonilla et al. (2008) and Cohn and Specia (2013). Considering a set of D tasks, we define the corresponding multi-task kernel as: k ((x, d), (x , d )) = kdata (x, x )  Bd,d ,
1

(1)

http://sheffieldml.github.io/GPy/

Following the notation in (Rasmussen and Williams, 2006), we refer Km(n-m) as Kmn and 308

its transpose as Knm . The block structure of K forms the basis of the so-called Nystr om approximation: ~ = Knm K-1 Kmn , K (2) mm which results in the following predictive posterior: ~ -1 y  N (kT m G Kmn y,
-1 k ( x , x ) - kT m Kmm km + ~ -1 km ),  2 kT G n m

(3)

~ =  2 Kmm + Kmn Knm and km is the where G n vector of kernel evaluations between test input x and the m inducing inputs. The resulting training complexity is O(m2 n). The remaining question is how to choose the inducing inputs. We follow the approach of Snelson and Ghahramani (2006), which note that these inducing inputs do not need to be a subset of the training data. Their method considers each input as a hyperparameter, which is then optimised jointly with the kernel hyperparameters. 2.3 Features For all tasks we used the QuEst framework (Specia et al., 2013) to extract a set of 80 black-box features as in Shah et al. (2013), for which we had all the necessary resources available. Examples of the features extracted include:  N-gram-based features:  Number of tokens in source and target segments;  Language model (LM) probability of source and target segments;  Percentage of source 13-grams observed in different frequency quartiles of a large corpus of the source language;  Average number of translations per source word in the segment as given by IBM 1 model from a large parallel corpus of the language, with probabilities thresholded in different ways.  POS-based features:  Ratio of percentage of nouns/verbs/etc in the source and target segments;  Ratio of punctuation symbols in source and target segments;  Percentage of direct object personal or possessive pronouns incorrectly translated. 309

For the full set of features we refer readers to QuEst website.2 To perform feature selection, we followed the approach used in Shah et al. (2013) and ranked the features according to their learned lengthscales (from the lowest to the highest). The lengthscale of a feature can be interpreted as the relevance of such feature for the model. Therefore, the outcome of a GP model using an ARD kernel can be viewed as a list of features ranked by relevance, and this information can be used for feature selection by discarding the lowest ranked (least useful) ones.

3

Preliminary Experiments

Our submissions are based on multi-task settings. For task 1.1, we consider each language pair as a different task, training one model for all pairs. For tasks 1.2 and 1.3, we used additional datasets and encoded each one as a different task (totalling 3 tasks): WMT13: these are the datasets provided in last year's QE shared task (Bojar et al., 2013). We combined training and test sets, totalling 2, 754 sentences for HTER prediction and 1, 003 sentences for post-editing time prediction, both for English-Spanish. EAMT11: this dataset is provided by Specia (2011) and is composed of 1, 000 EnglishSpanish sentences annotated in terms of HTER and post-editing time. For each task we prepared two submissions: one trained on a standard GP with the full 80 features set and another one trained on a sparse GP with a subset of 40 features. The features were chosen by training a smaller model on a subset of 400 instances and following the procedure explained in Section 2.3 for feature selection, with a pre-define cutoff point on the number of features (40), based on previous experiments. The sparse models were trained using 400 inducing inputs. To select an appropriate multi-task setting for our submissions we performed preliminary experiments using a 90%/10% split on the corresponding training set for each task. The resulting MAE scores are shown in Tables 1 and 2, for standard and sparse GPs, respectively. The boldface figures correspond to the settings we choose for the
2 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox

Independent Pooled Combined Combined+

en-es 0.4905 0.4957 0.4939 0.4932

Task 1.1 es-en en-de 0.5325 0.5962 0.5171 0.6012 0.5162 0.6007 0.5182 0.5990

de-en 0.5452 0.5612 0.5550 0.5514

Task 1.2 en-es 0.2047 0.2036 0.2321 0.2296

Task 1.3 en-es 0.4486 0.8599 0.7489 0.4472

Table 1: MAE results for preliminary experiments on standard GPs. Post-editing time scores for task 1.3 are shown on log time per word. en-es 0.5036 0.4890 0.4872 0.4935 Task 1.1 es-en en-de 0.5274 0.6002 0.5131 0.5927 0.5183 0.5871 0.5255 0.5864 de-en 0.5532 0.5532 0.5451 0.5458 Task 1.2 en-es 0.3432 0.1597 0.2871 0.1659 Task 1.3 en-es 0.3906 0.6410 0.6449 0.4040

Independent Pooled Combined Combined+

Table 2: MAE results for preliminary experiments on sparse GPs. Post-editing time scores for task 1.3 are shown on log time per word. official submissions, after re-training on the corresponding full training sets. To check the speed-ups obtained from using sparse GPs, we measured wall clock times for training and prediction in Task 1.1 using the "Independent" multi-task setting. Table 3 shows the resulting times and the corresponding speed-ups when comparing to the standard GP. For comparison, we also trained a model using 200 inducing inputs, although we did not use the results of this model in our submissions. Standard GP Sparse GP (m=400) Sparse GP (m=200) Time (secs) 12122 3376 978 Speed-up  3.59x 12.39x coarse behaviour of the labels, ordinal regression GP models (like the one proposed in (Chu et al., 2005)) could be useful for this task. Results for Task 1.2 are shown in Table 5. The standard GP model performed unusually poorly when compared to the baseline or the sparse GP model. To investigate this, we inspected the resulting model hyperparameters. We found out that 2 was optimised to a very low value, the noise n close to zero, which characterises overfitting. The same behaviour was not observed with the sparse model, even though it had a much higher number of hyperparameters to optimise, and was therefore more prone to overfitting. We plan to investigate this issue further but a possible cause could be bad starting values for the hyperparameters. Table 6 shows results for Task 1.3. In this task, the standard GP model outperformed the baseline, with the sparse GP model following very closely. These figures represent significant improvements compared to our submission to the same task in last year's shared task (Beck et al., 2013), where we were not able to beat the baseline. The main differences between last year's and this year's models are the use of additional datasets and a higher number of features (25 vs. 40). The competitive results for the sparse GP models are very promising because they show we can combine multiple datasets to improve post-editing time prediction while employing a sparse model to cope with speed issues. 310

Table 3: Wall clock times and speed-ups for GPs training and prediction: full versus sparse GPs.

4

Official Results and Discussion

Table 4 shows the results for Task 1.1. Using standard GPs we obtained improved results over the baseline for English-Spanish and EnglishGerman only, with particularly substantial improvements for English-Spanish, which also happens for sparse GPs. This may be related to the larger size of this dataset when compared to the others. Our results here are mostly inconclusive though and we plan to investigate this setting more in depth in the future. Specifically, due to the

Standard GP Sparse GP Baseline

en-es   0.21 -0.33 0.17 0.27 0.14 -0.22

es-en   0.11 -0.15 0.12 -0.17 0.12 -0.21

en-de   0.26 -0.36 0.23 -0.33 0.23 -0.34

de-en   0.24 -0.27 0.14 -0.17 0.21 -0.25 de-en MAE RMSE 0.65 0.77 0.66 0.79 0.65 0.78

Standard GP Sparse GP Baseline

en-es MAE RMSE 0.49 0.63 0.54 0.69 0.52 0.66

es-en MAE RMSE 0.62 0.77 0.54 0.69 0.57 0.68

en-de MAE RMSE 0.63 0.74 0.64 0.75 0.64 0.76

Table 4: Official results for task 1.1. The top table shows results for the ranking subtask (: DeltaAvg; : Spearman's correlation). The bottom table shows results for the scoring subtask. Ranking   0.72 0.09 7.69 0.43 5.08 0.31 Scoring MAE RMSE 18.15 23.41 15.04 18.38 15.23 19.48 can easily make training and prediction times prohibitive, and thus another direction if work is to use recent advances in sparse GPs, like the one proposed by Hensman et al. (2013). We believe that the combination of these approaches could further improve the state-of-the-art performance in these tasks.

Standard GP Sparse GP Baseline

Table 5: Official results for task 1.2. Ranking   16.08 0.64 16.33 0.63 14.71 0.57 Scoring MAE RMSE 17.13 27.33 17.42 27.35 21.49 34.28

Acknowledgments
This work was supported by funding from CNPq/Brazil (No. 237999/2012-9, Daniel Beck) and from European Union's Seventh Framework Programme for research, technological development and demonstration under grant agreement no. 296347 (QTLaunchPad).

Standard GP Sparse GP Baseline

Table 6: Official results for task 1.3.

5

Conclusions

We proposed a new setting for training QE models based on Multi-task Gaussian Processes. Our settings combined different datasets in a sensible way, by considering each dataset as a different task and learning task covariances. We also proposed to speed-up training and prediction times by employing sparse GPs, which becomes crucial in multi-task settings. The results obtained are specially promising in the post-editing time task, where we obtained the same results as with standard GPs and improved over our models from the last evaluation campaign. In the future, we plan to employ our multi-task models in large-scale settings, like datasets annotated through crowdsourcing platforms. These datasets are usually labelled by dozens of annotators and multi-task GPs have proved an interesting framework for learning the annotation noise (Cohn and Specia, 2013). However, multiple tasks 311

References
 Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for Vector-Valued Functions: a Review. Foundations and Trends in Machine Learning, pages 137. Daniel Beck, Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. SHEF-Lite : When Less is More for Translation Quality Estimation. In Proceedings of WMT13, pages 337342. John Blatz, Erin Fitzgerald, and George Foster. 2004. Confidence estimation for machine translation. In Proceedings of the 20th Conference on Computational Linguistics, pages 315321. Ondej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13, pages 144.

Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. 2008. Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems. Wei Chu, Zoubin Ghahramani, Francesco Falciani, and David L Wild. 2005. Biomarker discovery in microarray gene expression data with Gaussian processes. Bioinformatics, 21(16):338593, August. Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of ACL. James Hensman, Nicol` o Fusi, and Neil D. Lawrence. 2013. Gaussian Processes for Big Data. In Proceedings of UAI. Carl Edward Rasmussen and Christopher K. I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press Cambridge. Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In Proceedings of MT Summit XIV. Edward Snelson and Zoubin Ghahramani. 2006. Sparse Gaussian Processes using Pseudo-inputs. In Proceedings of NIPS. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas. Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang, and John Shawe-Taylor. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of MT Summit XII. Lucia Specia, Kashif Shah, Jos e G. C. De Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. In Proceedings of ACL Demo Session. Lucia Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of EAMT.

312


