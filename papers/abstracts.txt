Abstract
The translation model of statistical machine translation systems is trained on parallel data coming from various sources and domains. These corpora are usually concatenated, word alignments are calculated and phrases are extracted. This means that the corpora are not weighted according to their importance to the domain of the translation task. This is in contrast to the training of the language model for which well known techniques are used to weight the various sources of texts. On a smaller granularity, the automatic calculated word alignments differ in quality. This is usually not considered when extracting phrases either. In this paper we propose a method to automatically weight the different corpora and alignments. This is achieved with a resampling technique. We report experimental results for a small (IWSLT) and large (NIST) Arabic/English translation tasks. In both cases, significant improvements in the BLEU score were observed.

1

Introduction
Department of Computer Science, University of Sheffield, UK {kashif.shah,l.specia}@sheffield.ac.uk Fondazione Bruno Kessler, University of Trento, Italy turchi@fbk.eu Abstract
We present a new version of Q U E ST  an open source framework for machine translation quality estimation  which brings a number of improvements: (i) it provides a Web interface and functionalities such that non-expert users, e.g. translators or lay-users of machine translations, can get quality predictions (or internal features of the framework) for translations without having to install the toolkit, obtain resources or build prediction models; (ii) it significantly improves over the previous runtime performance by keeping resources (such as language models) in memory; (iii) it provides an option for users to submit the source text only and automatically obtain translations from Bing Translator; (iv) it provides a ranking of multiple translations submitted by users for each source text according to their estimated quality. We exemplify the use of this new version through some experiments with the framework. Keywords: Machine Translation, Translation Evaluation, Translation Quality Estimation

1.

Introduction
Abstract
During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the official WMT'11 task, translating from English to French. Our method achieves improvements of about 0.6 points BLEU on the test set with respect to a system trained on data without any weighting.

a particular translation task may vary quite a lot. Nevertheless, the standard procedure is to concatenate all available parallel data, to perform word alignment using GIZA++ (Och and Ney, 2000) and to extract and score the phrase pairs by simple relative frequency. Doing this, the parallel data is (wrongly) considered as one homogeneous pool of knowledge. We argue that the parallel data is quite inhomogeneous in many practical applications with respect to several factors:  the data may come from different sources that are more or less relevant to the translation task (in-domain versus out-of-domain data).  more generally, the topic or genre of the data may be more or less relevant.  the data may be of different quality (carefully performed human translations versus automatically crawled and aligned data).  the recency of the data with respect to the task may have an influence. This is of interest in the news domain where named entities, etc change over time. There have been several attempts in the literature to address some of these problems. Matsoukas et al. (2009) proposed to weight each sentence in the training bitexts by optimizing a discriminative function on a tuning set. Sentencelevel features are extracted to estimate the weights that are relevant to the given task. Foster et al. (2010) proposed an extended approach by an instant weighting scheme which learns weights on individual phrase pairs instead of sentences and incorporated the instance-weighting model into a linear combination of feature functions. The technique presented in this paper is related to these previous works as it concerns the weighting of corpora or sentences. However, it does not

1

Introduction
Lucia Specia , Kashif Shah , Jose G. C. de Souza and Trevor Cohn Department of Computer Science University of Sheffield, UK {l.specia,kashif.shah,t.cohn}@sheffield.ac.uk Fondazione Bruno Kessler University of Trento, Italy desouza@fbk.eu Abstract
We describe Q U E ST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms. dence estimation, which we believe is a narrower term. A 6-week workshop on the topic at John Hopkins University in 2003 (Blatz et al., 2004) had as goal to estimate automatic metrics such as BLEU (Papineni et al., 2002) and WER. These metrics are difficult to interpret, particularly at the sentence-level, and results of their very many trials proved unsuccessful. The overall quality of MT was considerably lower at the time, and therefore pinpointing the very few good quality segments was a hard problem. No software nor datasets were made available after the workshop. A new surge of interest in the field started recently, motivated by the widespread used of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments similar to "fuzzy match scores" from translation memory systems is needed. QE work addresses this problem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. "Quality" is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al.,

1

Introduction
Abstract
The standard procedure to train the translation model of a phrase-based SMT system is to concatenate all available parallel data, to perform word alignment, to extract phrase pairs and to calculate translation probabilities by simple relative frequency. However, parallel data is quite inhomogeneous in many practical applications with respect to several factors like data source, alignment quality, appropriateness to the task, etc. We propose a general framework to take into account these factors during the calculation of the phrase-table, e.g. by better distributing the probability mass of the individual phrase pairs. No additional feature functions are needed. We report results on two well-known tasks: the IWSLT'11 and WMT'11 evaluations, in both conditions translating from English to French. We give detailed results for different functions to weight the bitexts. Our best systems improve a strong baseline by up to one BLEU point without any impact on the computational complexity during training or decoding.

1

Introduction
Daniel Beck and Kashif Shah and Trevor Cohn and Lucia Specia Department of Computer Science University of Sheffield Sheffield, United Kingdom {debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk Abstract
We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation (subtasks 1.1 and 1.3). Our submissions use the framework of Gaussian Processes to investigate lightweight approaches for this problem. We focus on two approaches, one based on feature selection and another based on active learning. Using only 25 (out of 160) features, our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only 25% of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive. the submissions by the University of Sheffield team. Our models are based on Gaussian Processes (GP) (Rasmussen and Williams, 2006), a non-parametric probabilistic framework. We explore the application of GP models in two contexts: 1) improving the prediction performance by applying a feature selection step based on optimised hyperparameters and 2) reducing the dataset size (and therefore the annotation effort) by performing Active Learning (AL). We submitted entries for two of the four proposed tasks. Task 1.1 focused on predicting HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 2254 EnglishSpanish news sentences translated by Moses (Koehn et al., 2007) and post-edited by a professional translator. The evaluation used a blind test set, measuring MAE (Mean Absolute Error) and RMSE (Root Mean Square Error), in the case of the scoring variant, and DeltaAvg and Spearman's rank correlation in the case of the ranking variant. Our submissions reached 1st (feature selection) and 2nd (active learning) places in the scoring variant, the task the models were optimised for, and outperformed the baseline by a large margin in the ranking variant. The aim of task 1.3 aimed at predicting postediting time using a dataset composed of 800 English-Spanish news sentences also translated by Moses but post-edited by five expert translators. Evaluation was done based on MAE and RMSE on a blind test set. For this task our models were not able to beat the baseline system, showing that more advanced modelling techniques should have been used for challenging quality annotation types and datasets such as this.

1

Introduction
Abstract
We describe experiments on quality estimation to select the best translation among multiple options for a given source sentence. We consider a realistic and challenging setting where the translation systems used are unknown, and no relative quality assessments are available for the training of prediction models. Our findings indicate that prediction errors are higher in this blind setting. However, these errors do not have a negative impact in performance when the predictions are used to select the best translation, compared to non-blind settings. This holds even when test conditions (text domains, MT systems) are different from model building conditions. In addition, we experiment with quality prediction for translations produced by both translation systems and human translators. Although the latter are on average of much higher quality, we show that automatically distinguishing the two types of translation is not a trivial problem.

1

Introduction
Abstract
We describe a systematic analysis on the effectiveness of features commonly exploited for the problem of predicting machine translation quality. Using a feature selection technique based on Gaussian Processes, we identify small subsets of features that perform well across many datasets for different language pairs, text domains, machine translation systems and quality labels. In addition, we show the potential of the reduced feature sets resulting from our feature selection technique to lead to significantly better performance in most datasets, as compared to the complete feature sets.

1

Introduction
Abstract
In this paper we present QUEST, an open source framework for machine translation quality estimation. The framework includes a feature extraction component and a machine learning component. We describe the architecture of the system and its use, focusing on the feature extraction component and on how to add new feature extractors. We also include experiments with features and learning algorithms available in the framework using the dataset of the WMT13 Quality Estimation shared task.

1. Introduction
 shef.mt.xmlwrap: This package contains XML wrappers to process the output of SMT systems for glass-box features. The most important classes are as follows:  FeatureExtractor: FeatureExtractor extracts glass-box and/or black-box features from a pair of source-target input files and a set of additional resources specified as input parameters.  Feature: Feature is an abstract class which models a feature. Typically, a Feature consist of a value, a procedure for calculating the value and a set of dependencies, i.e., resources that need to be available in order to be able to compute the feature value.  FeatureXXXX: These classes extend Feature and to provide their own method for computing a specific feature.  Sentence: Models a sentence as a span of text containing multiple types of information produced by pre-processing tools, and direct access to the sentence tokens, n-grams, phrases. It also allows any tool to add information related to the sentence via the setValue() method.  MTOutputProcessor: Receives as input an XML file containing sentences and lists of translation with various attributes and reads it into Sentence objects.  ResourceProcessor: Abstract class that is the basis for all classes that process output files from pre-processing tools.  Pipeline: Abstract class that sets the basis for handling the registration of the existing ResourceProcessors and defines their order.  ResourceManager: This class contains information about resources for a particular feature.  LanguageModel: LanguageModel stores information about the content of a language model file. It provides access to information such as the frequency of n-grams, and the cut-off points for various n-gram frequencies necessary for certain features.  Tokenizer: A wrapper around the Moses tokenizer. 3.6. Developer's Guide A hierarchy of a few of the most important classes is shown in Figure 2. There are two principles that underpin the design choice:  pre-processing must be separated from the computation of features, and  feature implementation must be modular in the sense that one is able to add features without having to modify other parts of the code. A typical application will contain a set of tools or resources (for pre-processing), with associated classes for processing the output of these tools. A Resource is usually a wrapper around an external process (such as a part-of-speech tagger or parser), but it can also be a brand new fully implemented pre-processing tool. The only requirement for a tool is to extend the abstract class shef.mt.tools.Resource. The implementation of a tool/resource wrapper depends on the specific requirements of that
25
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

particular tool and on the developer's preferences. Typically, it will take as input a file and a path to the external process it needs to run, as well as any additional parameters the external process requires, it will call the external process, capture its output and write it to a file. The interpretation of the tool's output is delegated to a subclass of shef.mt.tools.ResourceProcessor associated with that particular Resource. A ResourceProcessor typically:  Contains a function that initialises the associated Resource. As each Resource may require a different set of parameters upon initialisation, ResourceProcessor handles this by passing the necessary parameters from the configuration file to the respective function of the Resource.  Registers itself with the ResourceManager in order to signal the fact that it has successfully managed to initialise itself and it can pass information to be used by features. This registration should be done by calling ResourceManager.registerResource(String resourceName). resourceName is an arbitrary string, unique among all other Resources. If a feature requires this particular Resource for its computation, it needs to specify it as a requirement (see Section 3.7).  Reads in the output of a Resource sentence by sentence, retrieves some information related to that sentence and stores it in a Sentence object. The processing of a sentence is done in the processNextSentence(Sentence sentence) function which all ResourceProcessor-derived classes must implement. The information it retrieves depends on the requirements of the application. For example, shef.mt.tools.POSProcessor, which analyses the output of the TreeTagger, retrieves the number of nouns, verbs, pronouns and content words, since these are required by certain currently implemented features, but it can be easily extended to retrieve, for example, adjectives, or full lists of nouns instead of counts. A Sentence is an intermediate object that is, on the one hand, used by ResourceProcessor to store information and, on the other hand, by Feature to access this information. The implementation of the Sentence class already contains access methods to some of the most commonly used sentence features, such as the text it spans, its tokens, its n-grams, its phrases and its n-best translations (for glass-box features). For a full list of fields and methods, see the associated javadoc. Any other sentence information is stored in a HashMap with keys of type String and values of generic type Object. A pre-processing tool can store any value in the HashMap by calling setValue(String key, Object value) on the currently processed Sentence object. This allows tools to store both simple values (integer, float) as well as more complex ones (for example, the ResourceProcessor). A Pipeline defines the order in which processors will be initialised and run. They are defined in the shef.mt.pipelines package. They allow more flexibility for the execution of pre-processors, when there are dependencies between each other. At the moment QUEST offers a default pipeline which contains the tools required for the "vanilla" version of the code and new FeatureExtractors have to register there. A
26
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

more convenient solution would be a dynamic pipeline which automatically identifies the processors required by the enabled features and then initialises and runs only them. This functionality is currently under development in QUEST. 3.7. Adding a New Feature In order to add a new feature, one has to implement a class that extends A Feature will typically have an index and a description which should be set in the constructor. The description is optional, whilst the index is used in selecting and ordering the features at runtime, therefore it should be set. The only function a new Feature class has to implement is run(Sentence source, Sentence target). This will perform some computation over the source and/or target sentence and set the return value of the feature by calling setValue(float value). If the computation of the feature value relies on some pre-processing tools or resources, the constructor can add these resources or tools in order to ensure that the feature will not run if the required files are not present. This is done by a call to addResource(String resource_name), where resource_name has to match the name of the resource registered by the particular tool this feature depends on.
shef.mt.features.impl.Feature.

4. Benchmarking
In this section we briefly benchmark QUEST using the dataset of the main WMT13 shared task on QE (subtask 1.1) using all our features, and in particular the new source-based and IR features. The dataset contains English-Spanish sentence translations produced by an SMT system and judged for post-editing effort in [0,1] using TERp,7 computed against a human post-edited version of the translations (i.e. HTER). 2, 254 sentences were used for training, while 500 were used for testing. As learning algorithm we use SVR with radial basis function (RBF) kernel, which has been shown to perform very well in this task (Callison-Burch et al., 2012). The optimisation of parameters is done with grid search based on pre-set ranges of values as given in the code distribution. For feature selection, we use Gaussian Processes. Feature selection with Gaussian Processes is done by fitting per-feature RBF widths. The RBF width denotes the importance of a feature, the narrower the RBF the more important a change in the feature value is to the model prediction. To avoid the need of a development set to optimise the number of selected features, we select the 17 top-ranked features (as in our baseline system) and then train a model with these features. For given dataset we build the following systems with different feature sets:  BL: 17 baseline features that have been shown to perform well across languages in previous work and were used as a baseline in the WMT12 QE task
7 http://www.umiacs.umd.edu/~snover/terp/

27
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

(b) A particular feature extends the Feature class and is associated with the Sentence class (a) The Feature class

(c) An abstract Resource class acts as a wrapper for external processes

(d) ResourceProcessor reads the output of a tool and stores it in a Sentence object

28

Figure 2: Class hierarchy for most important classes.

Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

K. Shah, E. Avramidis, E. Biici, L. Specia

QuEst (1930)

 AF: All features available from the latest stable version of QUEST, either blackbox (BB) or glass-box (GB)  IR: IR-related features recently integrated into QUEST (Section 2.1)  AF+IR: All features available as above, plus recently added IR-related features  FS: Feature selection for automatic ranking and selection of top features from all of the above with Gaussian Processes. Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are used to evaluate the models. The error scores for all feature sets are reported in Table 1. Boldfaced figures are significantly better than all others (paired t-test with p  0.05).
Feature type BB System Baseline IR AF AF+IR FS AF FS   AF FS #feats. 17 35 108 143 17 48 17 191 17 MAE 14.32 14.57 14.07 13.52 12.61 17.03 16.57 14.03 12.51 RMSE 18.02 18.29 18.13 17.74 15.84 20.13 19.14 19.03 15.64

GB BB+GB

Table 1: Results with various feature sets.

Adding more BB features (systems AF) improves the results in most cases as compared to the baseline systems BL, however, in some cases the improvements are not significant. This behaviour is to be expected as adding more features may bring more relevant information, but at the same time it makes the representation more sparse and the learning prone to overfitting. Feature selection was limited to selecting the top 17 features for comparison with our baseline feature set. It is interesting to note that system FS outperformed the other systems in spite of using fewer features. GB features on their own perform worse than BB features but the combination of GB and BB followed by feature selection resulted in lower errors than BB features only, showing that the two features sets can be complementary, although in most cases BB features suffice. These are in line with the results reported in (Specia et al., 2013; Shah et al., 2013). A system submitted to the WMT13 QE shared task using QUEST with similar settings was the top performing submission for Task 1.1 (Beck et al., 2013).

5. Remarks
The source code for the framework, the datasets and extra resources can be downloaded from http://www.quest.dcs.shef.ac.uk/. The project is also set to receive contribution from interested researchers using a GitHub repository. The license for
29
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

PBML 100

OCTOBER 2013

the Java code, Python and shell scripts is BSD, a permissive license with no restrictions on the use or extensions of the software for any purposes, including commercial. For pre-existing code and resources, e.g., scikit-learn, GPy and Berkeley parser, their licenses apply, but features relying on these resources can be easily discarded if necessary.

Acknowledgements
This work was supported by the QuEst (EU FP7 PASCAL2 NoE, Harvest program) and QTLaunchPad (EU FP7 CSA No. 296347) projects. We would like to thank our many contributors, especially Jos G. C. Souza for the integration with scikit-learn, and Lukas Poustka for his work on the refactoring of some of the code.

Bibliography
Beck, Daniel, Kashif Shah, Trevor Cohn, and Lucia Specia. SHEF-Lite: When less is more for translation quality estimation. In Proceedings of WMT13, pages 337342, Sofia, 2013. Biici, E. The Regression Model of Machine Translation. PhD thesis, Ko University, 2011. Biici, E. Referential translation machines for quality estimation. In Proceedings of WMT13, pages 341349, Sofia, 2013. Biici, E., D. Groves, and J. van Genabith. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation, 2013. Bojar, O., C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of WMT13, pages 144, Sofia, 2013. Callison-Burch, C., P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of WMT12, pages 1051, Montral, 2012. Papineni, K., S. Roukos, T. Ward, and W. Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th ACL, pages 311318, Philadelphia, 2002. Rasmussen, C.E. and C.K.I. Williams. Gaussian processes for machine learning, volume 1. MIT Press, Cambridge, 2006. Shah, K., T. Cohn, and L. Specia. An investigation on the effectiveness of features for translation quality estimation. In Proceedings of MT Summit XIV, Nice, 2013. Specia, L., K. Shah, J. G. C. Souza, and T. Cohn. QuEst  a translation quality estimation framework. In Proceedings of the 51st ACL: System Demonstrations, pages 7984, Sofia, 2013.

Address for correspondence: Kashif Shah
Kashif.Shah@sheffield.ac.uk

Department of Computer Science University of Sheffield Regent Court, 211 Portobello, Sheffield, S1 4DP UK
30
Unauthenticated | 10.248.254.158 Download Date | 8/15/14 4:11 PM

LIUM's SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo ic Barrault, Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr

Abstract
This paper describes the development of FrenchEnglish and EnglishFrench statistical machine translation systems for the 2011 WMT shared task evaluation. Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words.

2

Resources Used

The following sections describe how the resources provided or allowed in the shared task were used to train the translation and language models of the system. 2.1 Bilingual data

1

Introduction
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 16 23, Athens, Greece. Ond rej Bojar and Ale s Tamchyna. 2011. Forms Wanted: Training SMT on Monolingual Data. Abstract at Machine Translation and Morphologically-Rich Languages. Research Workshop of the Israel Science Foundation University of Haifa, Israel, January. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263311. David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201228. Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 4957, Columbus, Ohio, June. Association for Computational Linguistics. Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation. In ACL 08. Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967 975, Prague, Czech Republic. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrased-based machine translation. In HLT/NACL, pages 127133.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, demonstration session. Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk. 2010. LIUM SMT machine translation system for WMT 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 121126, Uppsala, Sweden, July. Patrik Lambert, Holger Schwenk, Christophe Servan, and Sadaf Abdul-Rauf. 2011. Investigations on translation model adaptation using monolingual data. In Sixth Workshop on SMT, page this volume. Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 295302. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignement models. Computational Linguistics, 29(1):1951. Paul Ogilvie and Jamie Callan. 2001. Experiments using the Lemur toolkit. In In Proceedings of the Tenth Text Retrieval Conference (TREC-10), pages 103108. Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492 518. Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT, pages 182189. A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proc. of the Int. Conf. on Spoken Language Processing, pages 901904, Denver, CO.

469

SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation Quality Estimation
Daniel Beck and Kashif Shah and Lucia Specia Department of Computer Science University of Sheffield Sheffield, United Kingdom {debeck1,kashif.shah,l.specia}@sheffield.ac.uk Abstract
We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes, which aim to speed up training and prediction by providing sensible sparse approximations. (en-de), and German-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator. Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation. The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator. Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset. For each task, participants can submit two types of results: scoring and ranking. For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). For ranking, DeltaAvg and Spearman's rank correlation were used as evaluation metrics.

1

Introduction
